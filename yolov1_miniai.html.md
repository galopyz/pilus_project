# Yolo v1 using minai


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

TODO: use more modern techniques to speed up training: `torch.compile`,
`transform.v2`.

``` python
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.optim import lr_scheduler

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import fastcore.all as fc

from functools import partial
from collections import Counter

from minai import *
from pilus_project.core import *
from pilus_project.yolov1 import *
```

``` python
set_seed(42)
```

``` python
transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])
IMG_DIR = "../data/images"
LABEL_DIR = "../data/labels"

bs = 64
```

``` python
!ls ..
```

    8examples.csv      _proc        pilus_project       test.csv
    8examples_val.csv  data         pilus_project.egg-info  train.all.txt
    LICENSE        generate_csv.py  pyproject.toml      train.csv
    MANIFEST.in    nbs          settings.ini        voc_dataset.sh
    README.md      old_txt_files    setup.py            voc_label.py

``` python
trn_ds = VOCDataset("../train.csv", 
                    transform=transform,
                    img_dir=IMG_DIR,
                    label_dir=LABEL_DIR)
x0, y0 = trn_ds[0]
plot_image(x0, y0)
```

![](54_yolov1_miniai_files/figure-commonmark/cell-6-output-1.png)

``` python
# examples_val_8 = pd.read_csv("../test.csv").iloc[:8]
# examples_val_8
```

``` python
# examples_val_8.to_csv('../8examples_val.csv', index=False)
```

``` python
val_ds = VOCDataset(
    "../test.csv", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,
)
x0, y0 = val_ds[0]
plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])
```

![](54_yolov1_miniai_files/figure-commonmark/cell-9-output-1.png)

``` python
len(trn_ds), len(val_ds)
```

    (16550, 4951)

``` python
trn_dl, val_dl = get_dls(trn_ds, val_ds, bs)
xb, yb = next(iter(trn_dl))
xb.shape, yb.shape
```

    (torch.Size([64, 3, 448, 448]), torch.Size([64, 7, 7, 30]))

``` python
dls = DataLoaders(trn_dl, val_dl)
```

``` python
class Yolov1(nn.Module):
    def __init__(self, in_channels=3, **kwargs):
        super(Yolov1, self).__init__()
        self.architecture = architecture_config
        self.in_channels = in_channels
        self.darknet = self._create_conv_layers(self.architecture)
        self.fcs = self._create_fcs(**kwargs)

    def forward(self, x):
        x = self.darknet(x)
        return self.fcs(torch.flatten(x, start_dim=1))

    def _create_conv_layers(self, architecture):
        layers = []
        in_channels = self.in_channels

        for x in architecture:
            if type(x) == tuple:
                layers += [
                    CNNBlock(
                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],
                    )
                ]
                in_channels = x[1]

            elif type(x) == str:
                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]

            elif type(x) == list:
                conv1 = x[0]
                conv2 = x[1]
                num_repeats = x[2]

                for _ in range(num_repeats):
                    layers += [
                        CNNBlock(
                            in_channels,
                            conv1[1],
                            kernel_size=conv1[0],
                            stride=conv1[2],
                            padding=conv1[3],
                        )
                    ]
                    layers += [
                        CNNBlock(
                            conv1[1],
                            conv2[1],
                            kernel_size=conv2[0],
                            stride=conv2[2],
                            padding=conv2[3],
                        )
                    ]
                    in_channels = conv2[1]

        return nn.Sequential(*layers)

    def _create_fcs(self, split_size, num_boxes, num_classes):
        S, B, C = split_size, num_boxes, num_classes

        # In original paper this should be
        # nn.Linear(1024*S*S, 4096),
        # nn.LeakyReLU(0.1),
        # nn.Linear(4096, S*S*(B*5+C))

        return nn.Sequential(
            nn.Flatten(),
            nn.Linear(1024 * S * S, 4096),
            nn.Dropout(0.0),
            nn.LeakyReLU(0.1),
            nn.Linear(4096, S * S * (C + B * 5)),
        )
```

## Learner

``` python
class MeanAP:
    def __init__(self, num_classes=1, epsilon=1e-6, threshold=0.4, iou_threshold=0.5, box_format='midpoint'):
        self.num_classes = num_classes
        self.epsilon = epsilon
        self.threshold = threshold
        self.iou_threshold = iou_threshold
        self.box_format = box_format
        self.average_precisions = []
    
    def reset(self):
        self.average_precisions = []
    
    def compute(self):
        return sum(self.average_precisions) / len(self.average_precisions)
    
    def update(self, pred, label):
        # `get_bboxes` part
        all_pred_boxes = []
        all_true_boxes = []
        train_idx = 0
        batch_size = pred.shape[0]
        pred_boxes = cellboxes_to_boxes(pred)
        true_boxes = cellboxes_to_boxes(label)
        
        for idx in range(batch_size):
            nms_boxes = non_max_suppression(
                pred_boxes[idx],
                iou_threshold=self.iou_threshold,
                threshold=self.threshold,
                box_format=self.box_format,
            )

            for nms_box in nms_boxes:
                all_pred_boxes.append([train_idx] + nms_box)

            for box in true_boxes[idx]:
                # many will get converted to 0 pred
                if box[1] > self.threshold:
                    all_true_boxes.append([train_idx] + box)
        
        pred_boxes = all_pred_boxes
        true_boxes = all_true_boxes
        
        for c in range(self.num_classes):
            detections = []
            ground_truths = []

            # Go through all predictions and targets,
            # and only add the ones that belong to the
            # current class c
            for detection in pred_boxes:
                if detection[1] == c:
                    detections.append(detection)

            for true_box in true_boxes:
                if true_box[1] == c:
                    ground_truths.append(true_box)

            # find the amount of bboxes for each training example
            # Counter here finds how many ground truth bboxes we get
            # for each training example, so let's say img 0 has 3,
            # img 1 has 5 then we will obtain a dictionary with:
            # amount_bboxes = {0:3, 1:5}
            amount_bboxes = Counter([gt[0] for gt in ground_truths])

            # We then go through each key, val in this dictionary
            # and convert to the following (w.r.t same example):
            # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}
            for key, val in amount_bboxes.items():
                amount_bboxes[key] = torch.zeros(val)

            # sort by box probabilities which is index 2
            detections.sort(key=lambda x: x[2], reverse=True)
            TP = torch.zeros((len(detections)))
            FP = torch.zeros((len(detections)))
            total_true_bboxes = len(ground_truths)

            # If none exists for this class then we can safely skip
            if total_true_bboxes == 0:
                continue

            for detection_idx, detection in enumerate(detections):
                # Only take out the ground_truths that have the same
                # training idx as detection
                ground_truth_img = [
                    bbox for bbox in ground_truths if bbox[0] == detection[0]
                ]

                num_gts = len(ground_truth_img)
                best_iou = 0

                for idx, gt in enumerate(ground_truth_img):
                    iou = intersection_over_union(
                        torch.tensor(detection[3:]),
                        torch.tensor(gt[3:]),
                        box_format=self.box_format,
                    )

                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = idx

                if best_iou > self.iou_threshold:
                    # only detect ground truth detection once
                    if amount_bboxes[detection[0]][best_gt_idx] == 0:
                        # true positive and add this bounding box to seen
                        TP[detection_idx] = 1
                        amount_bboxes[detection[0]][best_gt_idx] = 1
                    else:
                        FP[detection_idx] = 1

                # if IOU is lower then the detection is a false positive
                else:
                    FP[detection_idx] = 1

            TP_cumsum = torch.cumsum(TP, dim=0)
            FP_cumsum = torch.cumsum(FP, dim=0)
            recalls = TP_cumsum / (total_true_bboxes + self.epsilon)
            precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + self.epsilon))
            precisions = torch.cat((torch.tensor([1]), precisions))
            recalls = torch.cat((torch.tensor([0]), recalls))
            # torch.trapz for numerical integration
            self.average_precisions.append(torch.trapz(precisions, recalls))
```

``` python
cbs = [
    TrainCB(),
    DeviceCB(),
    MetricsCB(MeanAP(num_classes=20)),
]
opt = partial(torch.optim.AdamW, betas=(0.9,0.95), eps=1e-5)
```

``` python
model = Yolov1(split_size=7, num_boxes=2, num_classes=20)

lr, epochs = 1e-6, 20
tmax = epochs * len(dls.train)
sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)
xtra = [BatchSchedCB(sched)]

learn = Learner(model, dls, YoloLoss(), lr=lr, cbs=cbs+xtra, opt_func=torch.optim.AdamW)
```

`learn.show_image_batch` needs to be fixed.

``` python
learn.show_image_batch()
```

![](54_yolov1_miniai_files/figure-commonmark/cell-17-output-1.png)

``` python
learn.fit(epochs, cbs=[ProgressCB(plot=True)])
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: left;">
<th data-quarto-table-cell-role="th">MeanAP</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.000</td>
<td>4073.525</td>
<td>0</td>
<td>train</td>
<td>06:23</td>
</tr>
<tr>
<td>0.000</td>
<td>6326.053</td>
<td>0</td>
<td>eval</td>
<td>00:59</td>
</tr>
<tr>
<td>0.002</td>
<td>2554.204</td>
<td>1</td>
<td>train</td>
<td>03:32</td>
</tr>
<tr>
<td>0.014</td>
<td>3933.714</td>
<td>1</td>
<td>eval</td>
<td>00:42</td>
</tr>
<tr>
<td>0.030</td>
<td>1555.705</td>
<td>2</td>
<td>train</td>
<td>03:55</td>
</tr>
<tr>
<td>0.068</td>
<td>2416.115</td>
<td>2</td>
<td>eval</td>
<td>01:02</td>
</tr>
<tr>
<td>0.084</td>
<td>1030.208</td>
<td>3</td>
<td>train</td>
<td>04:24</td>
</tr>
<tr>
<td>0.106</td>
<td>1800.721</td>
<td>3</td>
<td>eval</td>
<td>01:07</td>
</tr>
<tr>
<td>0.118</td>
<td>848.246</td>
<td>4</td>
<td>train</td>
<td>04:29</td>
</tr>
<tr>
<td>0.107</td>
<td>1615.993</td>
<td>4</td>
<td>eval</td>
<td>01:05</td>
</tr>
<tr>
<td>0.123</td>
<td>787.683</td>
<td>5</td>
<td>train</td>
<td>04:23</td>
</tr>
<tr>
<td>0.106</td>
<td>1543.894</td>
<td>5</td>
<td>eval</td>
<td>01:03</td>
</tr>
<tr>
<td>0.120</td>
<td>757.363</td>
<td>6</td>
<td>train</td>
<td>04:18</td>
</tr>
<tr>
<td>0.103</td>
<td>1502.395</td>
<td>6</td>
<td>eval</td>
<td>01:04</td>
</tr>
<tr>
<td>0.120</td>
<td>736.230</td>
<td>7</td>
<td>train</td>
<td>04:17</td>
</tr>
<tr>
<td>0.102</td>
<td>1473.216</td>
<td>7</td>
<td>eval</td>
<td>01:03</td>
</tr>
<tr>
<td>0.118</td>
<td>716.683</td>
<td>8</td>
<td>train</td>
<td>04:13</td>
</tr>
<tr>
<td>0.100</td>
<td>1445.808</td>
<td>8</td>
<td>eval</td>
<td>01:02</td>
</tr>
<tr>
<td>0.114</td>
<td>701.659</td>
<td>9</td>
<td>train</td>
<td>04:11</td>
</tr>
<tr>
<td>0.102</td>
<td>1425.591</td>
<td>9</td>
<td>eval</td>
<td>01:03</td>
</tr>
<tr>
<td>0.107</td>
<td>688.869</td>
<td>10</td>
<td>train</td>
<td>04:10</td>
</tr>
<tr>
<td>0.099</td>
<td>1401.951</td>
<td>10</td>
<td>eval</td>
<td>01:07</td>
</tr>
<tr>
<td>0.111</td>
<td>678.630</td>
<td>11</td>
<td>train</td>
<td>04:14</td>
</tr>
<tr>
<td>0.103</td>
<td>1389.554</td>
<td>11</td>
<td>eval</td>
<td>01:01</td>
</tr>
<tr>
<td>0.110</td>
<td>668.484</td>
<td>12</td>
<td>train</td>
<td>04:09</td>
</tr>
<tr>
<td>0.097</td>
<td>1374.863</td>
<td>12</td>
<td>eval</td>
<td>01:01</td>
</tr>
<tr>
<td>0.104</td>
<td>661.353</td>
<td>13</td>
<td>train</td>
<td>04:12</td>
</tr>
<tr>
<td>0.096</td>
<td>1368.327</td>
<td>13</td>
<td>eval</td>
<td>01:00</td>
</tr>
<tr>
<td>0.104</td>
<td>654.398</td>
<td>14</td>
<td>train</td>
<td>04:11</td>
</tr>
<tr>
<td>0.098</td>
<td>1362.825</td>
<td>14</td>
<td>eval</td>
<td>01:01</td>
</tr>
<tr>
<td>0.104</td>
<td>649.071</td>
<td>15</td>
<td>train</td>
<td>04:11</td>
</tr>
<tr>
<td>0.093</td>
<td>1358.246</td>
<td>15</td>
<td>eval</td>
<td>01:01</td>
</tr>
<tr>
<td>0.101</td>
<td>646.394</td>
<td>16</td>
<td>train</td>
<td>04:10</td>
</tr>
<tr>
<td>0.096</td>
<td>1356.344</td>
<td>16</td>
<td>eval</td>
<td>01:01</td>
</tr>
<tr>
<td>0.106</td>
<td>644.981</td>
<td>17</td>
<td>train</td>
<td>04:08</td>
</tr>
<tr>
<td>0.096</td>
<td>1351.769</td>
<td>17</td>
<td>eval</td>
<td>01:01</td>
</tr>
<tr>
<td>0.104</td>
<td>642.134</td>
<td>18</td>
<td>train</td>
<td>04:09</td>
</tr>
<tr>
<td>0.097</td>
<td>1352.166</td>
<td>18</td>
<td>eval</td>
<td>01:02</td>
</tr>
<tr>
<td>0.102</td>
<td>641.413</td>
<td>19</td>
<td>train</td>
<td>04:09</td>
</tr>
<tr>
<td>0.090</td>
<td>1348.824</td>
<td>19</td>
<td>eval</td>
<td>01:01</td>
</tr>
</tbody>
</table>

![](54_yolov1_miniai_files/figure-commonmark/cell-18-output-3.png)

## Testing

Letâ€™s see if the model learned about bounding boxes for the training
set.

Loading the weight from previous one:

``` python
# learn.opt_func = torch.optim.Adam(
#     learn.model.parameters(), lr=lr, weight_decay=0
# )

load_checkpoint(torch.load("yolov1_1e-6_20.pth.tar", map_location=torch.device('cpu'), weights_only=True),
                learn.model, learn.opt)
```

    => Loading checkpoint

``` python
learn.model.train(False)
for i in range(8):
    x0, y0 = trn_ds[i]
    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))
    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')
    
    compare_ims(plot_image(x0, bboxes), plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0]))
```

![](54_yolov1_miniai_files/figure-commonmark/cell-20-output-1.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-20-output-2.png)

    TypeError: Image data of dtype object cannot be converted to float
    [31m---------------------------------------------------------------------------[39m
    [31mTypeError[39m                                 Traceback (most recent call last)
    [36mCell[39m[36m [39m[32mIn[19][39m[32m, line 7[39m
    [32m      4[39m bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze([32m0[39m).to(DEVICE)))
    [32m      5[39m bboxes = non_max_suppression(bboxes[[32m0[39m], iou_threshold=[32m0.5[39m, threshold=[32m0.4[39m, box_format=[33m'[39m[33mmidpoint[39m[33m'[39m)
    [32m----> [39m[32m7[39m [43mcompare_ims[49m[43m([49m[43mplot_image[49m[43m([49m[43mx0[49m[43m,[49m[43m [49m[43mbboxes[49m[43m)[49m[43m,[49m[43m [49m[43mplot_image[49m[43m([49m[43mx0[49m[43m,[49m[43m [49m[43mcellboxes_to_boxes[49m[43m([49m[43my0[49m[43m.[49m[43munsqueeze[49m[43m([49m[32;43m0[39;49m[43m)[49m[43m)[49m[43m[[49m[32;43m0[39;49m[43m][49m[43m)[49m[43m)[49m

    [36mFile [39m[32m~/git/pilus_project/pilus_project/core.py:119[39m, in [36mcompare_ims[39m[34m(img1, img2)[39m
    [32m    117[39m plt.figure(figsize=([32m12[39m,[32m6[39m))
    [32m    118[39m plt.subplot([32m121[39m)
    [32m--> [39m[32m119[39m [43mplt[49m[43m.[49m[43mimshow[49m[43m([49m[43mimg1[49m[43m,[49m[43m [49m[43mcmap[49m[43m=[49m[33;43m'[39;49m[33;43mgray[39;49m[33;43m'[39;49m[43m)[49m
    [32m    120[39m plt.title([33m'[39m[33mimg1[39m[33m'[39m)
    [32m    121[39m plt.axis([33m'[39m[33moff[39m[33m'[39m)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/pyplot.py:3592[39m, in [36mimshow[39m[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)[39m
    [32m   3570[39m [38;5;129m@_copy_docstring_and_deprecators[39m(Axes.imshow)
    [32m   3571[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mimshow[39m(
    [32m   3572[39m     X: ArrayLike | PIL.Image.Image,
    [32m   (...)[39m[32m   3590[39m     **kwargs,
    [32m   3591[39m ) -> AxesImage:
    [32m-> [39m[32m3592[39m     __ret = [43mgca[49m[43m([49m[43m)[49m[43m.[49m[43mimshow[49m[43m([49m
    [32m   3593[39m [43m        [49m[43mX[49m[43m,[49m
    [32m   3594[39m [43m        [49m[43mcmap[49m[43m=[49m[43mcmap[49m[43m,[49m
    [32m   3595[39m [43m        [49m[43mnorm[49m[43m=[49m[43mnorm[49m[43m,[49m
    [32m   3596[39m [43m        [49m[43maspect[49m[43m=[49m[43maspect[49m[43m,[49m
    [32m   3597[39m [43m        [49m[43minterpolation[49m[43m=[49m[43minterpolation[49m[43m,[49m
    [32m   3598[39m [43m        [49m[43malpha[49m[43m=[49m[43malpha[49m[43m,[49m
    [32m   3599[39m [43m        [49m[43mvmin[49m[43m=[49m[43mvmin[49m[43m,[49m
    [32m   3600[39m [43m        [49m[43mvmax[49m[43m=[49m[43mvmax[49m[43m,[49m
    [32m   3601[39m [43m        [49m[43mcolorizer[49m[43m=[49m[43mcolorizer[49m[43m,[49m
    [32m   3602[39m [43m        [49m[43morigin[49m[43m=[49m[43morigin[49m[43m,[49m
    [32m   3603[39m [43m        [49m[43mextent[49m[43m=[49m[43mextent[49m[43m,[49m
    [32m   3604[39m [43m        [49m[43minterpolation_stage[49m[43m=[49m[43minterpolation_stage[49m[43m,[49m
    [32m   3605[39m [43m        [49m[43mfilternorm[49m[43m=[49m[43mfilternorm[49m[43m,[49m
    [32m   3606[39m [43m        [49m[43mfilterrad[49m[43m=[49m[43mfilterrad[49m[43m,[49m
    [32m   3607[39m [43m        [49m[43mresample[49m[43m=[49m[43mresample[49m[43m,[49m
    [32m   3608[39m [43m        [49m[43murl[49m[43m=[49m[43murl[49m[43m,[49m
    [32m   3609[39m [43m        [49m[43m*[49m[43m*[49m[43m([49m[43m{[49m[33;43m"[39;49m[33;43mdata[39;49m[33;43m"[39;49m[43m:[49m[43m [49m[43mdata[49m[43m}[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mdata[49m[43m [49m[38;5;129;43;01mis[39;49;00m[43m [49m[38;5;129;43;01mnot[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[43m{[49m[43m}[49m[43m)[49m[43m,[49m
    [32m   3610[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
    [32m   3611[39m [43m    [49m[43m)[49m
    [32m   3612[39m     sci(__ret)
    [32m   3613[39m     [38;5;28;01mreturn[39;00m __ret

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/__init__.py:1521[39m, in [36m_preprocess_data.<locals>.inner[39m[34m(ax, data, *args, **kwargs)[39m
    [32m   1518[39m [38;5;129m@functools[39m.wraps(func)
    [32m   1519[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34minner[39m(ax, *args, data=[38;5;28;01mNone[39;00m, **kwargs):
    [32m   1520[39m     [38;5;28;01mif[39;00m data [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [32m-> [39m[32m1521[39m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m
    [32m   1522[39m [43m            [49m[43max[49m[43m,[49m
    [32m   1523[39m [43m            [49m[43m*[49m[38;5;28;43mmap[39;49m[43m([49m[43mcbook[49m[43m.[49m[43msanitize_sequence[49m[43m,[49m[43m [49m[43margs[49m[43m)[49m[43m,[49m
    [32m   1524[39m [43m            [49m[43m*[49m[43m*[49m[43m{[49m[43mk[49m[43m:[49m[43m [49m[43mcbook[49m[43m.[49m[43msanitize_sequence[49m[43m([49m[43mv[49m[43m)[49m[43m [49m[38;5;28;43;01mfor[39;49;00m[43m [49m[43mk[49m[43m,[49m[43m [49m[43mv[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[43mkwargs[49m[43m.[49m[43mitems[49m[43m([49m[43m)[49m[43m}[49m[43m)[49m
    [32m   1526[39m     bound = new_sig.bind(ax, *args, **kwargs)
    [32m   1527[39m     auto_label = (bound.arguments.get(label_namer)
    [32m   1528[39m                   [38;5;129;01mor[39;00m bound.kwargs.get(label_namer))

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5945[39m, in [36mAxes.imshow[39m[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)[39m
    [32m   5942[39m [38;5;28;01mif[39;00m aspect [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
    [32m   5943[39m     [38;5;28mself[39m.set_aspect(aspect)
    [32m-> [39m[32m5945[39m [43mim[49m[43m.[49m[43mset_data[49m[43m([49m[43mX[49m[43m)[49m
    [32m   5946[39m im.set_alpha(alpha)
    [32m   5947[39m [38;5;28;01mif[39;00m im.get_clip_path() [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [32m   5948[39m     [38;5;66;03m# image does not already have clipping set, clip to Axes patch[39;00m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/image.py:675[39m, in [36m_ImageBase.set_data[39m[34m(self, A)[39m
    [32m    673[39m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(A, PIL.Image.Image):
    [32m    674[39m     A = pil_to_array(A)  [38;5;66;03m# Needed e.g. to apply png palette.[39;00m
    [32m--> [39m[32m675[39m [38;5;28mself[39m._A = [38;5;28;43mself[39;49m[43m.[49m[43m_normalize_image_array[49m[43m([49m[43mA[49m[43m)[49m
    [32m    676[39m [38;5;28mself[39m._imcache = [38;5;28;01mNone[39;00m
    [32m    677[39m [38;5;28mself[39m.stale = [38;5;28;01mTrue[39;00m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/image.py:638[39m, in [36m_ImageBase._normalize_image_array[39m[34m(A)[39m
    [32m    636[39m A = cbook.safe_masked_invalid(A, copy=[38;5;28;01mTrue[39;00m)
    [32m    637[39m [38;5;28;01mif[39;00m A.dtype != np.uint8 [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m np.can_cast(A.dtype, [38;5;28mfloat[39m, [33m"[39m[33msame_kind[39m[33m"[39m):
    [32m--> [39m[32m638[39m     [38;5;28;01mraise[39;00m [38;5;167;01mTypeError[39;00m([33mf[39m[33m"[39m[33mImage data of dtype [39m[38;5;132;01m{[39;00mA.dtype[38;5;132;01m}[39;00m[33m cannot be [39m[33m"[39m
    [32m    639[39m                     [33mf[39m[33m"[39m[33mconverted to float[39m[33m"[39m)
    [32m    640[39m [38;5;28;01mif[39;00m A.ndim == [32m3[39m [38;5;129;01mand[39;00m A.shape[-[32m1[39m] == [32m1[39m:
    [32m    641[39m     A = A.squeeze(-[32m1[39m)  [38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.[39;00m

    [31mTypeError[39m: Image data of dtype object cannot be converted to float

![](54_yolov1_miniai_files/figure-commonmark/cell-20-output-4.png)

``` python
learn.model.train(False)
for i in range(8):
    x0, y0 = val_ds[i]
    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))
    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')
    plot_image(x0, bboxes)
    plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])
```

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-1.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-2.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-3.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-4.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-5.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-6.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-7.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-8.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-9.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-10.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-11.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-12.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-13.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-14.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-15.png)

![](54_yolov1_miniai_files/figure-commonmark/cell-21-output-16.png)

That doesnâ€™t look that bad! Letâ€™s save the model states.

``` python
checkpoint = {
   "state_dict": learn.model.state_dict(),
   "optimizer": learn.opt.state_dict(),
}
torch.save(checkpoint, "yolov1_1e-6_20.pth.tar")
```
