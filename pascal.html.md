# Pascal Darknet Classification


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

``` python
```

    The autoreload extension is already loaded. To reload it, use:
      %reload_ext autoreload

## Data

### Data loading

``` python
set_seed(42)
```

Let’s take a look at VOC2007.

``` python
data_path = fc.Path.home()/'data/'
data_path.ls()
```

    (#3) [Path('/home/kappa/data/VOCtrainval_06-Nov-2007.tar'),Path('/home/kappa/data/VOCdevkit'),Path('/home/kappa/data/pili')]

``` python
ds = datasets.VOCDetection(root=data_path, year='2007', image_set='train', download=False)
ds
```

    Dataset VOCDetection
        Number of datapoints: 2501
        Root location: /home/kappa/data

### Checking out data

What’s in the data?

``` python
ds[0]
```

    (<PIL.Image.Image image mode=RGB size=500x333>,
     {'annotation': {'folder': 'VOC2007',
       'filename': '000012.jpg',
       'source': {'database': 'The VOC2007 Database',
        'annotation': 'PASCAL VOC2007',
        'image': 'flickr',
        'flickrid': '207539885'},
       'owner': {'flickrid': 'KevBow', 'name': '?'},
       'size': {'width': '500', 'height': '333', 'depth': '3'},
       'segmented': '0',
       'object': [{'name': 'car',
         'pose': 'Rear',
         'truncated': '0',
         'difficult': '0',
         'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}})

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L46"
target="_blank" style="float:right; font-size:smaller">source</a>

### show_voc_sample

>  show_voc_sample (ds, idx, figsize=(12, 10))

``` python
# set_seed(42)
# import random
# random_indices = random.sample(range(len(ds)), 5)
# for idx in random_indices:
#     show_voc_sample(ds, idx, figsize=(5,5))
```

![](51_pascal_files/figure-commonmark/cell-8-output-1.png)

    Image size: 500x333
    Number of objects: 1
    Object 1: aeroplane, Difficult: 0, Truncated: 0

![](51_pascal_files/figure-commonmark/cell-8-output-3.png)

    Image size: 332x500
    Number of objects: 2
    Object 1: person, Difficult: 0, Truncated: 0
    Object 2: person, Difficult: 0, Truncated: 0

![](51_pascal_files/figure-commonmark/cell-8-output-5.png)

    Image size: 500x375
    Number of objects: 5
    Object 1: person, Difficult: 0, Truncated: 1
    Object 2: bottle, Difficult: 0, Truncated: 1
    Object 3: bottle, Difficult: 0, Truncated: 1
    Object 4: person, Difficult: 0, Truncated: 1
    Object 5: person, Difficult: 0, Truncated: 1

![](51_pascal_files/figure-commonmark/cell-8-output-7.png)

    Image size: 500x333
    Number of objects: 1
    Object 1: tvmonitor, Difficult: 0, Truncated: 0

![](51_pascal_files/figure-commonmark/cell-8-output-9.png)

    Image size: 500x281
    Number of objects: 2
    Object 1: car, Difficult: 0, Truncated: 0
    Object 2: car, Difficult: 0, Truncated: 0

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L75"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_class_distribution

>  get_class_distribution (ds)

*Get distribution of classes in the dataset*

``` python
class_dist = get_class_distribution(ds)
plt.figure(figsize=(12, 6))
class_dist.plot(kind='bar')
plt.title('Class Distribution in VOC2007')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
```

![](51_pascal_files/figure-commonmark/cell-10-output-1.png)

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L86"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_image_sizes

>  get_image_sizes (ds, n=100)

*Get distribution of image sizes in the dataset*

``` python
sizes = get_image_sizes(ds)
plt.figure(figsize=(10, 6))
plt.scatter(sizes['width'], sizes['height'], alpha=0.5)
plt.title('Image Dimensions')
plt.xlabel('Width')
plt.ylabel('Height')
plt.grid(True, alpha=0.3)
```

![](51_pascal_files/figure-commonmark/cell-12-output-1.png)

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L95"
target="_blank" style="float:right; font-size:smaller">source</a>

### show_class_examples

>  show_class_examples (ds, class_name, n=4)

*Show examples of a specific class*

``` python
show_class_examples(ds, 'cat');
```

![](51_pascal_files/figure-commonmark/cell-14-output-1.png)

``` python
objects_per_image = [len(ds[i][1]['annotation']['object']) for i in range(len(ds))]
plt.figure(figsize=(10, 6))
plt.hist(objects_per_image, bins=10)
plt.title('Objects per Image')
plt.xlabel('Number of Objects')
plt.ylabel('Number of Images')
```

    Text(0, 0.5, 'Number of Images')

![](51_pascal_files/figure-commonmark/cell-15-output-2.png)

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L124"
target="_blank" style="float:right; font-size:smaller">source</a>

### calculate_dataset_stats

>  calculate_dataset_stats (dataloader, max_images=None)

\*Calculate mean and std of a dataset using a dataloader.

Args: dataloader: DataLoader instance max_images: Maximum number of
images to use (None = use all)

Returns: mean and std per channel\*

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L166"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_stats_dataloader

>  get_stats_dataloader (data_path, bs=32, year='2007')

*Create a dataloader for calculating dataset statistics*

``` python
stats_dl = get_stats_dataloader(data_path, bs=32)

mean, std = calculate_dataset_stats(stats_dl, max_images=2500)

print(f"Dataset mean: {mean.tolist()}")
print(f"Dataset std: {std.tolist()}")
```

      0%|          | 0/79 [00:00<?, ?it/s]

    Dataset mean: [0.45178133249282837, 0.4230543076992035, 0.39004892110824585]
    Dataset std: [0.26676368713378906, 0.261764258146286, 0.2731017470359802]

### Dataset

We create pytorch dataset.

Pytorch has options to add transforms to its dataset, so this is like
`minai`’s `TfmDataset`.

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L196"
target="_blank" style="float:right; font-size:smaller">source</a>

### create_voc_datasets

>  create_voc_datasets (data_path, train_tfms=None, valid_tfms=None,
>                           year='2007')

*Create training and validation datasets for VOC*

``` python
trn_ds, val_ds = create_voc_datasets(data_path)
trn_ds
```

    Dataset VOCDetection
        Number of datapoints: 2501
        Root location: /home/kappa/data
        StandardTransform
    Transform: Compose(
                     RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=InterpolationMode.BILINEAR, antialias=True)
                     RandomHorizontalFlip(p=0.5)
                     ToImage()
                     ToDtype(scale=True)
                     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)
               )

The target has many more information than we need. We only need
`annotation.object`’s names for classification purposes.

``` python
trn_ds[0]
```

    (Image([[[-0.8678, -0.9020, -0.8678,  ..., -1.0390, -1.0733, -1.1418],
             [-0.9020, -0.8849, -0.8849,  ..., -1.1247, -1.1418, -1.1589],
             [-0.8849, -0.9192, -0.9020,  ..., -1.2103, -1.2103, -1.1418],
             ...,
             [-0.4739, -0.4911, -0.5253,  ..., -0.7650, -0.7308, -0.7479],
             [-0.5938, -0.5253, -0.5767,  ..., -0.7650, -0.7137, -0.7479],
             [-0.6452, -0.5938, -0.5596,  ..., -0.7993, -0.7822, -0.7822]],
     
            [[-0.7577, -0.7927, -0.7577,  ..., -0.9328, -0.9678, -1.0378],
             [-0.7927, -0.7752, -0.7752,  ..., -1.0203, -1.0378, -1.0553],
             [-0.7752, -0.8102, -0.7927,  ..., -1.1078, -1.1078, -1.0378],
             ...,
             [-0.3725, -0.3901, -0.4251,  ..., -0.6352, -0.6001, -0.6176],
             [-0.4951, -0.4251, -0.4776,  ..., -0.6352, -0.5826, -0.6176],
             [-0.5476, -0.4951, -0.4601,  ..., -0.6702, -0.6527, -0.6527]],
     
            [[-0.5321, -0.5670, -0.5321,  ..., -0.7413, -0.7761, -0.8458],
             [-0.5670, -0.5495, -0.5495,  ..., -0.8284, -0.8458, -0.8633],
             [-0.5495, -0.5844, -0.5670,  ..., -0.9156, -0.9156, -0.8458],
             ...,
             [-0.1835, -0.2010, -0.2358,  ..., -0.3753, -0.3404, -0.3578],
             [-0.3055, -0.2358, -0.2881,  ..., -0.3753, -0.3230, -0.3578],
             [-0.3578, -0.3055, -0.2707,  ..., -0.4101, -0.3927, -0.3927]]], ),
     {'annotation': {'folder': 'VOC2007',
       'filename': '000012.jpg',
       'source': {'database': 'The VOC2007 Database',
        'annotation': 'PASCAL VOC2007',
        'image': 'flickr',
        'flickrid': '207539885'},
       'owner': {'flickrid': 'KevBow', 'name': '?'},
       'size': {'width': '500', 'height': '333', 'depth': '3'},
       'segmented': '0',
       'object': [{'name': 'car',
         'pose': 'Rear',
         'truncated': '0',
         'difficult': '0',
         'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}})

With
[`voc_extract`](https://galopyz.github.io/pilus_project/pascal.html#voc_extract),
we can get any field we want from the target.

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L221"
target="_blank" style="float:right; font-size:smaller">source</a>

### voc_extract

>  voc_extract (field='name')

*Create a function that extracts a specific field from VOC annotations*

Object name:

``` python
ds = datasets.VOCDetection(
    root=data_path, year="2007", image_set='train', download=False, 
    target_transform=voc_extract())
ds[0]
```

    (<PIL.Image.Image image mode=RGB size=500x333>, (#1) ['car'])

Bound box:

``` python
ds = datasets.VOCDetection(
    root=data_path, year="2007", image_set='train', download=False, 
    target_transform=voc_extract(field='bndbox'))
ds[0]
```

    (<PIL.Image.Image image mode=RGB size=500x333>,
     (#1) [{'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}])

For training, we actually need one-hot encoded vector because the
targets are multi-labels.

``` python
VOC_CLASSES
```

    (#20) ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']

``` python
names = ['car', 'dog']
names
```

    ['car', 'dog']

``` python
lbls = torch.zeros(len(VOC_CLASSES))
lbls
```

    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

`torch.scatter` is a good way to do this:

``` python
onehot = lbls.scatter(0, torch.tensor([1,3,5]), 1)
onehot
```

    tensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0.])

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L228"
target="_blank" style="float:right; font-size:smaller">source</a>

### onehot_tfm

>  onehot_tfm (targ, clss=['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
>                  'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
>                  'horse', 'motorbike', 'person', 'pottedplant', 'sheep',
>                  'sofa', 'train', 'tvmonitor'])

``` python
ds = datasets.VOCDetection(
    root=data_path, year="2007", image_set='train', download=False, 
    target_transform=onehot_tfm)
ds[0]
```

    (<PIL.Image.Image image mode=RGB size=500x333>,
     tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0.]))

How about going back to label from one hot encoding? We use `np.where`.
Why use numpy instead of pytorch? Because `rvs_onehot_tfm` is used for
displaying images. We will never use this during training.

``` python
onehot
```

    tensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0.])

``` python
np.where(onehot == 1)[0]
```

    array([1, 3, 5])

``` python
VOC_CLASSES[np.where(onehot == 1)[0]]
```

    (#3) ['bicycle','boat','bus']

``` python
_rvs_onehot_tfm(onehot)
```

    (#3) ['bicycle','boat','bus']

### DataLoader

We got the dataset, so we are ready to create a dataloader. There are
couple transformations we want to apply to images. We have images so
far, but we need pytorch tensors with the same image sizes. We also
normalize images using imagenet statistics.

``` python
to_tensor = v2.Compose([
    v2.Resize((224, 224)),
    v2.ToImage(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

``` python
trn_ds = datasets.VOCDetection(
    root=data_path, year="2007", image_set='train', download=False, 
    transform=to_tensor, target_transform=onehot_tfm)
val_ds = datasets.VOCDetection(
    root=data_path, year="2007", image_set='val', download=False, 
    transform=to_tensor, target_transform=onehot_tfm)
```

``` python
bs = 64
multi_label_loss = nn.BCEWithLogitsLoss()

trn_dl, val_dl = get_dls(trn_ds, val_ds, bs=bs)
```

``` python
xb,yb = next(iter(trn_dl))
xb.shape,yb[:10]
```

    (torch.Size([64, 3, 224, 224]),
     tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
              0., 0.],
             [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
              0., 0.],
             [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
              0., 0.],
             [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
              0., 0.],
             [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
              0., 0.],
             [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
              0., 0.],
             [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
              0., 0.],
             [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
              0., 0.],
             [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
              0., 0.],
             [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
              0., 1.]]))

Denormalize image before display.

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L243"
target="_blank" style="float:right; font-size:smaller">source</a>

### denorm

>  denorm (x)

``` python
show_image(xb[0], tfm_x=denorm);
```

![](51_pascal_files/figure-commonmark/cell-40-output-1.png)

------------------------------------------------------------------------

<a
href="https://github.com/galopyz/pilus_project/blob/main/pilus_project/pascal.py#L246"
target="_blank" style="float:right; font-size:smaller">source</a>

### get_classification_model

>  get_classification_model (num_classes=20)

*Create a multi-label classification model based on darknet19*

``` python
model = get_classification_model()
dls = DataLoaders(trn_dl, val_dl)
learn = TrainLearner(model, dls, multi_label_loss, lr=1e-3, 
                   cbs=[TrainCB(), DeviceCB(), ProgressCB(), MetricsCB()])
learn.summary()
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

    <div>
      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/1 00:00&lt;?]
    </div>
    &#10;
&#10;    <div>
      <progress value='0' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/40 00:00&lt;?]
    </div>
    &#10;

    Tot params: 20359636; MFLOPS: 970.9

<table>
<thead>
<tr>
<th>Module</th>
<th>Input</th>
<th>Output</th>
<th>Num params</th>
<th>MFLOPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sequential</td>
<td>(64, 3, 224, 224)</td>
<td>(64, 1024, 7, 7)</td>
<td>19824576</td>
<td>970.4</td>
</tr>
<tr>
<td>AdaptiveAvgPool2d</td>
<td>(64, 1024, 7, 7)</td>
<td>(64, 1024, 1, 1)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Flatten</td>
<td>(64, 1024, 1, 1)</td>
<td>(64, 1024)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Linear</td>
<td>(64, 1024)</td>
<td>(64, 512)</td>
<td>524800</td>
<td>0.5</td>
</tr>
<tr>
<td>ReLU</td>
<td>(64, 512)</td>
<td>(64, 512)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Dropout</td>
<td>(64, 512)</td>
<td>(64, 512)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Linear</td>
<td>(64, 512)</td>
<td>(64, 20)</td>
<td>10260</td>
<td>0.0</td>
</tr>
</tbody>
</table>

We also have to reverse the transform for the targets. It is in onehot
encoding, but we want class names.

``` python
yb
```

    tensor([[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 1.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 1.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 1., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]])

``` python
[', '.join(_rvs_onehot_tfm(y)) for y in np.array(yb)][:4]
```

    ['person', 'chair, person', 'bird', 'dog, person, sofa']

``` python
def rvs_onehot_tfm(yb): return [', '.join(_rvs_onehot_tfm(y)) for y in np.array(yb)]
```

``` python
import sys, gc, traceback, math, typing, random, numpy as np
from itertools import zip_longest
```

We want to transform x and y.

``` python
learn.show_image_batch(tfm_x=denorm, tfm_y=_rvs_onehot_tfm)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

    <div>
      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/1 00:00&lt;?]
    </div>
    &#10;
&#10;    <div>
      <progress value='0' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/40 00:00&lt;?]
    </div>
    &#10;

![](51_pascal_files/figure-commonmark/cell-47-output-3.png)

``` python
learn.lr_find(gamma=1.4, max_mult=2)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<div>
      <progress value='1' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>
      10.00% [1/10 00:13&lt;02:04]
    </div>
    &#10;

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: left;">
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.455</td>
<td>0</td>
<td>train</td>
<td>00:13</td>
</tr>
</tbody>
</table>

<p>
&#10;    <div>
      <progress value='0' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/40 00:00&lt;?]
    </div>

![](51_pascal_files/figure-commonmark/cell-48-output-3.png)

## Training Classification

``` python
model = get_classification_model()
learn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, 
                   cbs=[TrainCB(), DeviceCB(), ProgressCB(), MetricsCB()])
```

``` python
learn.summary()
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

    <div>
      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/1 00:00&lt;?]
    </div>
    &#10;
&#10;    <div>
      <progress value='0' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/40 00:00&lt;?]
    </div>
    &#10;

    Tot params: 20359636; MFLOPS: 970.9

<table>
<thead>
<tr>
<th>Module</th>
<th>Input</th>
<th>Output</th>
<th>Num params</th>
<th>MFLOPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sequential</td>
<td>(64, 3, 224, 224)</td>
<td>(64, 1024, 7, 7)</td>
<td>19824576</td>
<td>970.4</td>
</tr>
<tr>
<td>AdaptiveAvgPool2d</td>
<td>(64, 1024, 7, 7)</td>
<td>(64, 1024, 1, 1)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Flatten</td>
<td>(64, 1024, 1, 1)</td>
<td>(64, 1024)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Linear</td>
<td>(64, 1024)</td>
<td>(64, 512)</td>
<td>524800</td>
<td>0.5</td>
</tr>
<tr>
<td>ReLU</td>
<td>(64, 512)</td>
<td>(64, 512)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Dropout</td>
<td>(64, 512)</td>
<td>(64, 512)</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr>
<td>Linear</td>
<td>(64, 512)</td>
<td>(64, 20)</td>
<td>10260</td>
<td>0.0</td>
</tr>
</tbody>
</table>

``` python
model = get_classification_model()
learn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, 
                   cbs=[DeviceCB(), ProgressCB(), MetricsCB()])
learn.fit(3)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: left;">
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.371</td>
<td>0</td>
<td>train</td>
<td>00:13</td>
</tr>
<tr>
<td>0.261</td>
<td>0</td>
<td>eval</td>
<td>00:54</td>
</tr>
<tr>
<td>0.248</td>
<td>1</td>
<td>train</td>
<td>00:13</td>
</tr>
<tr>
<td>0.237</td>
<td>1</td>
<td>eval</td>
<td>00:11</td>
</tr>
<tr>
<td>0.241</td>
<td>2</td>
<td>train</td>
<td>00:13</td>
</tr>
<tr>
<td>0.237</td>
<td>2</td>
<td>eval</td>
<td>00:11</td>
</tr>
</tbody>
</table>

``` python
class TopKAccuracy(Callback):
    def __init__(self, k_values=[1, 5], class_names=VOC_CLASSES):
        """
        Implements Top-K accuracy for multi-label classification
        
        Args:
            k_values: List of k values to compute (e.g., [1, 5] for top-1 and top-5)
            class_names: List of class names
        """
        self.k_values = sorted(k_values)
        self.max_k = max(k_values)
        self.class_names = class_names
        
    def before_fit(self, learn):
        self.learn = learn
        
    def before_epoch(self, learn):
        # Initialize counters for each k
        self.correct = {k: 0 for k in self.k_values}
        self.total = 0
        
    def after_batch(self, learn):
        # Get predictions and targets
        logits = to_cpu(learn.preds)
        targets = to_cpu(learn.batch[1])
        batch_size = targets.size(0)
        
        # For each image in the batch
        for i in range(batch_size):
            # Get ground truth classes for this image
            true_classes = torch.where(targets[i] == 1)[0]
            if len(true_classes) == 0:
                continue  # Skip images with no labels
                
            # Get top-k predicted classes
            _, top_indices = torch.topk(logits[i], min(self.max_k, len(self.class_names)))
            
            # Check if any true class is in top-k predictions
            for k in self.k_values:
                top_k_indices = top_indices[:k]
                # For multi-label: if any true class is in top-k predictions, count as correct
                if any(cls in top_k_indices for cls in true_classes):
                    self.correct[k] += 1
            
            self.total += 1
        
    def after_epoch(self, learn):
        phase = 'train' if learn.training else 'valid'
        for k in self.k_values:
            accuracy = self.correct[k] / self.total if self.total > 0 else 0
            print(f"{phase} top-{k} accuracy: {accuracy:.4f}")
```

``` python
# Alternative implementation that considers a prediction correct only if 
# all true classes are in the top-k predictions
class StrictTopKAccuracy(Callback):
    def __init__(self, k_values=[1, 5], class_names=VOC_CLASSES):
        self.k_values = sorted(k_values)
        self.max_k = max(k_values)
        self.class_names = class_names
        
    def before_fit(self, learn):
        self.learn = learn
        
    def before_epoch(self, learn):
        self.correct = {k: 0 for k in self.k_values}
        self.total = 0
        
    def after_batch(self, learn):
        logits = to_cpu(learn.preds)
        targets = to_cpu(learn.batch[1])
        batch_size = targets.size(0)
        
        for i in range(batch_size):
            true_classes = torch.where(targets[i] == 1)[0]
            if len(true_classes) == 0:
                continue
                
            _, top_indices = torch.topk(logits[i], min(self.max_k, len(self.class_names)))
            
            for k in self.k_values:
                if k < len(true_classes):
                    continue  # Can't fit all true classes in top-k if k < number of true classes
                    
                top_k_indices = set(top_indices[:k].tolist())
                true_classes_set = set(true_classes.tolist())
                
                # Strict version: all true classes must be in top-k predictions
                if true_classes_set.issubset(top_k_indices):
                    self.correct[k] += 1
            
            self.total += 1
        
    def after_epoch(self, learn):
        phase = 'train' if learn.training else 'valid'
        for k in self.k_values:
            accuracy = self.correct[k] / self.total if self.total > 0 else 0
            print(f"{phase} strict top-{k} accuracy: {accuracy:.4f}")
```

``` python
model = get_classification_model()
learn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, 
                   cbs=[DeviceCB(), ProgressCB(), MetricsCB(top5=TopKMultilabelAccuracy(k=5)), 
                        TopKAccuracy(k_values=[1, 5])])
```

``` python
learn.fit(3)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<div>
      <progress value='2' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>
      66.67% [2/3 00:49&lt;00:24]
    </div>
    &#10;

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: left;">
<th data-quarto-table-cell-role="th">top5</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.000</td>
<td>0.374</td>
<td>0</td>
<td>train</td>
<td>00:13</td>
</tr>
<tr>
<td>0.000</td>
<td>0.265</td>
<td>0</td>
<td>eval</td>
<td>00:11</td>
</tr>
<tr>
<td>0.000</td>
<td>0.247</td>
<td>1</td>
<td>train</td>
<td>00:13</td>
</tr>
<tr>
<td>0.000</td>
<td>0.238</td>
<td>1</td>
<td>eval</td>
<td>00:11</td>
</tr>
<tr>
<td>0.000</td>
<td>0.240</td>
<td>2</td>
<td>train</td>
<td>00:13</td>
</tr>
</tbody>
</table>

<p>
&#10;    <div>
      <progress value='10' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>
      50.00% [10/20 00:05&lt;00:05 0.240]
    </div>

    train top-1 accuracy: 0.3631
    train top-5 accuracy: 0.6234
    valid top-1 accuracy: 0.4084
    valid top-5 accuracy: 0.6657
    train top-1 accuracy: 0.4266
    train top-5 accuracy: 0.6745
    valid top-1 accuracy: 0.4084
    valid top-5 accuracy: 0.7032
    train top-1 accuracy: 0.4314
    train top-5 accuracy: 0.7245

    KeyboardInterrupt: 
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mKeyboardInterrupt[0m                         Traceback (most recent call last)
    Cell [0;32mIn[75], line 1[0m
    [0;32m----> 1[0m [43mlearn[49m[38;5;241;43m.[39;49m[43mfit[49m[43m([49m[38;5;241;43m3[39;49m[43m)[49m

    File [0;32m~/git/minai/minai/core.py:260[0m, in [0;36mLearner.fit[0;34m(self, n_epochs, train, valid, cbs, lr)[0m
    [1;32m    258[0m     [38;5;28;01mif[39;00m lr [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m: lr [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mlr
    [1;32m    259[0m     [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39mopt_func: [38;5;28mself[39m[38;5;241m.[39mopt [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mopt_func([38;5;28mself[39m[38;5;241m.[39mmodel[38;5;241m.[39mparameters(), lr)
    [0;32m--> 260[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_fit[49m[43m([49m[43mtrain[49m[43m,[49m[43m [49m[43mvalid[49m[43m)[49m
    [1;32m    261[0m [38;5;28;01mfinally[39;00m:
    [1;32m    262[0m     [38;5;28;01mfor[39;00m cb [38;5;129;01min[39;00m cbs: [38;5;28mself[39m[38;5;241m.[39mcbs[38;5;241m.[39mremove(cb)

    File [0;32m~/git/minai/minai/core.py:194[0m, in [0;36mwith_cbs.__call__.<locals>._f[0;34m(o, *args, **kwargs)[0m
    [1;32m    192[0m [38;5;28;01mtry[39;00m:
    [1;32m    193[0m     o[38;5;241m.[39mcallback([38;5;124mf[39m[38;5;124m'[39m[38;5;124mbefore_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m[38;5;241m.[39mnm[38;5;132;01m}[39;00m[38;5;124m'[39m)
    [0;32m--> 194[0m     [43mf[49m[43m([49m[43mo[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m    195[0m     o[38;5;241m.[39mcallback([38;5;124mf[39m[38;5;124m'[39m[38;5;124mafter_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m[38;5;241m.[39mnm[38;5;132;01m}[39;00m[38;5;124m'[39m)
    [1;32m    196[0m [38;5;28;01mexcept[39;00m [38;5;28mglobals[39m()[[38;5;124mf[39m[38;5;124m'[39m[38;5;124mCancel[39m[38;5;132;01m{[39;00m[38;5;28mself[39m[38;5;241m.[39mnm[38;5;241m.[39mtitle()[38;5;132;01m}[39;00m[38;5;124mException[39m[38;5;124m'[39m]: [38;5;28;01mpass[39;00m

    File [0;32m~/git/minai/minai/core.py:250[0m, in [0;36mLearner._fit[0;34m(self, train, valid)[0m
    [1;32m    248[0m [38;5;28;01mif[39;00m train: [38;5;28mself[39m[38;5;241m.[39mone_epoch([38;5;28;01mTrue[39;00m)
    [1;32m    249[0m [38;5;28;01mif[39;00m valid:
    [0;32m--> 250[0m     [38;5;28;01mwith[39;00m torch[38;5;241m.[39minference_mode(): [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mone_epoch[49m[43m([49m[38;5;28;43;01mFalse[39;49;00m[43m)[49m

    File [0;32m~/git/minai/minai/core.py:241[0m, in [0;36mLearner.one_epoch[0;34m(self, training)[0m
    [1;32m    239[0m [38;5;28mself[39m[38;5;241m.[39mmodel[38;5;241m.[39mtrain(training)
    [1;32m    240[0m [38;5;28mself[39m[38;5;241m.[39mdl [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mtrain_dl [38;5;28;01mif[39;00m training [38;5;28;01melse[39;00m [38;5;28mself[39m[38;5;241m.[39mdls[38;5;241m.[39mvalid
    [0;32m--> 241[0m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_one_epoch[49m[43m([49m[43m)[49m

    File [0;32m~/git/minai/minai/core.py:194[0m, in [0;36mwith_cbs.__call__.<locals>._f[0;34m(o, *args, **kwargs)[0m
    [1;32m    192[0m [38;5;28;01mtry[39;00m:
    [1;32m    193[0m     o[38;5;241m.[39mcallback([38;5;124mf[39m[38;5;124m'[39m[38;5;124mbefore_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m[38;5;241m.[39mnm[38;5;132;01m}[39;00m[38;5;124m'[39m)
    [0;32m--> 194[0m     [43mf[49m[43m([49m[43mo[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m    195[0m     o[38;5;241m.[39mcallback([38;5;124mf[39m[38;5;124m'[39m[38;5;124mafter_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m[38;5;241m.[39mnm[38;5;132;01m}[39;00m[38;5;124m'[39m)
    [1;32m    196[0m [38;5;28;01mexcept[39;00m [38;5;28mglobals[39m()[[38;5;124mf[39m[38;5;124m'[39m[38;5;124mCancel[39m[38;5;132;01m{[39;00m[38;5;28mself[39m[38;5;241m.[39mnm[38;5;241m.[39mtitle()[38;5;132;01m}[39;00m[38;5;124mException[39m[38;5;124m'[39m]: [38;5;28;01mpass[39;00m

    File [0;32m~/git/minai/minai/core.py:236[0m, in [0;36mLearner._one_epoch[0;34m(self)[0m
    [1;32m    234[0m [38;5;129m@with_cbs[39m([38;5;124m'[39m[38;5;124mepoch[39m[38;5;124m'[39m)
    [1;32m    235[0m [38;5;28;01mdef[39;00m [38;5;21m_one_epoch[39m([38;5;28mself[39m):
    [0;32m--> 236[0m     [38;5;28;01mfor[39;00m [38;5;28mself[39m[38;5;241m.[39miter,[38;5;28mself[39m[38;5;241m.[39mbatch [38;5;129;01min[39;00m [38;5;28menumerate[39m([38;5;28mself[39m[38;5;241m.[39mdl): [38;5;28mself[39m[38;5;241m.[39m_one_batch()

    File [0;32m~/miniforge3/lib/python3.10/site-packages/fastprogress/fastprogress.py:41[0m, in [0;36mProgressBar.__iter__[0;34m(self)[0m
    [1;32m     39[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39mtotal [38;5;241m!=[39m [38;5;241m0[39m: [38;5;28mself[39m[38;5;241m.[39mupdate([38;5;241m0[39m)
    [1;32m     40[0m [38;5;28;01mtry[39;00m:
    [0;32m---> 41[0m     [38;5;28;01mfor[39;00m i,o [38;5;129;01min[39;00m [38;5;28menumerate[39m([38;5;28mself[39m[38;5;241m.[39mgen):
    [1;32m     42[0m         [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39mtotal [38;5;129;01mand[39;00m i [38;5;241m>[39m[38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mtotal: [38;5;28;01mbreak[39;00m
    [1;32m     43[0m         [38;5;28;01myield[39;00m o

    File [0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701[0m, in [0;36m_BaseDataLoaderIter.__next__[0;34m(self)[0m
    [1;32m    698[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39m_sampler_iter [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [1;32m    699[0m     [38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)[39;00m
    [1;32m    700[0m     [38;5;28mself[39m[38;5;241m.[39m_reset()  [38;5;66;03m# type: ignore[call-arg][39;00m
    [0;32m--> 701[0m data [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_next_data[49m[43m([49m[43m)[49m
    [1;32m    702[0m [38;5;28mself[39m[38;5;241m.[39m_num_yielded [38;5;241m+[39m[38;5;241m=[39m [38;5;241m1[39m
    [1;32m    703[0m [38;5;28;01mif[39;00m (
    [1;32m    704[0m     [38;5;28mself[39m[38;5;241m.[39m_dataset_kind [38;5;241m==[39m _DatasetKind[38;5;241m.[39mIterable
    [1;32m    705[0m     [38;5;129;01mand[39;00m [38;5;28mself[39m[38;5;241m.[39m_IterableDataset_len_called [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
    [1;32m    706[0m     [38;5;129;01mand[39;00m [38;5;28mself[39m[38;5;241m.[39m_num_yielded [38;5;241m>[39m [38;5;28mself[39m[38;5;241m.[39m_IterableDataset_len_called
    [1;32m    707[0m ):

    File [0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757[0m, in [0;36m_SingleProcessDataLoaderIter._next_data[0;34m(self)[0m
    [1;32m    755[0m [38;5;28;01mdef[39;00m [38;5;21m_next_data[39m([38;5;28mself[39m):
    [1;32m    756[0m     index [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_next_index()  [38;5;66;03m# may raise StopIteration[39;00m
    [0;32m--> 757[0m     data [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_dataset_fetcher[49m[38;5;241;43m.[39;49m[43mfetch[49m[43m([49m[43mindex[49m[43m)[49m  [38;5;66;03m# may raise StopIteration[39;00m
    [1;32m    758[0m     [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39m_pin_memory:
    [1;32m    759[0m         data [38;5;241m=[39m _utils[38;5;241m.[39mpin_memory[38;5;241m.[39mpin_memory(data, [38;5;28mself[39m[38;5;241m.[39m_pin_memory_device)

    File [0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52[0m, in [0;36m_MapDatasetFetcher.fetch[0;34m(self, possibly_batched_index)[0m
    [1;32m     50[0m         data [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mdataset[38;5;241m.[39m__getitems__(possibly_batched_index)
    [1;32m     51[0m     [38;5;28;01melse[39;00m:
    [0;32m---> 52[0m         data [38;5;241m=[39m [[38;5;28mself[39m[38;5;241m.[39mdataset[idx] [38;5;28;01mfor[39;00m idx [38;5;129;01min[39;00m possibly_batched_index]
    [1;32m     53[0m [38;5;28;01melse[39;00m:
    [1;32m     54[0m     data [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mdataset[possibly_batched_index]

    File [0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52[0m, in [0;36m<listcomp>[0;34m(.0)[0m
    [1;32m     50[0m         data [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mdataset[38;5;241m.[39m__getitems__(possibly_batched_index)
    [1;32m     51[0m     [38;5;28;01melse[39;00m:
    [0;32m---> 52[0m         data [38;5;241m=[39m [[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mdataset[49m[43m[[49m[43midx[49m[43m][49m [38;5;28;01mfor[39;00m idx [38;5;129;01min[39;00m possibly_batched_index]
    [1;32m     53[0m [38;5;28;01melse[39;00m:
    [1;32m     54[0m     data [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mdataset[possibly_batched_index]

    File [0;32m~/miniforge3/lib/python3.10/site-packages/torchvision/datasets/voc.py:201[0m, in [0;36mVOCDetection.__getitem__[0;34m(self, index)[0m
    [1;32m    193[0m [38;5;250m[39m[38;5;124;03m"""[39;00m
    [1;32m    194[0m [38;5;124;03mArgs:[39;00m
    [1;32m    195[0m [38;5;124;03m    index (int): Index[39;00m
    [0;32m   (...)[0m
    [1;32m    198[0m [38;5;124;03m    tuple: (image, target) where target is a dictionary of the XML tree.[39;00m
    [1;32m    199[0m [38;5;124;03m"""[39;00m
    [1;32m    200[0m img [38;5;241m=[39m Image[38;5;241m.[39mopen([38;5;28mself[39m[38;5;241m.[39mimages[index])[38;5;241m.[39mconvert([38;5;124m"[39m[38;5;124mRGB[39m[38;5;124m"[39m)
    [0;32m--> 201[0m target [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mparse_voc_xml([43mET_parse[49m[43m([49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mannotations[49m[43m[[49m[43mindex[49m[43m][49m[43m)[49m[38;5;241m.[39mgetroot())
    [1;32m    203[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39mtransforms [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
    [1;32m    204[0m     img, target [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mtransforms(img, target)

    File [0;32m~/miniforge3/lib/python3.10/site-packages/defusedxml/common.py:100[0m, in [0;36m_generate_etree_functions.<locals>.parse[0;34m(source, parser, forbid_dtd, forbid_entities, forbid_external)[0m
    [1;32m     93[0m [38;5;28;01mif[39;00m parser [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [1;32m     94[0m     parser [38;5;241m=[39m DefusedXMLParser(
    [1;32m     95[0m         target[38;5;241m=[39m_TreeBuilder(),
    [1;32m     96[0m         forbid_dtd[38;5;241m=[39mforbid_dtd,
    [1;32m     97[0m         forbid_entities[38;5;241m=[39mforbid_entities,
    [1;32m     98[0m         forbid_external[38;5;241m=[39mforbid_external,
    [1;32m     99[0m     )
    [0;32m--> 100[0m [38;5;28;01mreturn[39;00m [43m_parse[49m[43m([49m[43msource[49m[43m,[49m[43m [49m[43mparser[49m[43m)[49m

    File [0;32m~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:1222[0m, in [0;36mparse[0;34m(source, parser)[0m
    [1;32m   1213[0m [38;5;250m[39m[38;5;124;03m"""Parse XML document into element tree.[39;00m
    [1;32m   1214[0m 
    [1;32m   1215[0m [38;5;124;03m*source* is a filename or file object containing XML data,[39;00m
    [0;32m   (...)[0m
    [1;32m   1219[0m 
    [1;32m   1220[0m [38;5;124;03m"""[39;00m
    [1;32m   1221[0m tree [38;5;241m=[39m ElementTree()
    [0;32m-> 1222[0m [43mtree[49m[38;5;241;43m.[39;49m[43mparse[49m[43m([49m[43msource[49m[43m,[49m[43m [49m[43mparser[49m[43m)[49m
    [1;32m   1223[0m [38;5;28;01mreturn[39;00m tree

    File [0;32m~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:586[0m, in [0;36mElementTree.parse[0;34m(self, source, parser)[0m
    [1;32m    584[0m     [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m data:
    [1;32m    585[0m         [38;5;28;01mbreak[39;00m
    [0;32m--> 586[0m     [43mparser[49m[38;5;241;43m.[39;49m[43mfeed[49m[43m([49m[43mdata[49m[43m)[49m
    [1;32m    587[0m [38;5;28mself[39m[38;5;241m.[39m_root [38;5;241m=[39m parser[38;5;241m.[39mclose()
    [1;32m    588[0m [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_root

    File [0;32m~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:1713[0m, in [0;36mXMLParser.feed[0;34m(self, data)[0m
    [1;32m   1711[0m [38;5;250m[39m[38;5;124;03m"""Feed encoded data to parser."""[39;00m
    [1;32m   1712[0m [38;5;28;01mtry[39;00m:
    [0;32m-> 1713[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mparser[49m[38;5;241;43m.[39;49m[43mParse[49m[43m([49m[43mdata[49m[43m,[49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m)[49m
    [1;32m   1714[0m [38;5;28;01mexcept[39;00m [38;5;28mself[39m[38;5;241m.[39m_error [38;5;28;01mas[39;00m v:
    [1;32m   1715[0m     [38;5;28mself[39m[38;5;241m.[39m_raiseerror(v)

    File [0;32m/home/conda/feedstock_root/build_artifacts/python-split_1687559129017/work/Modules/pyexpat.c:416[0m, in [0;36mStartElement[0;34m()[0m

    File [0;32m~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:1641[0m, in [0;36mXMLParser._start[0;34m(self, tag, attr_list)[0m
    [1;32m   1638[0m [38;5;28;01mdef[39;00m [38;5;21m_end_ns[39m([38;5;28mself[39m, prefix):
    [1;32m   1639[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39mtarget[38;5;241m.[39mend_ns(prefix [38;5;129;01mor[39;00m [38;5;124m'[39m[38;5;124m'[39m)
    [0;32m-> 1641[0m [38;5;28;01mdef[39;00m [38;5;21m_start[39m([38;5;28mself[39m, tag, attr_list):
    [1;32m   1642[0m     [38;5;66;03m# Handler for expat's StartElementHandler. Since ordered_attributes[39;00m
    [1;32m   1643[0m     [38;5;66;03m# is set, the attributes are reported as a list of alternating[39;00m
    [1;32m   1644[0m     [38;5;66;03m# attribute name,value.[39;00m
    [1;32m   1645[0m     fixname [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_fixname
    [1;32m   1646[0m     tag [38;5;241m=[39m fixname(tag)

    [0;31mKeyboardInterrupt[0m: 

``` python
model = get_classification_model()
learn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, 
                   cbs=[DeviceCB(), ProgressCB(), 
                        MetricsCB(mAP=MultilabelAUPRC(num_labels=20), hamming=MultilabelAccuracy(criteria='hamming'), overlap=MultilabelAccuracy(criteria='overlap'), contain=MultilabelAccuracy(criteria='contain'), belong=MultilabelAccuracy(criteria='belong'),top1=MultilabelAccuracy(criteria='exact_match'), top5=TopKMultilabelAccuracy(criteria='contain', k=5), ), 
                        TopKAccuracy(k_values=[1, 5]), 
                        StrictTopKAccuracy(k_values=[1, 5])])
```

``` python
learn.fit(5)
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<div>
      <progress value='2' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>
      40.00% [2/5 09:29&lt;14:14]
    </div>
    &#10;

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: left;">
<th data-quarto-table-cell-role="th">mAP</th>
<th data-quarto-table-cell-role="th">hamming</th>
<th data-quarto-table-cell-role="th">overlap</th>
<th data-quarto-table-cell-role="th">contain</th>
<th data-quarto-table-cell-role="th">belong</th>
<th data-quarto-table-cell-role="th">top1</th>
<th data-quarto-table-cell-role="th">top5</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.085</td>
<td>0.920</td>
<td>0.002</td>
<td>0.001</td>
<td>0.998</td>
<td>0.001</td>
<td>0.299</td>
<td>0.374</td>
<td>0</td>
<td>train</td>
<td>03:34</td>
</tr>
<tr>
<td>0.103</td>
<td>0.922</td>
<td>0.000</td>
<td>0.000</td>
<td>1.000</td>
<td>0.000</td>
<td>0.362</td>
<td>0.263</td>
<td>0</td>
<td>eval</td>
<td>01:20</td>
</tr>
<tr>
<td>0.099</td>
<td>0.920</td>
<td>0.006</td>
<td>0.001</td>
<td>0.995</td>
<td>0.001</td>
<td>0.355</td>
<td>0.249</td>
<td>1</td>
<td>train</td>
<td>03:20</td>
</tr>
<tr>
<td>0.149</td>
<td>0.922</td>
<td>0.000</td>
<td>0.000</td>
<td>1.000</td>
<td>0.000</td>
<td>0.423</td>
<td>0.238</td>
<td>1</td>
<td>eval</td>
<td>01:14</td>
</tr>
</tbody>
</table>

<p>
&#10;    <div>
      <progress value='6' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>
      15.00% [6/40 00:31&lt;02:58 0.237]
    </div>

    train top-1 accuracy: 0.3894
    train top-5 accuracy: 0.6301
    train strict top-1 accuracy: 0.0768
    train strict top-5 accuracy: 0.2991
    valid top-1 accuracy: 0.4084
    valid top-5 accuracy: 0.7040
    valid strict top-1 accuracy: 0.0797
    valid strict top-5 accuracy: 0.3622
    train top-1 accuracy: 0.4278
    train top-5 accuracy: 0.6853
    train strict top-1 accuracy: 0.0832
    train strict top-5 accuracy: 0.3555
    valid top-1 accuracy: 0.4084
    valid top-5 accuracy: 0.7104
    valid strict top-1 accuracy: 0.0797
    valid strict top-5 accuracy: 0.4231

    KeyboardInterrupt: 
    [31m---------------------------------------------------------------------------[39m
    [31mKeyboardInterrupt[39m                         Traceback (most recent call last)
    [36mCell[39m[36m [39m[32mIn[117][39m[32m, line 1[39m
    [32m----> [39m[32m1[39m [43mlearn[49m[43m.[49m[43mfit[49m[43m([49m[32;43m5[39;49m[43m)[49m

    [36mFile [39m[32m~/git/minai/minai/core.py:260[39m, in [36mLearner.fit[39m[34m(self, n_epochs, train, valid, cbs, lr)[39m
    [32m    258[39m     [38;5;28;01mif[39;00m lr [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m: lr = [38;5;28mself[39m.lr
    [32m    259[39m     [38;5;28;01mif[39;00m [38;5;28mself[39m.opt_func: [38;5;28mself[39m.opt = [38;5;28mself[39m.opt_func([38;5;28mself[39m.model.parameters(), lr)
    [32m--> [39m[32m260[39m     [38;5;28;43mself[39;49m[43m.[49m[43m_fit[49m[43m([49m[43mtrain[49m[43m,[49m[43m [49m[43mvalid[49m[43m)[49m
    [32m    261[39m [38;5;28;01mfinally[39;00m:
    [32m    262[39m     [38;5;28;01mfor[39;00m cb [38;5;129;01min[39;00m cbs: [38;5;28mself[39m.cbs.remove(cb)

    [36mFile [39m[32m~/git/minai/minai/core.py:194[39m, in [36mwith_cbs.__call__.<locals>._f[39m[34m(o, *args, **kwargs)[39m
    [32m    192[39m [38;5;28;01mtry[39;00m:
    [32m    193[39m     o.callback([33mf[39m[33m'[39m[33mbefore_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m--> [39m[32m194[39m     [43mf[49m[43m([49m[43mo[49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
    [32m    195[39m     o.callback([33mf[39m[33m'[39m[33mafter_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m    196[39m [38;5;28;01mexcept[39;00m [38;5;28mglobals[39m()[[33mf[39m[33m'[39m[33mCancel[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm.title()[38;5;132;01m}[39;00m[33mException[39m[33m'[39m]: [38;5;28;01mpass[39;00m

    [36mFile [39m[32m~/git/minai/minai/core.py:248[39m, in [36mLearner._fit[39m[34m(self, train, valid)[39m
    [32m    246[39m [38;5;28;01mif[39;00m [38;5;28mself[39m.epoch_sz [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m: [38;5;28mself[39m.train_dl = CycleDL([38;5;28mself[39m.train_dl, [38;5;28mself[39m.epoch_sz)
    [32m    247[39m [38;5;28;01mfor[39;00m [38;5;28mself[39m.epoch [38;5;129;01min[39;00m [38;5;28mself[39m.epochs:
    [32m--> [39m[32m248[39m     [38;5;28;01mif[39;00m train: [38;5;28;43mself[39;49m[43m.[49m[43mone_epoch[49m[43m([49m[38;5;28;43;01mTrue[39;49;00m[43m)[49m
    [32m    249[39m     [38;5;28;01mif[39;00m valid:
    [32m    250[39m         [38;5;28;01mwith[39;00m torch.inference_mode(): [38;5;28mself[39m.one_epoch([38;5;28;01mFalse[39;00m)

    [36mFile [39m[32m~/git/minai/minai/core.py:241[39m, in [36mLearner.one_epoch[39m[34m(self, training)[39m
    [32m    239[39m [38;5;28mself[39m.model.train(training)
    [32m    240[39m [38;5;28mself[39m.dl = [38;5;28mself[39m.train_dl [38;5;28;01mif[39;00m training [38;5;28;01melse[39;00m [38;5;28mself[39m.dls.valid
    [32m--> [39m[32m241[39m [38;5;28;43mself[39;49m[43m.[49m[43m_one_epoch[49m[43m([49m[43m)[49m

    [36mFile [39m[32m~/git/minai/minai/core.py:194[39m, in [36mwith_cbs.__call__.<locals>._f[39m[34m(o, *args, **kwargs)[39m
    [32m    192[39m [38;5;28;01mtry[39;00m:
    [32m    193[39m     o.callback([33mf[39m[33m'[39m[33mbefore_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m--> [39m[32m194[39m     [43mf[49m[43m([49m[43mo[49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
    [32m    195[39m     o.callback([33mf[39m[33m'[39m[33mafter_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m    196[39m [38;5;28;01mexcept[39;00m [38;5;28mglobals[39m()[[33mf[39m[33m'[39m[33mCancel[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm.title()[38;5;132;01m}[39;00m[33mException[39m[33m'[39m]: [38;5;28;01mpass[39;00m

    [36mFile [39m[32m~/git/minai/minai/core.py:236[39m, in [36mLearner._one_epoch[39m[34m(self)[39m
    [32m    234[39m [38;5;129m@with_cbs[39m([33m'[39m[33mepoch[39m[33m'[39m)
    [32m    235[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m_one_epoch[39m([38;5;28mself[39m):
    [32m--> [39m[32m236[39m     [38;5;28;43;01mfor[39;49;00m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43miter[49m[43m,[49m[38;5;28;43mself[39;49m[43m.[49m[43mbatch[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[38;5;28;43menumerate[39;49m[43m([49m[38;5;28;43mself[39;49m[43m.[49m[43mdl[49m[43m)[49m[43m:[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43m_one_batch[49m[43m([49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/fastprogress/fastprogress.py:41[39m, in [36mProgressBar.__iter__[39m[34m(self)[39m
    [32m     39[39m [38;5;28;01mif[39;00m [38;5;28mself[39m.total != [32m0[39m: [38;5;28mself[39m.update([32m0[39m)
    [32m     40[39m [38;5;28;01mtry[39;00m:
    [32m---> [39m[32m41[39m [43m    [49m[38;5;28;43;01mfor[39;49;00m[43m [49m[43mi[49m[43m,[49m[43mo[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[38;5;28;43menumerate[39;49m[43m([49m[38;5;28;43mself[39;49m[43m.[49m[43mgen[49m[43m)[49m[43m:[49m
    [32m     42[39m [43m        [49m[38;5;28;43;01mif[39;49;00m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mtotal[49m[43m [49m[38;5;129;43;01mand[39;49;00m[43m [49m[43mi[49m[43m [49m[43m>[49m[43m=[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mtotal[49m[43m:[49m[43m [49m[38;5;28;43;01mbreak[39;49;00m
    [32m     43[39m [43m        [49m[38;5;28;43;01myield[39;49;00m[43m [49m[43mo[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701[39m, in [36m_BaseDataLoaderIter.__next__[39m[34m(self)[39m
    [32m    698[39m [38;5;28;01mif[39;00m [38;5;28mself[39m._sampler_iter [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [32m    699[39m     [38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)[39;00m
    [32m    700[39m     [38;5;28mself[39m._reset()  [38;5;66;03m# type: ignore[call-arg][39;00m
    [32m--> [39m[32m701[39m data = [38;5;28;43mself[39;49m[43m.[49m[43m_next_data[49m[43m([49m[43m)[49m
    [32m    702[39m [38;5;28mself[39m._num_yielded += [32m1[39m
    [32m    703[39m [38;5;28;01mif[39;00m (
    [32m    704[39m     [38;5;28mself[39m._dataset_kind == _DatasetKind.Iterable
    [32m    705[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m._IterableDataset_len_called [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
    [32m    706[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m._num_yielded > [38;5;28mself[39m._IterableDataset_len_called
    [32m    707[39m ):

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757[39m, in [36m_SingleProcessDataLoaderIter._next_data[39m[34m(self)[39m
    [32m    755[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m_next_data[39m([38;5;28mself[39m):
    [32m    756[39m     index = [38;5;28mself[39m._next_index()  [38;5;66;03m# may raise StopIteration[39;00m
    [32m--> [39m[32m757[39m     data = [38;5;28;43mself[39;49m[43m.[49m[43m_dataset_fetcher[49m[43m.[49m[43mfetch[49m[43m([49m[43mindex[49m[43m)[49m  [38;5;66;03m# may raise StopIteration[39;00m
    [32m    758[39m     [38;5;28;01mif[39;00m [38;5;28mself[39m._pin_memory:
    [32m    759[39m         data = _utils.pin_memory.pin_memory(data, [38;5;28mself[39m._pin_memory_device)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52[39m, in [36m_MapDatasetFetcher.fetch[39m[34m(self, possibly_batched_index)[39m
    [32m     50[39m         data = [38;5;28mself[39m.dataset.__getitems__(possibly_batched_index)
    [32m     51[39m     [38;5;28;01melse[39;00m:
    [32m---> [39m[32m52[39m         data = [43m[[49m[38;5;28;43mself[39;49m[43m.[49m[43mdataset[49m[43m[[49m[43midx[49m[43m][49m[43m [49m[38;5;28;43;01mfor[39;49;00m[43m [49m[43midx[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[43mpossibly_batched_index[49m[43m][49m
    [32m     53[39m [38;5;28;01melse[39;00m:
    [32m     54[39m     data = [38;5;28mself[39m.dataset[possibly_batched_index]

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52[39m, in [36m<listcomp>[39m[34m(.0)[39m
    [32m     50[39m         data = [38;5;28mself[39m.dataset.__getitems__(possibly_batched_index)
    [32m     51[39m     [38;5;28;01melse[39;00m:
    [32m---> [39m[32m52[39m         data = [[38;5;28;43mself[39;49m[43m.[49m[43mdataset[49m[43m[[49m[43midx[49m[43m][49m [38;5;28;01mfor[39;00m idx [38;5;129;01min[39;00m possibly_batched_index]
    [32m     53[39m [38;5;28;01melse[39;00m:
    [32m     54[39m     data = [38;5;28mself[39m.dataset[possibly_batched_index]

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/datasets/voc.py:204[39m, in [36mVOCDetection.__getitem__[39m[34m(self, index)[39m
    [32m    201[39m target = [38;5;28mself[39m.parse_voc_xml(ET_parse([38;5;28mself[39m.annotations[index]).getroot())
    [32m    203[39m [38;5;28;01mif[39;00m [38;5;28mself[39m.transforms [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
    [32m--> [39m[32m204[39m     img, target = [38;5;28;43mself[39;49m[43m.[49m[43mtransforms[49m[43m([49m[43mimg[49m[43m,[49m[43m [49m[43mtarget[49m[43m)[49m
    [32m    206[39m [38;5;28;01mreturn[39;00m img, target

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/datasets/vision.py:95[39m, in [36mStandardTransform.__call__[39m[34m(self, input, target)[39m
    [32m     93[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__call__[39m([38;5;28mself[39m, [38;5;28minput[39m: Any, target: Any) -> Tuple[Any, Any]:
    [32m     94[39m     [38;5;28;01mif[39;00m [38;5;28mself[39m.transform [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
    [32m---> [39m[32m95[39m         [38;5;28minput[39m = [38;5;28;43mself[39;49m[43m.[49m[43mtransform[49m[43m([49m[38;5;28;43minput[39;49m[43m)[49m
    [32m     96[39m     [38;5;28;01mif[39;00m [38;5;28mself[39m.target_transform [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
    [32m     97[39m         target = [38;5;28mself[39m.target_transform(target)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
    [32m   1734[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [32m   1735[39m [38;5;28;01melse[39;00m:
    [32m-> [39m[32m1736[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
    [32m   1742[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [32m   1743[39m [38;5;66;03m# this function, and just call forward.[39;00m
    [32m   1744[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
    [32m   1745[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [32m   1746[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [32m-> [39m[32m1747[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
    [32m   1749[39m result = [38;5;28;01mNone[39;00m
    [32m   1750[39m called_always_called_hooks = [38;5;28mset[39m()

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py:51[39m, in [36mCompose.forward[39m[34m(self, *inputs)[39m
    [32m     49[39m needs_unpacking = [38;5;28mlen[39m(inputs) > [32m1[39m
    [32m     50[39m [38;5;28;01mfor[39;00m transform [38;5;129;01min[39;00m [38;5;28mself[39m.transforms:
    [32m---> [39m[32m51[39m     outputs = [43mtransform[49m[43m([49m[43m*[49m[43minputs[49m[43m)[49m
    [32m     52[39m     inputs = outputs [38;5;28;01mif[39;00m needs_unpacking [38;5;28;01melse[39;00m (outputs,)
    [32m     53[39m [38;5;28;01mreturn[39;00m outputs

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
    [32m   1734[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [32m   1735[39m [38;5;28;01melse[39;00m:
    [32m-> [39m[32m1736[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
    [32m   1742[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [32m   1743[39m [38;5;66;03m# this function, and just call forward.[39;00m
    [32m   1744[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
    [32m   1745[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [32m   1746[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [32m-> [39m[32m1747[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
    [32m   1749[39m result = [38;5;28;01mNone[39;00m
    [32m   1750[39m called_always_called_hooks = [38;5;28mset[39m()

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:50[39m, in [36mTransform.forward[39m[34m(self, *inputs)[39m
    [32m     45[39m needs_transform_list = [38;5;28mself[39m._needs_transform_list(flat_inputs)
    [32m     46[39m params = [38;5;28mself[39m._get_params(
    [32m     47[39m     [inpt [38;5;28;01mfor[39;00m (inpt, needs_transform) [38;5;129;01min[39;00m [38;5;28mzip[39m(flat_inputs, needs_transform_list) [38;5;28;01mif[39;00m needs_transform]
    [32m     48[39m )
    [32m---> [39m[32m50[39m flat_outputs = [43m[[49m
    [32m     51[39m [43m    [49m[38;5;28;43mself[39;49m[43m.[49m[43m_transform[49m[43m([49m[43minpt[49m[43m,[49m[43m [49m[43mparams[49m[43m)[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mneeds_transform[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[43minpt[49m
    [32m     52[39m [43m    [49m[38;5;28;43;01mfor[39;49;00m[43m [49m[43m([49m[43minpt[49m[43m,[49m[43m [49m[43mneeds_transform[49m[43m)[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[38;5;28;43mzip[39;49m[43m([49m[43mflat_inputs[49m[43m,[49m[43m [49m[43mneeds_transform_list[49m[43m)[49m
    [32m     53[39m [43m[49m[43m][49m
    [32m     55[39m [38;5;28;01mreturn[39;00m tree_unflatten(flat_outputs, spec)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:51[39m, in [36m<listcomp>[39m[34m(.0)[39m
    [32m     45[39m needs_transform_list = [38;5;28mself[39m._needs_transform_list(flat_inputs)
    [32m     46[39m params = [38;5;28mself[39m._get_params(
    [32m     47[39m     [inpt [38;5;28;01mfor[39;00m (inpt, needs_transform) [38;5;129;01min[39;00m [38;5;28mzip[39m(flat_inputs, needs_transform_list) [38;5;28;01mif[39;00m needs_transform]
    [32m     48[39m )
    [32m     50[39m flat_outputs = [
    [32m---> [39m[32m51[39m     [38;5;28;43mself[39;49m[43m.[49m[43m_transform[49m[43m([49m[43minpt[49m[43m,[49m[43m [49m[43mparams[49m[43m)[49m [38;5;28;01mif[39;00m needs_transform [38;5;28;01melse[39;00m inpt
    [32m     52[39m     [38;5;28;01mfor[39;00m (inpt, needs_transform) [38;5;129;01min[39;00m [38;5;28mzip[39m(flat_inputs, needs_transform_list)
    [32m     53[39m ]
    [32m     55[39m [38;5;28;01mreturn[39;00m tree_unflatten(flat_outputs, spec)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_geometry.py:160[39m, in [36mResize._transform[39m[34m(self, inpt, params)[39m
    [32m    159[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m_transform[39m([38;5;28mself[39m, inpt: Any, params: Dict[[38;5;28mstr[39m, Any]) -> Any:
    [32m--> [39m[32m160[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_kernel[49m[43m([49m
    [32m    161[39m [43m        [49m[43mF[49m[43m.[49m[43mresize[49m[43m,[49m
    [32m    162[39m [43m        [49m[43minpt[49m[43m,[49m
    [32m    163[39m [43m        [49m[38;5;28;43mself[39;49m[43m.[49m[43msize[49m[43m,[49m
    [32m    164[39m [43m        [49m[43minterpolation[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43minterpolation[49m[43m,[49m
    [32m    165[39m [43m        [49m[43mmax_size[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mmax_size[49m[43m,[49m
    [32m    166[39m [43m        [49m[43mantialias[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mantialias[49m[43m,[49m
    [32m    167[39m [43m    [49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:35[39m, in [36mTransform._call_kernel[39m[34m(self, functional, inpt, *args, **kwargs)[39m
    [32m     33[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m_call_kernel[39m([38;5;28mself[39m, functional: Callable, inpt: Any, *args: Any, **kwargs: Any) -> Any:
    [32m     34[39m     kernel = _get_kernel(functional, [38;5;28mtype[39m(inpt), allow_passthrough=[38;5;28;01mTrue[39;00m)
    [32m---> [39m[32m35[39m     [38;5;28;01mreturn[39;00m [43mkernel[49m[43m([49m[43minpt[49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_geometry.py:310[39m, in [36m__resize_image_pil_dispatch[39m[34m(image, size, interpolation, max_size, antialias)[39m
    [32m    308[39m [38;5;28;01mif[39;00m antialias [38;5;129;01mis[39;00m [38;5;28;01mFalse[39;00m:
    [32m    309[39m     warnings.warn([33m"[39m[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.[39m[33m"[39m)
    [32m--> [39m[32m310[39m [38;5;28;01mreturn[39;00m [43m_resize_image_pil[49m[43m([49m[43mimage[49m[43m,[49m[43m [49m[43msize[49m[43m=[49m[43msize[49m[43m,[49m[43m [49m[43minterpolation[49m[43m=[49m[43minterpolation[49m[43m,[49m[43m [49m[43mmax_size[49m[43m=[49m[43mmax_size[49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_geometry.py:297[39m, in [36m_resize_image_pil[39m[34m(image, size, interpolation, max_size)[39m
    [32m    294[39m [38;5;28;01mif[39;00m (new_height, new_width) == (old_height, old_width):
    [32m    295[39m     [38;5;28;01mreturn[39;00m image
    [32m--> [39m[32m297[39m [38;5;28;01mreturn[39;00m [43mimage[49m[43m.[49m[43mresize[49m[43m([49m[43m([49m[43mnew_width[49m[43m,[49m[43m [49m[43mnew_height[49m[43m)[49m[43m,[49m[43m [49m[43mresample[49m[43m=[49m[43mpil_modes_mapping[49m[43m[[49m[43minterpolation[49m[43m][49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/PIL/Image.py:2328[39m, in [36mImage.resize[39m[34m(self, size, resample, box, reducing_gap)[39m
    [32m   2316[39m         [38;5;28mself[39m = (
    [32m   2317[39m             [38;5;28mself[39m.reduce(factor, box=reduce_box)
    [32m   2318[39m             [38;5;28;01mif[39;00m [38;5;28mcallable[39m([38;5;28mself[39m.reduce)
    [32m   2319[39m             [38;5;28;01melse[39;00m Image.reduce([38;5;28mself[39m, factor, box=reduce_box)
    [32m   2320[39m         )
    [32m   2321[39m         box = (
    [32m   2322[39m             (box[[32m0[39m] - reduce_box[[32m0[39m]) / factor_x,
    [32m   2323[39m             (box[[32m1[39m] - reduce_box[[32m1[39m]) / factor_y,
    [32m   2324[39m             (box[[32m2[39m] - reduce_box[[32m0[39m]) / factor_x,
    [32m   2325[39m             (box[[32m3[39m] - reduce_box[[32m1[39m]) / factor_y,
    [32m   2326[39m         )
    [32m-> [39m[32m2328[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m._new([38;5;28;43mself[39;49m[43m.[49m[43mim[49m[43m.[49m[43mresize[49m[43m([49m[43msize[49m[43m,[49m[43m [49m[43mresample[49m[43m,[49m[43m [49m[43mbox[49m[43m)[49m)

    [31mKeyboardInterrupt[39m: 

``` python
# Code from https://github.com/pytorch/vision/blob/main/gallery/transforms/helpers.py

import matplotlib.pyplot as plt
import torch
from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
from torchvision import tv_tensors
from torchvision.transforms.v2 import functional as F

def plot(imgs, row_title=None, **imshow_kwargs):
    if not isinstance(imgs[0], list):
        # Make a 2d grid even if there's just 1 row
        imgs = [imgs]

    num_rows = len(imgs)
    num_cols = len(imgs[0])
    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)
    for row_idx, row in enumerate(imgs):
        for col_idx, img in enumerate(row):
            boxes = None
            masks = None
            if isinstance(img, tuple):
                img, target = img
                if isinstance(target, dict):
                    boxes = target.get("boxes")
                    masks = target.get("masks")
                elif isinstance(target, tv_tensors.BoundingBoxes):
                    boxes = target
                else:
                    raise ValueError(f"Unexpected target type: {type(target)}")
            img = F.to_image(img)
            if img.dtype.is_floating_point and img.min() < 0:
                # Poor man's re-normalization for the colors to be OK-ish. This
                # is useful for images coming out of Normalize()
                img -= img.min()
                img /= img.max()

            img = F.to_dtype(img, torch.uint8, scale=True)
            if boxes is not None:
                img = draw_bounding_boxes(img, boxes, colors="yellow", width=3)
            if masks is not None:
                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=["green"] * masks.shape[0], alpha=.65)

            ax = axs[row_idx, col_idx]
            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)
            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

    if row_title is not None:
        for row_idx in range(num_rows):
            axs[row_idx, 0].set(ylabel=row_title[row_idx])

    plt.tight_layout()
```
