{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pascal_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e5625",
   "metadata": {},
   "source": [
    "# Pascal Darknet Detection\n",
    "> Performing bounding box detection on PASCAL VOC 2007 using Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c675538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from minai import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms.v2.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "import fastcore.all as fc\n",
    "from fastcore.utils import L\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from pilus_project.core import *\n",
    "from pilus_project.darknet import *\n",
    "from pilus_project.pascal_class import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e2637",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80856470",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da78738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/kappa/data/VOCtrainval_06-Nov-2007.tar'),Path('/home/kappa/data/VOCdevkit'),Path('/home/kappa/data/pili')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = fc.Path.home()/'data/'\n",
    "data_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a2a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset VOCDetection\n",
       "    Number of datapoints: 2501\n",
       "    Root location: /home/kappa/data\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
       "                 RandomHorizontalFlip(p=0.5)\n",
       "                 ToImage()\n",
       "                 ToDtype(scale=True)\n",
       "                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       "           )"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds, val_ds = create_voc_datasets(data_path)\n",
    "trn_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=500x333>,\n",
       " (#1) [{'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.VOCDetection(\n",
    "    root=data_path, year=\"2007\", image_set='train', download=False, \n",
    "    target_transform=voc_extract(field='bndbox'))\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87529e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dede06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = datasets.VOCDetection(\n",
    "    root=data_path, year=\"2007\", image_set='train', download=False, \n",
    "    transform=to_tensor)\n",
    "val_ds = datasets.VOCDetection(\n",
    "    root=data_path, year=\"2007\", image_set='val', download=False, \n",
    "    transform=to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafe01f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m bs = \u001b[32m64\u001b[39m\n\u001b[32m      3\u001b[39m trn_dl, val_dl = get_dls(trn_ds, val_ds, bs=bs)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m xb,yb = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrn_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m xb.shape,yb[:\u001b[32m10\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:211\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:212\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m    171\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m         }\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m    171\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m         }\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:207\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    205\u001b[39m elem_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) == elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33meach element in list of batch should be of equal size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "\n",
    "trn_dl, val_dl = get_dls(trn_ds, val_ds, bs=bs)\n",
    "xb,yb = next(iter(trn_dl))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_extract_detection(targ, clss=VOC_CLASSES):\n",
    "    \"Extract class names and bounding boxes from VOC annotations\"\n",
    "    objects = fc.nested_attr(targ, 'annotation.object')\n",
    "    if not objects: return [], []\n",
    "    \n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for obj in objects:\n",
    "        name = obj['name']\n",
    "        if name in clss:\n",
    "            bbox = obj['bndbox']\n",
    "            # Convert string coordinates to floats\n",
    "            x1, y1 = float(bbox['xmin']), float(bbox['ymin'])\n",
    "            x2, y2 = float(bbox['xmax']), float(bbox['ymax'])\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(clss.index(name))\n",
    "    \n",
    "    return torch.tensor(boxes), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDetectionDataset(Dataset):\n",
    "    \"Dataset for VOC object detection\"\n",
    "    def __init__(self, voc_ds, transforms=None):\n",
    "        self.voc_ds = voc_ds\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self): return len(self.voc_ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.voc_ds[idx]\n",
    "        boxes, labels = voc_extract_detection(target)\n",
    "        \n",
    "        # Handle empty boxes\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb26289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection_transforms():\n",
    "    \"Create transforms for detection that handle both images and bounding boxes\"\n",
    "    return v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.RandomShortestSize(min_size=480, max_size=800),\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.Normalize(mean=xmean, std=xstd)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a94608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate_fn(batch):\n",
    "    \"Custom collate function for detection data with variable-sized boxes and labels\"\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, targ in batch:\n",
    "        images.append(img)\n",
    "        targets.append(targ)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba97956",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m trn_dl, val_dl = get_detection_dls(data_path, bs=\u001b[32m4\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m xb, yb = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrn_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m show_detection_batch(xb, yb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[31mTypeError\u001b[39m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trn_dl, val_dl = get_detection_dls(data_path, bs=4)\n",
    "xb, yb = next(iter(trn_dl))\n",
    "show_detection_batch(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe701d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection_dls(data_path, bs=8, year='2007'):\n",
    "    \"Create dataloaders for object detection\"\n",
    "    transforms = get_detection_transforms()\n",
    "    \n",
    "    train_ds = datasets.VOCDetection(root=data_path, year=year, image_set='train', download=False, transform=transforms)\n",
    "    valid_ds = datasets.VOCDetection(root=data_path, year=year, image_set='val', download=False, transform=transforms)\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=detection_collate_fn, num_workers=4)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=False, collate_fn=detection_collate_fn, num_workers=4)\n",
    "    \n",
    "    return get_dls(train_dl, valid_dl, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detection_batch(images, targets, max_n=6, figsize=(15, 10)):\n",
    "    \"Show detection results with bounding boxes\"\n",
    "    n = min(len(images), max_n)\n",
    "    fig, axs = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1: axs = [axs]\n",
    "    \n",
    "    for i in range(n):\n",
    "        img = denorm(images[i])\n",
    "        img = img.permute(1, 2, 0).cpu().numpy()\n",
    "        ax = axs[i]\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        # Draw boxes\n",
    "        for box, label in zip(targets[i]['boxes'], targets[i]['labels']):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            class_name = VOC_CLASSES[label]\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1, class_name, color='white', backgroundcolor='red', fontsize=8)\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection_model(num_classes=len(VOC_CLASSES) + 1):  # +1 for background\n",
    "    \"Create object detection model based on darknet19\"\n",
    "    from torchvision.models.detection import FasterRCNN\n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    \n",
    "    # Use darknet19 as backbone\n",
    "    backbone = get_darknet19()\n",
    "    \n",
    "    # Remove the last layer (which is for classification)\n",
    "    backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "    \n",
    "    # FasterRCNN needs to know the number of output channels\n",
    "    backbone.out_channels = 512\n",
    "    \n",
    "    # Define anchor generator\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    \n",
    "    # Define ROI pooler\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    \n",
    "    # Create the detection model\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651f7fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_dls() missing 1 required positional argument: 'bs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m detection_dls = \u001b[43mget_detection_dls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Smaller batch size for detection\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Visualize a batch\u001b[39;00m\n\u001b[32m      4\u001b[39m xb, yb = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(detection_dls.train))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mget_detection_dls\u001b[39m\u001b[34m(data_path, bs, year)\u001b[39m\n\u001b[32m      8\u001b[39m train_dl = DataLoader(train_ds, batch_size=bs, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=detection_collate_fn, num_workers=\u001b[32m4\u001b[39m)\n\u001b[32m      9\u001b[39m valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn=detection_collate_fn, num_workers=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_dls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: get_dls() missing 1 required positional argument: 'bs'"
     ]
    }
   ],
   "source": [
    "detection_dls = get_detection_dls(data_path, bs=4)  # Smaller batch size for detection\n",
    "\n",
    "# Visualize a batch\n",
    "xb, yb = next(iter(detection_dls.train))\n",
    "show_detection_batch(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382426d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_932/2076025865.py\", line 10, in detection_collate_fn\n    images = torch.stack(images)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/tv_tensors/_tv_tensor.py\", line 77, in __torch_function__\n    output = func(*args, **kwargs or dict())\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 697, 480] at entry 0 and [3, 480, 720] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m detection_dls = get_detection_dls(data_path, bs=\u001b[32m4\u001b[39m)  \u001b[38;5;66;03m# Smaller batch size for detection\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Visualize a batch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m xb, yb = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdetection_dls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m show_detection_batch(xb, yb)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create detection model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1465\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/_utils.py:715\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_932/2076025865.py\", line 10, in detection_collate_fn\n    images = torch.stack(images)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/tv_tensors/_tv_tensor.py\", line 77, in __torch_function__\n    output = func(*args, **kwargs or dict())\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [3, 697, 480] at entry 0 and [3, 480, 720] at entry 1\n"
     ]
    }
   ],
   "source": [
    "# Create detection model\n",
    "detection_model = get_detection_model()\n",
    "\n",
    "# Detection models in torchvision handle the loss internally\n",
    "# So we need a custom learner that doesn't calculate loss separately\n",
    "class DetectionLearner(Learner):\n",
    "    def predict(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "    \n",
    "    def get_loss(self):\n",
    "        # Loss is already calculated inside the model during training\n",
    "        if self.training:\n",
    "            self.loss = sum(loss for loss in self.preds.values())\n",
    "        else:\n",
    "            self.loss = torch.tensor(0.0, device=self.batch[0].device)\n",
    "\n",
    "# Create learner\n",
    "learn = DetectionLearner(detection_model, detection_dls, None, lr=0.005,\n",
    "                       cbs=[DeviceCB(), ProgressCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efb1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdabf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b1826b",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate detection results\n",
    "def evaluate_detection(model, dataloader, iou_threshold=0.5):\n",
    "    \"Evaluate detection model using mAP\"\n",
    "    from torchvision.ops import box_iou\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(def_device)\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Move predictions to CPU\n",
    "            for i in range(len(predictions)):\n",
    "                pred = {k: v.cpu() for k, v in predictions[i].items()}\n",
    "                all_preds.append(pred)\n",
    "                all_targets.append(targets[i])\n",
    "    \n",
    "    # Calculate mAP\n",
    "    # This is a simplified version - a full implementation would use the COCO API\n",
    "    aps = []\n",
    "    for class_idx in range(len(VOC_CLASSES)):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for pred, target in zip(all_preds, all_targets):\n",
    "            # Get predictions and targets for this class\n",
    "            pred_boxes = pred['boxes'][pred['labels'] == class_idx]\n",
    "            pred_scores = pred['scores'][pred['labels'] == class_idx]\n",
    "            target_boxes = target['boxes'][target['labels'] == class_idx]\n",
    "            \n",
    "            if len(target_boxes) == 0:\n",
    "                continue\n",
    "                \n",
    "            if len(pred_boxes) == 0:\n",
    "                precisions.append(0)\n",
    "                recalls.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Sort predictions by score\n",
    "            sorted_idx = torch.argsort(pred_scores, descending=True)\n",
    "            pred_boxes = pred_boxes[sorted_idx]\n",
    "            \n",
    "            # Calculate IoU\n",
    "            ious = box_iou(pred_boxes, target_boxes)\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            tp = (ious > iou_threshold).any(dim=1).float()\n",
    "            fp = 1 - tp\n",
    "            \n",
    "            tp_cumsum = torch.cumsum(tp, dim=0)\n",
    "            fp_cumsum = torch.cumsum(fp, dim=0)\n",
    "            \n",
    "            precision = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "            recall = tp_cumsum / len(target_boxes)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        # Calculate AP\n",
    "        if precisions and recalls:\n",
    "            # Simplified AP calculation\n",
    "            ap = sum([p * r for p, r in zip(precisions, recalls)]) / len(precisions)\n",
    "            aps.append(ap)\n",
    "    \n",
    "    return sum(aps) / len(aps) if aps else 0\n",
    "\n",
    "# Evaluate the model\n",
    "mAP = evaluate_detection(learn.model, detection_dls.valid)\n",
    "print(f\"mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = 1 if kernel_size == 3 else 0\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x): return self.leaky(self.bn(self.conv(x)))\n",
    "\n",
    "class Darknet19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Initial layers\n",
    "            ConvBlock(3, 32),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # First block\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 64, kernel_size=1),\n",
    "            ConvBlock(64, 128),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Second block\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 128, kernel_size=1),\n",
    "            ConvBlock(128, 256),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Third block\n",
    "            ConvBlock(256, 512),\n",
    "            ConvBlock(512, 256, kernel_size=1),\n",
    "            ConvBlock(256, 512),\n",
    "            ConvBlock(512, 256, kernel_size=1),\n",
    "            ConvBlock(256, 512),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Fourth block\n",
    "            ConvBlock(512, 1024),\n",
    "            ConvBlock(1024, 512, kernel_size=1),\n",
    "            ConvBlock(512, 1024),\n",
    "            ConvBlock(1024, 512, kernel_size=1),\n",
    "            ConvBlock(512, 1024)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.features(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6502b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv2(nn.Module):\n",
    "    def __init__(self, num_classes, num_anchors=5):\n",
    "        super().__init__()\n",
    "        self.backbone = Darknet19()\n",
    "        \n",
    "        # Passthrough layer from earlier feature map\n",
    "        self.passthrough_conv = ConvBlock(512, 64, kernel_size=1)\n",
    "        \n",
    "        # Detection head\n",
    "        self.detection = nn.Sequential(\n",
    "            ConvBlock(1024 + 256, 1024),  # +256 from passthrough (64*4)\n",
    "            nn.Conv2d(1024, num_anchors * (5 + num_classes), kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def reorg_layer(self, x):\n",
    "        # Reorganize 26x26x64 to 13x13x256\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, channels, height//2, 2, width//2, 2)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4)\n",
    "        x = x.contiguous().view(batch_size, channels*4, height//2, width//2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get backbone features\n",
    "        for i, layer in enumerate(self.backbone.features):\n",
    "            x = layer(x)\n",
    "            if i == 13:  # Save feature map for passthrough\n",
    "                passthrough = self.passthrough_conv(x)\n",
    "                passthrough = self.reorg_layer(passthrough)\n",
    "        \n",
    "        # Concatenate passthrough with final features\n",
    "        x = torch.cat([passthrough, x], dim=1)\n",
    "        \n",
    "        # Detection head\n",
    "        return self.detection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8fb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
