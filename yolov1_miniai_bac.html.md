# Yolo v1 on bacteria


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

``` python
import torch
import torchvision.transforms as transforms
from torch.optim import lr_scheduler

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import fastcore.all as fc

from functools import partial
from collections import Counter

from minai import *
from pilus_project.yolov1 import *
```

``` python
set_seed(42)
```

``` python
transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])
IMG_DIR = "../data/images"
LABEL_DIR = "../data/labels"

bs = 8
```

``` python
!ls ..
```

    8examples.csv      _proc        pilus_project       test.csv
    8examples_val.csv  data         pilus_project.egg-info  train.all.txt
    LICENSE        generate_csv.py  pyproject.toml      train.csv
    MANIFEST.in    nbs          settings.ini        voc_dataset.sh
    README.md      old_txt_files    setup.py            voc_label.py

``` python
pd.read_csv("../8examples.csv")
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&#10;    .dataframe tbody tr th {
        vertical-align: top;
    }
&#10;    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">img</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>000007.jpg</td>
<td>000007.txt</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>000009.jpg</td>
<td>000009.txt</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>000016.jpg</td>
<td>000016.txt</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>000019.jpg</td>
<td>000019.txt</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>000020.jpg</td>
<td>000020.txt</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">5</td>
<td>000021.jpg</td>
<td>000021.txt</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">6</td>
<td>000122.jpg</td>
<td>000122.txt</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">7</td>
<td>000129.jpg</td>
<td>000129.txt</td>
</tr>
</tbody>
</table>

</div>

``` python
trn_ds = VOCDataset("../8examples.csv", 
                    transform=transform,
                    img_dir=IMG_DIR,
                    label_dir=LABEL_DIR)
x0, y0 = trn_ds[0]
plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])
```

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-7-output-1.png)

``` python
# examples_val_8 = pd.read_csv("../test.csv").iloc[:8]
# examples_val_8
```

``` python
# examples_val_8.to_csv('../8examples_val.csv', index=False)
```

``` python
val_ds = VOCDataset(
    "../8examples_val.csv", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,
)
x0, y0 = val_ds[0]
plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])
```

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-10-output-1.png)

``` python
len(val_ds)
```

    8

``` python
trn_dl, val_dl = get_dls(trn_ds, val_ds, bs)
xb, yb = next(iter(trn_dl))
xb.shape, yb.shape
```

    (torch.Size([8, 3, 448, 448]), torch.Size([8, 7, 7, 30]))

``` python
dls = DataLoaders(trn_dl, val_dl)
```

## Learner

``` python
class MeanAP:
    def __init__(self, num_classes=1, epsilon=1e-6, threshold=0.4, iou_threshold=0.5, box_format='midpoint'):
        self.num_classes = num_classes
        self.epsilon = epsilon
        self.threshold = threshold
        self.iou_threshold = iou_threshold
        self.box_format = box_format
        self.average_precisions = []
    
    def reset(self):
        self.average_precisions = []
    
    def compute(self):
        return sum(self.average_precisions) / len(self.average_precisions)
    
    def update(self, pred, label):
        # `get_bboxes` part
        all_pred_boxes = []
        all_true_boxes = []
        train_idx = 0
        batch_size = pred.shape[0]
        pred_boxes = cellboxes_to_boxes(pred)
        true_boxes = cellboxes_to_boxes(label)
        
        for idx in range(batch_size):
            nms_boxes = non_max_suppression(
                pred_boxes[idx],
                iou_threshold=self.iou_threshold,
                threshold=self.threshold,
                box_format=self.box_format,
            )

            for nms_box in nms_boxes:
                all_pred_boxes.append([train_idx] + nms_box)

            for box in true_boxes[idx]:
                # many will get converted to 0 pred
                if box[1] > self.threshold:
                    all_true_boxes.append([train_idx] + box)
        
        pred_boxes = all_pred_boxes
        true_boxes = all_true_boxes
        
        for c in range(self.num_classes):
            detections = []
            ground_truths = []

            # Go through all predictions and targets,
            # and only add the ones that belong to the
            # current class c
            for detection in pred_boxes:
                if detection[1] == c:
                    detections.append(detection)

            for true_box in true_boxes:
                if true_box[1] == c:
                    ground_truths.append(true_box)

            # find the amount of bboxes for each training example
            # Counter here finds how many ground truth bboxes we get
            # for each training example, so let's say img 0 has 3,
            # img 1 has 5 then we will obtain a dictionary with:
            # amount_bboxes = {0:3, 1:5}
            amount_bboxes = Counter([gt[0] for gt in ground_truths])

            # We then go through each key, val in this dictionary
            # and convert to the following (w.r.t same example):
            # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}
            for key, val in amount_bboxes.items():
                amount_bboxes[key] = torch.zeros(val)

            # sort by box probabilities which is index 2
            detections.sort(key=lambda x: x[2], reverse=True)
            TP = torch.zeros((len(detections)))
            FP = torch.zeros((len(detections)))
            total_true_bboxes = len(ground_truths)

            # If none exists for this class then we can safely skip
            if total_true_bboxes == 0:
                continue

            for detection_idx, detection in enumerate(detections):
                # Only take out the ground_truths that have the same
                # training idx as detection
                ground_truth_img = [
                    bbox for bbox in ground_truths if bbox[0] == detection[0]
                ]

                num_gts = len(ground_truth_img)
                best_iou = 0

                for idx, gt in enumerate(ground_truth_img):
                    iou = intersection_over_union(
                        torch.tensor(detection[3:]),
                        torch.tensor(gt[3:]),
                        box_format=self.box_format,
                    )

                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = idx

                if best_iou > self.iou_threshold:
                    # only detect ground truth detection once
                    if amount_bboxes[detection[0]][best_gt_idx] == 0:
                        # true positive and add this bounding box to seen
                        TP[detection_idx] = 1
                        amount_bboxes[detection[0]][best_gt_idx] = 1
                    else:
                        FP[detection_idx] = 1

                # if IOU is lower then the detection is a false positive
                else:
                    FP[detection_idx] = 1

            TP_cumsum = torch.cumsum(TP, dim=0)
            FP_cumsum = torch.cumsum(FP, dim=0)
            recalls = TP_cumsum / (total_true_bboxes + self.epsilon)
            precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + self.epsilon))
            precisions = torch.cat((torch.tensor([1]), precisions))
            recalls = torch.cat((torch.tensor([0]), recalls))
            # torch.trapz for numerical integration
            self.average_precisions.append(torch.trapz(precisions, recalls))
```

``` python
cbs = [
    TrainCB(),
    DeviceCB(),
    MetricsCB(MeanAP(num_classes=20)),
]
opt = partial(torch.optim.AdamW, betas=(0.9,0.95), eps=1e-5)
```

``` python
model = Yolov1(split_size=7, num_boxes=2, num_classes=20)

lr, epochs = 1e-5, 150
tmax = epochs * len(dls.train)
sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)
xtra = [BatchSchedCB(sched)]

learn = Learner(model, dls, YoloLoss(), lr=lr, cbs=cbs+xtra, opt_func=torch.optim.AdamW)
```

``` python
learn.show_image_batch()
```

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-17-output-1.png)

``` python
learn.fit(epochs, cbs=[ProgressCB(plot=True)])
```

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

<div>
      <progress value='0' class='' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/150 00:00&lt;?]
    </div>
    &#10;

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: left;">
<th data-quarto-table-cell-role="th">MeanAP</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.000</td>
<td>648.640</td>
<td>0</td>
<td>train</td>
<td>00:04</td>
</tr>
</tbody>
</table>

<p>
&#10;    <div>
      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/1 00:00&lt;?]
    </div>

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-18-output-3.png)

    KeyboardInterrupt: 
    [31m---------------------------------------------------------------------------[39m
    [31mKeyboardInterrupt[39m                         Traceback (most recent call last)
    [36mCell[39m[36m [39m[32mIn[15][39m[32m, line 1[39m
    [32m----> [39m[32m1[39m [43mlearn[49m[43m.[49m[43mfit[49m[43m([49m[43mepochs[49m[43m,[49m[43m [49m[43mcbs[49m[43m=[49m[43m[[49m[43mProgressCB[49m[43m([49m[43mplot[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m)[49m[43m][49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:264[39m, in [36mLearner.fit[39m[34m(self, n_epochs, train, valid, cbs, lr)[39m
    [32m    262[39m     [38;5;28;01mif[39;00m lr [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m: lr = [38;5;28mself[39m.lr
    [32m    263[39m     [38;5;28;01mif[39;00m [38;5;28mself[39m.opt_func: [38;5;28mself[39m.opt = [38;5;28mself[39m.opt_func([38;5;28mself[39m.model.parameters(), lr)
    [32m--> [39m[32m264[39m     [38;5;28;43mself[39;49m[43m.[49m[43m_fit[49m[43m([49m[43mtrain[49m[43m,[49m[43m [49m[43mvalid[49m[43m)[49m
    [32m    265[39m [38;5;28;01mfinally[39;00m:
    [32m    266[39m     [38;5;28;01mfor[39;00m cb [38;5;129;01min[39;00m cbs: [38;5;28mself[39m.cbs.remove(cb)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:198[39m, in [36mwith_cbs.__call__.<locals>._f[39m[34m(o, *args, **kwargs)[39m
    [32m    196[39m [38;5;28;01mtry[39;00m:
    [32m    197[39m     o.callback([33mf[39m[33m'[39m[33mbefore_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m--> [39m[32m198[39m     [43mf[49m[43m([49m[43mo[49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
    [32m    199[39m     o.callback([33mf[39m[33m'[39m[33mafter_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m    200[39m [38;5;28;01mexcept[39;00m [38;5;28mglobals[39m()[[33mf[39m[33m'[39m[33mCancel[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm.title()[38;5;132;01m}[39;00m[33mException[39m[33m'[39m]: [38;5;28;01mpass[39;00m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:254[39m, in [36mLearner._fit[39m[34m(self, train, valid)[39m
    [32m    252[39m [38;5;28;01mif[39;00m train: [38;5;28mself[39m.one_epoch([38;5;28;01mTrue[39;00m)
    [32m    253[39m [38;5;28;01mif[39;00m valid:
    [32m--> [39m[32m254[39m     [38;5;28;01mwith[39;00m torch.inference_mode(): [38;5;28;43mself[39;49m[43m.[49m[43mone_epoch[49m[43m([49m[38;5;28;43;01mFalse[39;49;00m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:245[39m, in [36mLearner.one_epoch[39m[34m(self, training)[39m
    [32m    243[39m [38;5;28mself[39m.model.train(training)
    [32m    244[39m [38;5;28mself[39m.dl = [38;5;28mself[39m.train_dl [38;5;28;01mif[39;00m training [38;5;28;01melse[39;00m [38;5;28mself[39m.dls.valid
    [32m--> [39m[32m245[39m [38;5;28;43mself[39;49m[43m.[49m[43m_one_epoch[49m[43m([49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:198[39m, in [36mwith_cbs.__call__.<locals>._f[39m[34m(o, *args, **kwargs)[39m
    [32m    196[39m [38;5;28;01mtry[39;00m:
    [32m    197[39m     o.callback([33mf[39m[33m'[39m[33mbefore_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m--> [39m[32m198[39m     [43mf[49m[43m([49m[43mo[49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
    [32m    199[39m     o.callback([33mf[39m[33m'[39m[33mafter_[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm[38;5;132;01m}[39;00m[33m'[39m)
    [32m    200[39m [38;5;28;01mexcept[39;00m [38;5;28mglobals[39m()[[33mf[39m[33m'[39m[33mCancel[39m[38;5;132;01m{[39;00m[38;5;28mself[39m.nm.title()[38;5;132;01m}[39;00m[33mException[39m[33m'[39m]: [38;5;28;01mpass[39;00m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:240[39m, in [36mLearner._one_epoch[39m[34m(self)[39m
    [32m    238[39m [38;5;129m@with_cbs[39m([33m'[39m[33mepoch[39m[33m'[39m)
    [32m    239[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m_one_epoch[39m([38;5;28mself[39m):
    [32m--> [39m[32m240[39m     [38;5;28;43;01mfor[39;49;00m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43miter[49m[43m,[49m[38;5;28;43mself[39;49m[43m.[49m[43mbatch[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[38;5;28;43menumerate[39;49m[43m([49m[38;5;28;43mself[39;49m[43m.[49m[43mdl[49m[43m)[49m[43m:[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43m_one_batch[49m[43m([49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/fastprogress/fastprogress.py:41[39m, in [36mProgressBar.__iter__[39m[34m(self)[39m
    [32m     39[39m [38;5;28;01mif[39;00m [38;5;28mself[39m.total != [32m0[39m: [38;5;28mself[39m.update([32m0[39m)
    [32m     40[39m [38;5;28;01mtry[39;00m:
    [32m---> [39m[32m41[39m [43m    [49m[38;5;28;43;01mfor[39;49;00m[43m [49m[43mi[49m[43m,[49m[43mo[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[38;5;28;43menumerate[39;49m[43m([49m[38;5;28;43mself[39;49m[43m.[49m[43mgen[49m[43m)[49m[43m:[49m
    [32m     42[39m [43m        [49m[38;5;28;43;01mif[39;49;00m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mtotal[49m[43m [49m[38;5;129;43;01mand[39;49;00m[43m [49m[43mi[49m[43m [49m[43m>[49m[43m=[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mtotal[49m[43m:[49m[43m [49m[38;5;28;43;01mbreak[39;49;00m
    [32m     43[39m [43m        [49m[38;5;28;43;01myield[39;49;00m[43m [49m[43mo[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701[39m, in [36m_BaseDataLoaderIter.__next__[39m[34m(self)[39m
    [32m    698[39m [38;5;28;01mif[39;00m [38;5;28mself[39m._sampler_iter [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [32m    699[39m     [38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)[39;00m
    [32m    700[39m     [38;5;28mself[39m._reset()  [38;5;66;03m# type: ignore[call-arg][39;00m
    [32m--> [39m[32m701[39m data = [38;5;28;43mself[39;49m[43m.[49m[43m_next_data[49m[43m([49m[43m)[49m
    [32m    702[39m [38;5;28mself[39m._num_yielded += [32m1[39m
    [32m    703[39m [38;5;28;01mif[39;00m (
    [32m    704[39m     [38;5;28mself[39m._dataset_kind == _DatasetKind.Iterable
    [32m    705[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m._IterableDataset_len_called [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
    [32m    706[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m._num_yielded > [38;5;28mself[39m._IterableDataset_len_called
    [32m    707[39m ):

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757[39m, in [36m_SingleProcessDataLoaderIter._next_data[39m[34m(self)[39m
    [32m    755[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m_next_data[39m([38;5;28mself[39m):
    [32m    756[39m     index = [38;5;28mself[39m._next_index()  [38;5;66;03m# may raise StopIteration[39;00m
    [32m--> [39m[32m757[39m     data = [38;5;28;43mself[39;49m[43m.[49m[43m_dataset_fetcher[49m[43m.[49m[43mfetch[49m[43m([49m[43mindex[49m[43m)[49m  [38;5;66;03m# may raise StopIteration[39;00m
    [32m    758[39m     [38;5;28;01mif[39;00m [38;5;28mself[39m._pin_memory:
    [32m    759[39m         data = _utils.pin_memory.pin_memory(data, [38;5;28mself[39m._pin_memory_device)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52[39m, in [36m_MapDatasetFetcher.fetch[39m[34m(self, possibly_batched_index)[39m
    [32m     50[39m         data = [38;5;28mself[39m.dataset.__getitems__(possibly_batched_index)
    [32m     51[39m     [38;5;28;01melse[39;00m:
    [32m---> [39m[32m52[39m         data = [43m[[49m[38;5;28;43mself[39;49m[43m.[49m[43mdataset[49m[43m[[49m[43midx[49m[43m][49m[43m [49m[38;5;28;43;01mfor[39;49;00m[43m [49m[43midx[49m[43m [49m[38;5;129;43;01min[39;49;00m[43m [49m[43mpossibly_batched_index[49m[43m][49m
    [32m     53[39m [38;5;28;01melse[39;00m:
    [32m     54[39m     data = [38;5;28mself[39m.dataset[possibly_batched_index]

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52[39m, in [36m<listcomp>[39m[34m(.0)[39m
    [32m     50[39m         data = [38;5;28mself[39m.dataset.__getitems__(possibly_batched_index)
    [32m     51[39m     [38;5;28;01melse[39;00m:
    [32m---> [39m[32m52[39m         data = [[38;5;28;43mself[39;49m[43m.[49m[43mdataset[49m[43m[[49m[43midx[49m[43m][49m [38;5;28;01mfor[39;00m idx [38;5;129;01min[39;00m possibly_batched_index]
    [32m     53[39m [38;5;28;01melse[39;00m:
    [32m     54[39m     data = [38;5;28mself[39m.dataset[possibly_batched_index]

    [36mFile [39m[32m~/git/pilus_project/pilus_project/yolov1.py:405[39m, in [36mVOCDataset.__getitem__[39m[34m(self, index)[39m
    [32m    401[39m boxes = torch.tensor(boxes)
    [32m    403[39m [38;5;28;01mif[39;00m [38;5;28mself[39m.transform:
    [32m    404[39m     [38;5;66;03m# image = self.transform(image)[39;00m
    [32m--> [39m[32m405[39m     image, boxes = [38;5;28;43mself[39;49m[43m.[49m[43mtransform[49m[43m([49m[43mimage[49m[43m,[49m[43m [49m[43mboxes[49m[43m)[49m
    [32m    407[39m [38;5;66;03m# Convert To Cells[39;00m
    [32m    408[39m label_matrix = torch.zeros(([38;5;28mself[39m.S, [38;5;28mself[39m.S, [38;5;28mself[39m.C + [32m5[39m * [38;5;28mself[39m.B))

    [36mFile [39m[32m~/git/pilus_project/pilus_project/yolov1.py:692[39m, in [36mCompose.__call__[39m[34m(self, img, bboxes)[39m
    [32m    690[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__call__[39m([38;5;28mself[39m, img, bboxes):
    [32m    691[39m     [38;5;28;01mfor[39;00m t [38;5;129;01min[39;00m [38;5;28mself[39m.transforms:
    [32m--> [39m[32m692[39m         img, bboxes = [43mt[49m[43m([49m[43mimg[49m[43m)[49m, bboxes
    [32m    694[39m     [38;5;28;01mreturn[39;00m img, bboxes

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
    [32m   1734[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
    [32m   1735[39m [38;5;28;01melse[39;00m:
    [32m-> [39m[32m1736[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
    [32m   1742[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
    [32m   1743[39m [38;5;66;03m# this function, and just call forward.[39;00m
    [32m   1744[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
    [32m   1745[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
    [32m   1746[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
    [32m-> [39m[32m1747[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
    [32m   1749[39m result = [38;5;28;01mNone[39;00m
    [32m   1750[39m called_always_called_hooks = [38;5;28mset[39m()

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/transforms.py:354[39m, in [36mResize.forward[39m[34m(self, img)[39m
    [32m    346[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mforward[39m([38;5;28mself[39m, img):
    [32m    347[39m [38;5;250m    [39m[33;03m"""[39;00m
    [32m    348[39m [33;03m    Args:[39;00m
    [32m    349[39m [33;03m        img (PIL Image or Tensor): Image to be scaled.[39;00m
    [32m   (...)[39m[32m    352[39m [33;03m        PIL Image or Tensor: Rescaled image.[39;00m
    [32m    353[39m [33;03m    """[39;00m
    [32m--> [39m[32m354[39m     [38;5;28;01mreturn[39;00m [43mF[49m[43m.[49m[43mresize[49m[43m([49m[43mimg[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43msize[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43minterpolation[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mmax_size[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mantialias[49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/functional.py:477[39m, in [36mresize[39m[34m(img, size, interpolation, max_size, antialias)[39m
    [32m    475[39m         warnings.warn([33m"[39m[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.[39m[33m"[39m)
    [32m    476[39m     pil_interpolation = pil_modes_mapping[interpolation]
    [32m--> [39m[32m477[39m     [38;5;28;01mreturn[39;00m [43mF_pil[49m[43m.[49m[43mresize[49m[43m([49m[43mimg[49m[43m,[49m[43m [49m[43msize[49m[43m=[49m[43moutput_size[49m[43m,[49m[43m [49m[43minterpolation[49m[43m=[49m[43mpil_interpolation[49m[43m)[49m
    [32m    479[39m [38;5;28;01mreturn[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py:250[39m, in [36mresize[39m[34m(img, size, interpolation)[39m
    [32m    247[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28misinstance[39m(size, [38;5;28mlist[39m) [38;5;129;01mand[39;00m [38;5;28mlen[39m(size) == [32m2[39m):
    [32m    248[39m     [38;5;28;01mraise[39;00m [38;5;167;01mTypeError[39;00m([33mf[39m[33m"[39m[33mGot inappropriate size arg: [39m[38;5;132;01m{[39;00msize[38;5;132;01m}[39;00m[33m"[39m)
    [32m--> [39m[32m250[39m [38;5;28;01mreturn[39;00m [43mimg[49m[43m.[49m[43mresize[49m[43m([49m[38;5;28;43mtuple[39;49m[43m([49m[43msize[49m[43m[[49m[43m:[49m[43m:[49m[43m-[49m[32;43m1[39;49m[43m][49m[43m)[49m[43m,[49m[43m [49m[43minterpolation[49m[43m)[49m

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/PIL/Image.py:2293[39m, in [36mImage.resize[39m[34m(self, size, resample, box, reducing_gap)[39m
    [32m   2290[39m     msg = [33m"[39m[33mreducing_gap must be 1.0 or greater[39m[33m"[39m
    [32m   2291[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(msg)
    [32m-> [39m[32m2293[39m [38;5;28;43mself[39;49m[43m.[49m[43mload[49m[43m([49m[43m)[49m
    [32m   2294[39m [38;5;28;01mif[39;00m box [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
    [32m   2295[39m     box = ([32m0[39m, [32m0[39m) + [38;5;28mself[39m.size

    [36mFile [39m[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/PIL/ImageFile.py:293[39m, in [36mImageFile.load[39m[34m(self)[39m
    [32m    290[39m         [38;5;28;01mraise[39;00m [38;5;167;01mOSError[39;00m(msg)
    [32m    292[39m b = b + s
    [32m--> [39m[32m293[39m n, err_code = [43mdecoder[49m[43m.[49m[43mdecode[49m[43m([49m[43mb[49m[43m)[49m
    [32m    294[39m [38;5;28;01mif[39;00m n < [32m0[39m:
    [32m    295[39m     [38;5;28;01mbreak[39;00m

    [31mKeyboardInterrupt[39m: 

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-18-output-5.png)

## Testing

Let’s see if the model learned about bounding boxes for the training
set.

Loading the weight from previous one:

``` python
# learn.opt_func = torch.optim.Adam(
#     learn.model.parameters(), lr=lr, weight_decay=0
# )

# load_checkpoint(torch.load("overfit.pth.tar"), learn.model, learn.opt_func)
```

``` python
learn.model.train(False)
for i in range(8):
    x0, y0 = trn_ds[i]
    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))
    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')
    plot_image(x0, bboxes)
```

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-1.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-2.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-3.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-4.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-5.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-6.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-7.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-20-output-8.png)

``` python
learn.model.train(False)
for i in range(8):
    x0, y0 = val_ds[i]
    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))
    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')
    plot_image(x0, bboxes)
```

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-1.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-2.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-3.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-4.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-5.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-6.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-7.png)

![](55_yolov1_miniai_bac_files/figure-commonmark/cell-21-output-8.png)
