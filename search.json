[
  {
    "objectID": "yolov1_miniai.html",
    "href": "yolov1_miniai.html",
    "title": "Yolo v1 using minai",
    "section": "",
    "text": "TODO: use more modern techniques to speed up training: torch.compile, transform.v2.\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.optim import lr_scheduler\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport fastcore.all as fc\n\nfrom functools import partial\nfrom collections import Counter\n\nfrom minai import *\nfrom pilus_project.core import *\nfrom pilus_project.yolov1 import *\nset_seed(42)\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\nIMG_DIR = \"../data/images\"\nLABEL_DIR = \"../data/labels\"\n\nbs = 64\n!ls ..\n\n8examples.csv      _proc        pilus_project       test.csv\n8examples_val.csv  data         pilus_project.egg-info  train.all.txt\nLICENSE        generate_csv.py  pyproject.toml      train.csv\nMANIFEST.in    nbs          settings.ini        voc_dataset.sh\nREADME.md      old_txt_files    setup.py            voc_label.py\ntrn_ds = VOCDataset(\"../train.csv\", \n                    transform=transform,\n                    img_dir=IMG_DIR,\n                    label_dir=LABEL_DIR)\nx0, y0 = trn_ds[0]\nplot_image(x0, y0)\n# examples_val_8 = pd.read_csv(\"../test.csv\").iloc[:8]\n# examples_val_8\n# examples_val_8.to_csv('../8examples_val.csv', index=False)\nval_ds = VOCDataset(\n    \"../test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n)\nx0, y0 = val_ds[0]\nplot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])\nlen(trn_ds), len(val_ds)\n\n(16550, 4951)\ntrn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\nxb, yb = next(iter(trn_dl))\nxb.shape, yb.shape\n\n(torch.Size([64, 3, 448, 448]), torch.Size([64, 7, 7, 30]))\ndls = DataLoaders(trn_dl, val_dl)\nclass Yolov1(nn.Module):\n    def __init__(self, in_channels=3, **kwargs):\n        super(Yolov1, self).__init__()\n        self.architecture = architecture_config\n        self.in_channels = in_channels\n        self.darknet = self._create_conv_layers(self.architecture)\n        self.fcs = self._create_fcs(**kwargs)\n\n    def forward(self, x):\n        x = self.darknet(x)\n        return self.fcs(torch.flatten(x, start_dim=1))\n\n    def _create_conv_layers(self, architecture):\n        layers = []\n        in_channels = self.in_channels\n\n        for x in architecture:\n            if type(x) == tuple:\n                layers += [\n                    CNNBlock(\n                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n                    )\n                ]\n                in_channels = x[1]\n\n            elif type(x) == str:\n                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n\n            elif type(x) == list:\n                conv1 = x[0]\n                conv2 = x[1]\n                num_repeats = x[2]\n\n                for _ in range(num_repeats):\n                    layers += [\n                        CNNBlock(\n                            in_channels,\n                            conv1[1],\n                            kernel_size=conv1[0],\n                            stride=conv1[2],\n                            padding=conv1[3],\n                        )\n                    ]\n                    layers += [\n                        CNNBlock(\n                            conv1[1],\n                            conv2[1],\n                            kernel_size=conv2[0],\n                            stride=conv2[2],\n                            padding=conv2[3],\n                        )\n                    ]\n                    in_channels = conv2[1]\n\n        return nn.Sequential(*layers)\n\n    def _create_fcs(self, split_size, num_boxes, num_classes):\n        S, B, C = split_size, num_boxes, num_classes\n\n        # In original paper this should be\n        # nn.Linear(1024*S*S, 4096),\n        # nn.LeakyReLU(0.1),\n        # nn.Linear(4096, S*S*(B*5+C))\n\n        return nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(1024 * S * S, 4096),\n            nn.Dropout(0.0),\n            nn.LeakyReLU(0.1),\n            nn.Linear(4096, S * S * (C + B * 5)),\n        )",
    "crumbs": [
      "Yolo v1 using minai"
    ]
  },
  {
    "objectID": "yolov1_miniai.html#learner",
    "href": "yolov1_miniai.html#learner",
    "title": "Yolo v1 using minai",
    "section": "Learner",
    "text": "Learner\n\nclass MeanAP:\n    def __init__(self, num_classes=1, epsilon=1e-6, threshold=0.4, iou_threshold=0.5, box_format='midpoint'):\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n        self.box_format = box_format\n        self.average_precisions = []\n    \n    def reset(self):\n        self.average_precisions = []\n    \n    def compute(self):\n        return sum(self.average_precisions) / len(self.average_precisions)\n    \n    def update(self, pred, label):\n        # `get_bboxes` part\n        all_pred_boxes = []\n        all_true_boxes = []\n        train_idx = 0\n        batch_size = pred.shape[0]\n        pred_boxes = cellboxes_to_boxes(pred)\n        true_boxes = cellboxes_to_boxes(label)\n        \n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                pred_boxes[idx],\n                iou_threshold=self.iou_threshold,\n                threshold=self.threshold,\n                box_format=self.box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_boxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] &gt; self.threshold:\n                    all_true_boxes.append([train_idx] + box)\n        \n        pred_boxes = all_pred_boxes\n        true_boxes = all_true_boxes\n        \n        for c in range(self.num_classes):\n            detections = []\n            ground_truths = []\n\n            # Go through all predictions and targets,\n            # and only add the ones that belong to the\n            # current class c\n            for detection in pred_boxes:\n                if detection[1] == c:\n                    detections.append(detection)\n\n            for true_box in true_boxes:\n                if true_box[1] == c:\n                    ground_truths.append(true_box)\n\n            # find the amount of bboxes for each training example\n            # Counter here finds how many ground truth bboxes we get\n            # for each training example, so let's say img 0 has 3,\n            # img 1 has 5 then we will obtain a dictionary with:\n            # amount_bboxes = {0:3, 1:5}\n            amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n            # We then go through each key, val in this dictionary\n            # and convert to the following (w.r.t same example):\n            # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n            for key, val in amount_bboxes.items():\n                amount_bboxes[key] = torch.zeros(val)\n\n            # sort by box probabilities which is index 2\n            detections.sort(key=lambda x: x[2], reverse=True)\n            TP = torch.zeros((len(detections)))\n            FP = torch.zeros((len(detections)))\n            total_true_bboxes = len(ground_truths)\n\n            # If none exists for this class then we can safely skip\n            if total_true_bboxes == 0:\n                continue\n\n            for detection_idx, detection in enumerate(detections):\n                # Only take out the ground_truths that have the same\n                # training idx as detection\n                ground_truth_img = [\n                    bbox for bbox in ground_truths if bbox[0] == detection[0]\n                ]\n\n                num_gts = len(ground_truth_img)\n                best_iou = 0\n\n                for idx, gt in enumerate(ground_truth_img):\n                    iou = intersection_over_union(\n                        torch.tensor(detection[3:]),\n                        torch.tensor(gt[3:]),\n                        box_format=self.box_format,\n                    )\n\n                    if iou &gt; best_iou:\n                        best_iou = iou\n                        best_gt_idx = idx\n\n                if best_iou &gt; self.iou_threshold:\n                    # only detect ground truth detection once\n                    if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                        # true positive and add this bounding box to seen\n                        TP[detection_idx] = 1\n                        amount_bboxes[detection[0]][best_gt_idx] = 1\n                    else:\n                        FP[detection_idx] = 1\n\n                # if IOU is lower then the detection is a false positive\n                else:\n                    FP[detection_idx] = 1\n\n            TP_cumsum = torch.cumsum(TP, dim=0)\n            FP_cumsum = torch.cumsum(FP, dim=0)\n            recalls = TP_cumsum / (total_true_bboxes + self.epsilon)\n            precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + self.epsilon))\n            precisions = torch.cat((torch.tensor([1]), precisions))\n            recalls = torch.cat((torch.tensor([0]), recalls))\n            # torch.trapz for numerical integration\n            self.average_precisions.append(torch.trapz(precisions, recalls))\n\n\ncbs = [\n    TrainCB(),\n    DeviceCB(),\n    MetricsCB(MeanAP(num_classes=20)),\n]\nopt = partial(torch.optim.AdamW, betas=(0.9,0.95), eps=1e-5)\n\n\nmodel = Yolov1(split_size=7, num_boxes=2, num_classes=20)\n\nlr, epochs = 1e-6, 20\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\n\nlearn = Learner(model, dls, YoloLoss(), lr=lr, cbs=cbs+xtra, opt_func=torch.optim.AdamW)\n\nlearn.show_image_batch needs to be fixed.\n\nlearn.show_image_batch()\n\n\n\n\n\n\n\n\n\nlearn.fit(epochs, cbs=[ProgressCB(plot=True)])\n\n\n\n\n\n\n\n\nMeanAP\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.000\n4073.525\n0\ntrain\n06:23\n\n\n0.000\n6326.053\n0\neval\n00:59\n\n\n0.002\n2554.204\n1\ntrain\n03:32\n\n\n0.014\n3933.714\n1\neval\n00:42\n\n\n0.030\n1555.705\n2\ntrain\n03:55\n\n\n0.068\n2416.115\n2\neval\n01:02\n\n\n0.084\n1030.208\n3\ntrain\n04:24\n\n\n0.106\n1800.721\n3\neval\n01:07\n\n\n0.118\n848.246\n4\ntrain\n04:29\n\n\n0.107\n1615.993\n4\neval\n01:05\n\n\n0.123\n787.683\n5\ntrain\n04:23\n\n\n0.106\n1543.894\n5\neval\n01:03\n\n\n0.120\n757.363\n6\ntrain\n04:18\n\n\n0.103\n1502.395\n6\neval\n01:04\n\n\n0.120\n736.230\n7\ntrain\n04:17\n\n\n0.102\n1473.216\n7\neval\n01:03\n\n\n0.118\n716.683\n8\ntrain\n04:13\n\n\n0.100\n1445.808\n8\neval\n01:02\n\n\n0.114\n701.659\n9\ntrain\n04:11\n\n\n0.102\n1425.591\n9\neval\n01:03\n\n\n0.107\n688.869\n10\ntrain\n04:10\n\n\n0.099\n1401.951\n10\neval\n01:07\n\n\n0.111\n678.630\n11\ntrain\n04:14\n\n\n0.103\n1389.554\n11\neval\n01:01\n\n\n0.110\n668.484\n12\ntrain\n04:09\n\n\n0.097\n1374.863\n12\neval\n01:01\n\n\n0.104\n661.353\n13\ntrain\n04:12\n\n\n0.096\n1368.327\n13\neval\n01:00\n\n\n0.104\n654.398\n14\ntrain\n04:11\n\n\n0.098\n1362.825\n14\neval\n01:01\n\n\n0.104\n649.071\n15\ntrain\n04:11\n\n\n0.093\n1358.246\n15\neval\n01:01\n\n\n0.101\n646.394\n16\ntrain\n04:10\n\n\n0.096\n1356.344\n16\neval\n01:01\n\n\n0.106\n644.981\n17\ntrain\n04:08\n\n\n0.096\n1351.769\n17\neval\n01:01\n\n\n0.104\n642.134\n18\ntrain\n04:09\n\n\n0.097\n1352.166\n18\neval\n01:02\n\n\n0.102\n641.413\n19\ntrain\n04:09\n\n\n0.090\n1348.824\n19\neval\n01:01",
    "crumbs": [
      "Yolo v1 using minai"
    ]
  },
  {
    "objectID": "yolov1_miniai.html#testing",
    "href": "yolov1_miniai.html#testing",
    "title": "Yolo v1 using minai",
    "section": "Testing",
    "text": "Testing\nLet’s see if the model learned about bounding boxes for the training set.\nLoading the weight from previous one:\n\n# learn.opt_func = torch.optim.Adam(\n#     learn.model.parameters(), lr=lr, weight_decay=0\n# )\n\nload_checkpoint(torch.load(\"yolov1_1e-6_20.pth.tar\", map_location=torch.device('cpu'), weights_only=True),\n                learn.model, learn.opt)\n\n=&gt; Loading checkpoint\n\n\n\nlearn.model.train(False)\nfor i in range(8):\n    x0, y0 = trn_ds[i]\n    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))\n    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')\n    \n    compare_ims(plot_image(x0, bboxes), plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[19], line 7\n      4 bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))\n      5 bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')\n----&gt; 7 compare_ims(plot_image(x0, bboxes), plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0]))\n\nFile ~/git/pilus_project/pilus_project/core.py:119, in compare_ims(img1, img2)\n    117 plt.figure(figsize=(12,6))\n    118 plt.subplot(121)\n--&gt; 119 plt.imshow(img1, cmap='gray')\n    120 plt.title('img1')\n    121 plt.axis('off')\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/pyplot.py:3592, in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\n   3570 @_copy_docstring_and_deprecators(Axes.imshow)\n   3571 def imshow(\n   3572     X: ArrayLike | PIL.Image.Image,\n   (...)   3590     **kwargs,\n   3591 ) -&gt; AxesImage:\n-&gt; 3592     __ret = gca().imshow(\n   3593         X,\n   3594         cmap=cmap,\n   3595         norm=norm,\n   3596         aspect=aspect,\n   3597         interpolation=interpolation,\n   3598         alpha=alpha,\n   3599         vmin=vmin,\n   3600         vmax=vmax,\n   3601         colorizer=colorizer,\n   3602         origin=origin,\n   3603         extent=extent,\n   3604         interpolation_stage=interpolation_stage,\n   3605         filternorm=filternorm,\n   3606         filterrad=filterrad,\n   3607         resample=resample,\n   3608         url=url,\n   3609         **({\"data\": data} if data is not None else {}),\n   3610         **kwargs,\n   3611     )\n   3612     sci(__ret)\n   3613     return __ret\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/__init__.py:1521, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1518 @functools.wraps(func)\n   1519 def inner(ax, *args, data=None, **kwargs):\n   1520     if data is None:\n-&gt; 1521         return func(\n   1522             ax,\n   1523             *map(cbook.sanitize_sequence, args),\n   1524             **{k: cbook.sanitize_sequence(v) for k, v in kwargs.items()})\n   1526     bound = new_sig.bind(ax, *args, **kwargs)\n   1527     auto_label = (bound.arguments.get(label_namer)\n   1528                   or bound.kwargs.get(label_namer))\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5945, in Axes.imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\n   5942 if aspect is not None:\n   5943     self.set_aspect(aspect)\n-&gt; 5945 im.set_data(X)\n   5946 im.set_alpha(alpha)\n   5947 if im.get_clip_path() is None:\n   5948     # image does not already have clipping set, clip to Axes patch\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/image.py:675, in _ImageBase.set_data(self, A)\n    673 if isinstance(A, PIL.Image.Image):\n    674     A = pil_to_array(A)  # Needed e.g. to apply png palette.\n--&gt; 675 self._A = self._normalize_image_array(A)\n    676 self._imcache = None\n    677 self.stale = True\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/matplotlib/image.py:638, in _ImageBase._normalize_image_array(A)\n    636 A = cbook.safe_masked_invalid(A, copy=True)\n    637 if A.dtype != np.uint8 and not np.can_cast(A.dtype, float, \"same_kind\"):\n--&gt; 638     raise TypeError(f\"Image data of dtype {A.dtype} cannot be \"\n    639                     f\"converted to float\")\n    640 if A.ndim == 3 and A.shape[-1] == 1:\n    641     A = A.squeeze(-1)  # If just (M, N, 1), assume scalar and apply colormap.\n\nTypeError: Image data of dtype object cannot be converted to float\n\n\n\n\n\n\n\n\n\n\n\nlearn.model.train(False)\nfor i in range(8):\n    x0, y0 = val_ds[i]\n    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))\n    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')\n    plot_image(x0, bboxes)\n    plot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat doesn’t look that bad! Let’s save the model states.\n\ncheckpoint = {\n   \"state_dict\": learn.model.state_dict(),\n   \"optimizer\": learn.opt.state_dict(),\n}\ntorch.save(checkpoint, \"yolov1_1e-6_20.pth.tar\")",
    "crumbs": [
      "Yolo v1 using minai"
    ]
  },
  {
    "objectID": "darknet.html",
    "href": "darknet.html",
    "title": "Darknet",
    "section": "",
    "text": "Darknet19 is very simple backbone for image detection model, yolov2. To learn how image detections work with bounding boxes, this seems like a good place to start.\nfrom datasets import load_dataset, load_dataset_builder\nfrom torcheval.metrics import  MulticlassAccuracy\nimport torchvision.transforms.v2.functional as TF\n\nimport fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\n\nimport httpx\nfrom minai import *\nset_seed(42)",
    "crumbs": [
      "Darknet"
    ]
  },
  {
    "objectID": "darknet.html#data",
    "href": "darknet.html#data",
    "title": "Darknet",
    "section": "Data",
    "text": "Data\nWe use imagenet tiny to test our darknet before moving on to the actual imagenet with 1000 classes. We will grab images from huggingface datasets.\n\nds_name = 'zh-plus/tiny-imagenet'\ndsd = load_dataset(ds_name)\ndsd\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 100000\n    })\n    valid: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nUnfortunately, some images are gray images (1 channel) instead of colors (3 channels). We repeat channels to make it 3 channels. We also make the images to be scaled to between 0 and 1.\n\ndef transforms(b):\n    def process_image(img):\n        tensor = TF.to_dtype(TF.to_image(img), dtype=torch.float32, scale=True)\n        if tensor.shape[0] == 1: tensor = tensor.repeat(3, 1, 1) \n        return tensor\n    \n    b['image'] = [process_image(o) for o in b['image']]\n    return b\n\n\nds = dsd.with_transform(transforms)\ndls = DataLoaders.from_dd(ds,batch_size=64)\nxb, yb = next(iter(dls.train))\nxb.shape, yb.shape, yb[:5]\n\n(torch.Size([64, 3, 64, 64]),\n torch.Size([64]),\n tensor([ 82,  83, 124, 184, 169]))\n\n\n\nyb[:5]\n\ntensor([ 82,  83, 124, 184, 169])\n\n\n\nlbls = np.array(ds['train'].features['label'].names)\nlbls[:5]\n\narray(['n01443537', 'n01629819', 'n01641577', 'n01644900', 'n01698640'],\n      dtype='&lt;U9')\n\n\nSo, the labels are not very distinguishable. Let’s get the actual labels.\n\nimport ast\n\n\nres = httpx.get('https://huggingface.co/datasets/zh-plus/tiny-imagenet/raw/main/classes.py')\ncls = ast.literal_eval(res.text.split('=')[1])\ndict(list(cls.items())[:5])\n\n{'n00001740': 'entity',\n 'n00001930': 'physical entity',\n 'n00002137': 'abstraction, abstract entity',\n 'n00002452': 'thing',\n 'n00002684': 'object, physical object'}\n\n\n\ndef get_lbls(yb): return [cls[o].split(',')[0] for o in lbls[yb]]\n\n\nget_lbls(yb[:5])\n\n['Christmas stocking', 'cliff dwelling', 'poncho', 'bee', 'ice lolly']\n\n\n\nshow_images(xb[:5], ncols=5, titles=get_lbls(yb[:5]))\n\n\n\n\n\n\n\n\nThat looks about right.",
    "crumbs": [
      "Darknet"
    ]
  },
  {
    "objectID": "darknet.html#basic-model",
    "href": "darknet.html#basic-model",
    "title": "Darknet",
    "section": "Basic model",
    "text": "Basic model\nBefore using the darknet, I would like to use a simple model to get the training going.\n\nsource\n\nget_simple_model\n\n get_simple_model ()\n\n\nmodel = get_simple_model()\n\n\ncbs = [\n    TrainCB(), # Handles the core steps in the training loop. Can be left out if using TrainLearner\n    DeviceCB(), # Handles making sure data and model are on the right device\n    MetricsCB(accuracy=MulticlassAccuracy()), # Keep track of any relevant metrics\n    ProgressCB(), # Displays metrics and loss during training, optionally plot=True for a pretty graph\n]\n\n\nloss_fn = nn.CrossEntropyLoss()\nlearn = Learner(model, dls, loss_fn, lr=8, cbs=cbs)\nlearn.show_image_batch()\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      0.00% [0/1563 00:00&lt;?]\n    \n    \n\n\n\n\n\n\n\n\n\n\nlearn.summary()\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      0.00% [0/1563 00:00&lt;?]\n    \n    \n\n\nTot params: 15600; MFLOPS: 14.4\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nConv2d\n(64, 3, 64, 64)\n(64, 200, 31, 31)\n15200\n14.4\n\n\nBatchNorm2d\n(64, 200, 31, 31)\n(64, 200, 31, 31)\n400\n0.0\n\n\nReLU\n(64, 200, 31, 31)\n(64, 200, 31, 31)\n0\n0.0\n\n\nAdaptiveAvgPool2d\n(64, 200, 31, 31)\n(64, 200, 1, 1)\n0\n0.0\n\n\nFlatten\n(64, 200, 1, 1)\n(64, 200)\n0\n0.0\n\n\n\n\n\n\nlearn.lr_find(gamma=1.7)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n    \n      \n      2.24% [35/1563 00:01&lt;01:22 15.285]\n    \n    \n\n\n\n\n\n\n\n\n\n\nlearn.fit(3)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.011\n5.293\n0\ntrain\n01:04\n\n\n0.013\n5.283\n0\neval\n00:03\n\n\n0.015\n5.280\n1\ntrain\n01:05\n\n\n0.014\n5.274\n1\neval\n00:03\n\n\n0.017\n5.272\n2\ntrain\n01:07\n\n\n0.017\n5.266\n2\neval\n00:03\n\n\n\n\n\nGood. It trains. So, the dataloader is set properly. Let’s move on to darknet.",
    "crumbs": [
      "Darknet"
    ]
  },
  {
    "objectID": "darknet.html#darknet",
    "href": "darknet.html#darknet",
    "title": "Darknet",
    "section": "Darknet",
    "text": "Darknet\nLet’s try darknet with a simple classification head.\n\nsource\n\nConvBlock\n\n ConvBlock (in_ch, out_ch, ks=3, use_norm=True, use_act=True,\n            act=LeakyReLU(negative_slope=0.1))\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nget_darknet19\n\n get_darknet19 (conv=&lt;class '__main__.ConvBlock'&gt;,\n                pool=MaxPool2d(kernel_size=2, stride=2, padding=0,\n                dilation=1, ceil_mode=False))\n\n\nhead = nn.Sequential(\n    ConvBlock(1024, 200, ks=1),\n    nn.AdaptiveAvgPool2d(1),\n    nn.Flatten()\n)\nhead\n\nSequential(\n  (0): ConvBlock(\n    (conv): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (1): AdaptiveAvgPool2d(output_size=1)\n  (2): Flatten(start_dim=1, end_dim=-1)\n)\n\n\n\nget_darknet19()+head\n\nSequential(\n  (0): ConvBlock(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (2): ConvBlock(\n    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (4): ConvBlock(\n    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (5): ConvBlock(\n    (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (6): ConvBlock(\n    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (8): ConvBlock(\n    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (9): ConvBlock(\n    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (10): ConvBlock(\n    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (12): ConvBlock(\n    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (13): ConvBlock(\n    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (14): ConvBlock(\n    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (15): ConvBlock(\n    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (16): ConvBlock(\n    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (18): ConvBlock(\n    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (19): ConvBlock(\n    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (20): ConvBlock(\n    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (21): ConvBlock(\n    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (22): ConvBlock(\n    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (23): ConvBlock(\n    (conv): Conv2d(1024, 200, kernel_size=(1, 1), stride=(1, 1))\n    (norm): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): LeakyReLU(negative_slope=0.1)\n  )\n  (24): AdaptiveAvgPool2d(output_size=1)\n  (25): Flatten(start_dim=1, end_dim=-1)\n)\n\n\n\nlearn = Learner(get_darknet19()+head, dls, loss_fn, lr=0.3, cbs=cbs)\nlearn.summary()\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      0.00% [0/1563 00:00&lt;?]\n    \n    \n\n\nTot params: 20029976; MFLOPS: 224.6\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nConvBlock\n(64, 3, 64, 64)\n(64, 32, 64, 64)\n960\n3.5\n\n\nMaxPool2d\n(64, 32, 64, 64)\n(64, 32, 32, 32)\n0\n0.0\n\n\nMaxPool2d\n(64, 32, 64, 64)\n(64, 32, 32, 32)\n0\n0.0\n\n\nMaxPool2d\n(64, 32, 64, 64)\n(64, 32, 32, 32)\n0\n0.0\n\n\nMaxPool2d\n(64, 32, 64, 64)\n(64, 32, 32, 32)\n0\n0.0\n\n\nMaxPool2d\n(64, 32, 64, 64)\n(64, 32, 32, 32)\n0\n0.0\n\n\nConvBlock\n(64, 32, 32, 32)\n(64, 64, 32, 32)\n18624\n18.9\n\n\nMaxPool2d\n(64, 64, 32, 32)\n(64, 64, 16, 16)\n0\n0.0\n\n\nMaxPool2d\n(64, 64, 32, 32)\n(64, 64, 16, 16)\n0\n0.0\n\n\nMaxPool2d\n(64, 64, 32, 32)\n(64, 64, 16, 16)\n0\n0.0\n\n\nMaxPool2d\n(64, 64, 32, 32)\n(64, 64, 16, 16)\n0\n0.0\n\n\nMaxPool2d\n(64, 64, 32, 32)\n(64, 64, 16, 16)\n0\n0.0\n\n\nConvBlock\n(64, 64, 16, 16)\n(64, 128, 16, 16)\n74112\n18.9\n\n\nConvBlock\n(64, 128, 16, 16)\n(64, 64, 16, 16)\n8384\n2.1\n\n\nConvBlock\n(64, 64, 16, 16)\n(64, 128, 16, 16)\n74112\n18.9\n\n\nMaxPool2d\n(64, 128, 16, 16)\n(64, 128, 8, 8)\n0\n0.0\n\n\nMaxPool2d\n(64, 128, 16, 16)\n(64, 128, 8, 8)\n0\n0.0\n\n\nMaxPool2d\n(64, 128, 16, 16)\n(64, 128, 8, 8)\n0\n0.0\n\n\nMaxPool2d\n(64, 128, 16, 16)\n(64, 128, 8, 8)\n0\n0.0\n\n\nMaxPool2d\n(64, 128, 16, 16)\n(64, 128, 8, 8)\n0\n0.0\n\n\nConvBlock\n(64, 128, 8, 8)\n(64, 256, 8, 8)\n295680\n18.9\n\n\nConvBlock\n(64, 256, 8, 8)\n(64, 128, 8, 8)\n33152\n2.1\n\n\nConvBlock\n(64, 128, 8, 8)\n(64, 256, 8, 8)\n295680\n18.9\n\n\nMaxPool2d\n(64, 256, 8, 8)\n(64, 256, 4, 4)\n0\n0.0\n\n\nMaxPool2d\n(64, 256, 8, 8)\n(64, 256, 4, 4)\n0\n0.0\n\n\nMaxPool2d\n(64, 256, 8, 8)\n(64, 256, 4, 4)\n0\n0.0\n\n\nMaxPool2d\n(64, 256, 8, 8)\n(64, 256, 4, 4)\n0\n0.0\n\n\nMaxPool2d\n(64, 256, 8, 8)\n(64, 256, 4, 4)\n0\n0.0\n\n\nConvBlock\n(64, 256, 4, 4)\n(64, 512, 4, 4)\n1181184\n18.9\n\n\nConvBlock\n(64, 512, 4, 4)\n(64, 256, 4, 4)\n131840\n2.1\n\n\nConvBlock\n(64, 256, 4, 4)\n(64, 512, 4, 4)\n1181184\n18.9\n\n\nConvBlock\n(64, 512, 4, 4)\n(64, 256, 4, 4)\n131840\n2.1\n\n\nConvBlock\n(64, 256, 4, 4)\n(64, 512, 4, 4)\n1181184\n18.9\n\n\nMaxPool2d\n(64, 512, 4, 4)\n(64, 512, 2, 2)\n0\n0.0\n\n\nMaxPool2d\n(64, 512, 4, 4)\n(64, 512, 2, 2)\n0\n0.0\n\n\nMaxPool2d\n(64, 512, 4, 4)\n(64, 512, 2, 2)\n0\n0.0\n\n\nMaxPool2d\n(64, 512, 4, 4)\n(64, 512, 2, 2)\n0\n0.0\n\n\nMaxPool2d\n(64, 512, 4, 4)\n(64, 512, 2, 2)\n0\n0.0\n\n\nConvBlock\n(64, 512, 2, 2)\n(64, 1024, 2, 2)\n4721664\n18.9\n\n\nConvBlock\n(64, 1024, 2, 2)\n(64, 512, 2, 2)\n525824\n2.1\n\n\nConvBlock\n(64, 512, 2, 2)\n(64, 1024, 2, 2)\n4721664\n18.9\n\n\nConvBlock\n(64, 1024, 2, 2)\n(64, 512, 2, 2)\n525824\n2.1\n\n\nConvBlock\n(64, 512, 2, 2)\n(64, 1024, 2, 2)\n4721664\n18.9\n\n\nConvBlock\n(64, 1024, 2, 2)\n(64, 200, 2, 2)\n205400\n0.8\n\n\nAdaptiveAvgPool2d\n(64, 200, 2, 2)\n(64, 200, 1, 1)\n0\n0.0\n\n\nFlatten\n(64, 200, 1, 1)\n(64, 200)\n0\n0.0\n\n\n\n\n\n\n# lr = 1\nlearn.fit(10)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.024\n4.961\n0\ntrain\n00:30\n\n\n0.037\n4.760\n0\neval\n00:02\n\n\n0.061\n4.454\n1\ntrain\n00:30\n\n\n0.044\n4.779\n1\neval\n00:01\n\n\n0.082\n4.209\n2\ntrain\n00:30\n\n\n0.070\n4.322\n2\neval\n00:01\n\n\n0.103\n4.035\n3\ntrain\n00:30\n\n\n0.102\n4.119\n3\neval\n00:01\n\n\n0.131\n3.830\n4\ntrain\n00:30\n\n\n0.131\n3.840\n4\neval\n00:01\n\n\n0.159\n3.650\n5\ntrain\n00:30\n\n\n0.136\n3.868\n5\neval\n00:01\n\n\n0.185\n3.499\n6\ntrain\n00:30\n\n\n0.137\n3.937\n6\neval\n00:01\n\n\n0.206\n3.379\n7\ntrain\n00:31\n\n\n0.171\n3.708\n7\neval\n00:01\n\n\n0.227\n3.256\n8\ntrain\n00:31\n\n\n0.092\n4.795\n8\neval\n00:01\n\n\n0.249\n3.146\n9\ntrain\n00:31\n\n\n0.231\n3.292\n9\neval\n00:01\n\n\n\n\n\n\n# lr = 0.1\nlearn.fit(10)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.039\n4.780\n0\ntrain\n00:30\n\n\n0.057\n4.529\n0\neval\n00:01\n\n\n0.092\n4.218\n1\ntrain\n00:30\n\n\n0.044\n5.159\n1\neval\n00:01\n\n\n0.137\n3.904\n2\ntrain\n00:31\n\n\n0.135\n3.906\n2\neval\n00:01\n\n\n0.163\n3.705\n3\ntrain\n00:31\n\n\n0.113\n4.146\n3\neval\n00:01\n\n\n0.185\n3.565\n4\ntrain\n00:31\n\n\n0.166\n3.697\n4\neval\n00:01\n\n\n0.206\n3.444\n5\ntrain\n00:31\n\n\n0.122\n4.253\n5\neval\n00:01\n\n\n0.224\n3.347\n6\ntrain\n00:30\n\n\n0.208\n3.458\n6\neval\n00:01\n\n\n0.239\n3.251\n7\ntrain\n00:30\n\n\n0.196\n3.567\n7\neval\n00:01\n\n\n0.257\n3.167\n8\ntrain\n00:30\n\n\n0.168\n3.895\n8\neval\n00:01\n\n\n0.273\n3.078\n9\ntrain\n00:30\n\n\n0.178\n3.765\n9\neval\n00:01\n\n\n\n\n\n\n\noptimization\nLet’s try to use different optimizers, such as AdamW.\n\nfrom functools import partial\nfrom torch import optim\n\n\n# optim.AdamW?\n\n\nopt_func = partial(optim.AdamW, eps=1e-5)\nmodel = get_darknet19()+head\nlearn = Learner(model, dls, loss_fn, lr=0.1, cbs=cbs, opt_func=opt_func)\nlearn.lr_find()\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n    \n      \n      3.58% [56/1563 00:01&lt;00:32 8.154]\n    \n    \n\n\n\n\n\n\n\n\n\n\nmodel = get_darknet19()+head\nlearn = Learner(model, dls, loss_fn, lr=0.1, cbs=cbs, opt_func=opt_func)\n\n\nlearn.fit(10)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.012\n5.155\n0\ntrain\n00:33\n\n\n0.011\n5.270\n0\neval\n00:01\n\n\n0.015\n5.071\n1\ntrain\n00:33\n\n\n0.012\n5.314\n1\neval\n00:01\n\n\n0.015\n5.047\n2\ntrain\n00:33\n\n\n0.012\n5.279\n2\neval\n00:01\n\n\n0.020\n4.963\n3\ntrain\n00:33\n\n\n0.008\n6.116\n3\neval\n00:02\n\n\n0.026\n4.854\n4\ntrain\n00:33\n\n\n0.023\n5.260\n4\neval\n00:01\n\n\n0.028\n4.826\n5\ntrain\n00:33\n\n\n0.012\n5.225\n5\neval\n00:01\n\n\n0.028\n4.821\n6\ntrain\n00:33\n\n\n0.009\n5.513\n6\neval\n00:01\n\n\n0.028\n4.819\n7\ntrain\n00:33\n\n\n0.018\n5.326\n7\neval\n00:01\n\n\n0.028\n4.813\n8\ntrain\n00:33\n\n\n0.014\n5.297\n8\neval\n00:01\n\n\n0.028\n4.815\n9\ntrain\n00:34\n\n\n0.020\n4.995\n9\neval\n00:01",
    "crumbs": [
      "Darknet"
    ]
  },
  {
    "objectID": "yolov1_miniai_bac.html",
    "href": "yolov1_miniai_bac.html",
    "title": "Yolo v1 on bacteria",
    "section": "",
    "text": "import torch\nimport torchvision.transforms as transforms\nfrom torch.optim import lr_scheduler\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport fastcore.all as fc\n\nfrom functools import partial\nfrom collections import Counter\n\nfrom minai import *\nfrom pilus_project.yolov1 import *\nset_seed(42)\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\nIMG_DIR = \"../data/images\"\nLABEL_DIR = \"../data/labels\"\n\nbs = 8\n!ls ..\n\n8examples.csv      _proc        pilus_project       test.csv\n8examples_val.csv  data         pilus_project.egg-info  train.all.txt\nLICENSE        generate_csv.py  pyproject.toml      train.csv\nMANIFEST.in    nbs          settings.ini        voc_dataset.sh\nREADME.md      old_txt_files    setup.py            voc_label.py\npd.read_csv(\"../8examples.csv\")\n\n\n\n\n\n\n\n\nimg\nlabel\n\n\n\n\n0\n000007.jpg\n000007.txt\n\n\n1\n000009.jpg\n000009.txt\n\n\n2\n000016.jpg\n000016.txt\n\n\n3\n000019.jpg\n000019.txt\n\n\n4\n000020.jpg\n000020.txt\n\n\n5\n000021.jpg\n000021.txt\n\n\n6\n000122.jpg\n000122.txt\n\n\n7\n000129.jpg\n000129.txt\ntrn_ds = VOCDataset(\"../8examples.csv\", \n                    transform=transform,\n                    img_dir=IMG_DIR,\n                    label_dir=LABEL_DIR)\nx0, y0 = trn_ds[0]\nplot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])\n# examples_val_8 = pd.read_csv(\"../test.csv\").iloc[:8]\n# examples_val_8\n# examples_val_8.to_csv('../8examples_val.csv', index=False)\nval_ds = VOCDataset(\n    \"../8examples_val.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n)\nx0, y0 = val_ds[0]\nplot_image(x0, cellboxes_to_boxes(y0.unsqueeze(0))[0])\nlen(val_ds)\n\n8\ntrn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\nxb, yb = next(iter(trn_dl))\nxb.shape, yb.shape\n\n(torch.Size([8, 3, 448, 448]), torch.Size([8, 7, 7, 30]))\ndls = DataLoaders(trn_dl, val_dl)",
    "crumbs": [
      "Yolo v1 on bacteria"
    ]
  },
  {
    "objectID": "yolov1_miniai_bac.html#learner",
    "href": "yolov1_miniai_bac.html#learner",
    "title": "Yolo v1 on bacteria",
    "section": "Learner",
    "text": "Learner\n\nclass MeanAP:\n    def __init__(self, num_classes=1, epsilon=1e-6, threshold=0.4, iou_threshold=0.5, box_format='midpoint'):\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n        self.box_format = box_format\n        self.average_precisions = []\n    \n    def reset(self):\n        self.average_precisions = []\n    \n    def compute(self):\n        return sum(self.average_precisions) / len(self.average_precisions)\n    \n    def update(self, pred, label):\n        # `get_bboxes` part\n        all_pred_boxes = []\n        all_true_boxes = []\n        train_idx = 0\n        batch_size = pred.shape[0]\n        pred_boxes = cellboxes_to_boxes(pred)\n        true_boxes = cellboxes_to_boxes(label)\n        \n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                pred_boxes[idx],\n                iou_threshold=self.iou_threshold,\n                threshold=self.threshold,\n                box_format=self.box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_boxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] &gt; self.threshold:\n                    all_true_boxes.append([train_idx] + box)\n        \n        pred_boxes = all_pred_boxes\n        true_boxes = all_true_boxes\n        \n        for c in range(self.num_classes):\n            detections = []\n            ground_truths = []\n\n            # Go through all predictions and targets,\n            # and only add the ones that belong to the\n            # current class c\n            for detection in pred_boxes:\n                if detection[1] == c:\n                    detections.append(detection)\n\n            for true_box in true_boxes:\n                if true_box[1] == c:\n                    ground_truths.append(true_box)\n\n            # find the amount of bboxes for each training example\n            # Counter here finds how many ground truth bboxes we get\n            # for each training example, so let's say img 0 has 3,\n            # img 1 has 5 then we will obtain a dictionary with:\n            # amount_bboxes = {0:3, 1:5}\n            amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n            # We then go through each key, val in this dictionary\n            # and convert to the following (w.r.t same example):\n            # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n            for key, val in amount_bboxes.items():\n                amount_bboxes[key] = torch.zeros(val)\n\n            # sort by box probabilities which is index 2\n            detections.sort(key=lambda x: x[2], reverse=True)\n            TP = torch.zeros((len(detections)))\n            FP = torch.zeros((len(detections)))\n            total_true_bboxes = len(ground_truths)\n\n            # If none exists for this class then we can safely skip\n            if total_true_bboxes == 0:\n                continue\n\n            for detection_idx, detection in enumerate(detections):\n                # Only take out the ground_truths that have the same\n                # training idx as detection\n                ground_truth_img = [\n                    bbox for bbox in ground_truths if bbox[0] == detection[0]\n                ]\n\n                num_gts = len(ground_truth_img)\n                best_iou = 0\n\n                for idx, gt in enumerate(ground_truth_img):\n                    iou = intersection_over_union(\n                        torch.tensor(detection[3:]),\n                        torch.tensor(gt[3:]),\n                        box_format=self.box_format,\n                    )\n\n                    if iou &gt; best_iou:\n                        best_iou = iou\n                        best_gt_idx = idx\n\n                if best_iou &gt; self.iou_threshold:\n                    # only detect ground truth detection once\n                    if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                        # true positive and add this bounding box to seen\n                        TP[detection_idx] = 1\n                        amount_bboxes[detection[0]][best_gt_idx] = 1\n                    else:\n                        FP[detection_idx] = 1\n\n                # if IOU is lower then the detection is a false positive\n                else:\n                    FP[detection_idx] = 1\n\n            TP_cumsum = torch.cumsum(TP, dim=0)\n            FP_cumsum = torch.cumsum(FP, dim=0)\n            recalls = TP_cumsum / (total_true_bboxes + self.epsilon)\n            precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + self.epsilon))\n            precisions = torch.cat((torch.tensor([1]), precisions))\n            recalls = torch.cat((torch.tensor([0]), recalls))\n            # torch.trapz for numerical integration\n            self.average_precisions.append(torch.trapz(precisions, recalls))\n\n\ncbs = [\n    TrainCB(),\n    DeviceCB(),\n    MetricsCB(MeanAP(num_classes=20)),\n]\nopt = partial(torch.optim.AdamW, betas=(0.9,0.95), eps=1e-5)\n\n\nmodel = Yolov1(split_size=7, num_boxes=2, num_classes=20)\n\nlr, epochs = 1e-5, 150\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\n\nlearn = Learner(model, dls, YoloLoss(), lr=lr, cbs=cbs+xtra, opt_func=torch.optim.AdamW)\n\n\nlearn.show_image_batch()\n\n\n\n\n\n\n\n\n\nlearn.fit(epochs, cbs=[ProgressCB(plot=True)])\n\n\n\n\n\n\n      \n      0.00% [0/150 00:00&lt;?]\n    \n    \n\n\n\n\nMeanAP\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.000\n648.640\n0\ntrain\n00:04\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 learn.fit(epochs, cbs=[ProgressCB(plot=True)])\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:264, in Learner.fit(self, n_epochs, train, valid, cbs, lr)\n    262     if lr is None: lr = self.lr\n    263     if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n--&gt; 264     self._fit(train, valid)\n    265 finally:\n    266     for cb in cbs: self.cbs.remove(cb)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:198, in with_cbs.__call__.&lt;locals&gt;._f(o, *args, **kwargs)\n    196 try:\n    197     o.callback(f'before_{self.nm}')\n--&gt; 198     f(o, *args, **kwargs)\n    199     o.callback(f'after_{self.nm}')\n    200 except globals()[f'Cancel{self.nm.title()}Exception']: pass\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:254, in Learner._fit(self, train, valid)\n    252 if train: self.one_epoch(True)\n    253 if valid:\n--&gt; 254     with torch.inference_mode(): self.one_epoch(False)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:245, in Learner.one_epoch(self, training)\n    243 self.model.train(training)\n    244 self.dl = self.train_dl if training else self.dls.valid\n--&gt; 245 self._one_epoch()\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:198, in with_cbs.__call__.&lt;locals&gt;._f(o, *args, **kwargs)\n    196 try:\n    197     o.callback(f'before_{self.nm}')\n--&gt; 198     f(o, *args, **kwargs)\n    199     o.callback(f'after_{self.nm}')\n    200 except globals()[f'Cancel{self.nm.title()}Exception']: pass\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/minai/core.py:240, in Learner._one_epoch(self)\n    238 @with_cbs('epoch')\n    239 def _one_epoch(self):\n--&gt; 240     for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/fastprogress/fastprogress.py:41, in ProgressBar.__iter__(self)\n     39 if self.total != 0: self.update(0)\n     40 try:\n---&gt; 41     for i,o in enumerate(self.gen):\n     42         if self.total and i &gt;= self.total: break\n     43         yield o\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701, in _BaseDataLoaderIter.__next__(self)\n    698 if self._sampler_iter is None:\n    699     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    700     self._reset()  # type: ignore[call-arg]\n--&gt; 701 data = self._next_data()\n    702 self._num_yielded += 1\n    703 if (\n    704     self._dataset_kind == _DatasetKind.Iterable\n    705     and self._IterableDataset_len_called is not None\n    706     and self._num_yielded &gt; self._IterableDataset_len_called\n    707 ):\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757, in _SingleProcessDataLoaderIter._next_data(self)\n    755 def _next_data(self):\n    756     index = self._next_index()  # may raise StopIteration\n--&gt; 757     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    758     if self._pin_memory:\n    759         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n     50         data = self.dataset.__getitems__(possibly_batched_index)\n     51     else:\n---&gt; 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52, in &lt;listcomp&gt;(.0)\n     50         data = self.dataset.__getitems__(possibly_batched_index)\n     51     else:\n---&gt; 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n\nFile ~/git/pilus_project/pilus_project/yolov1.py:405, in VOCDataset.__getitem__(self, index)\n    401 boxes = torch.tensor(boxes)\n    403 if self.transform:\n    404     # image = self.transform(image)\n--&gt; 405     image, boxes = self.transform(image, boxes)\n    407 # Convert To Cells\n    408 label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n\nFile ~/git/pilus_project/pilus_project/yolov1.py:692, in Compose.__call__(self, img, bboxes)\n    690 def __call__(self, img, bboxes):\n    691     for t in self.transforms:\n--&gt; 692         img, bboxes = t(img), bboxes\n    694     return img, bboxes\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/transforms.py:354, in Resize.forward(self, img)\n    346 def forward(self, img):\n    347     \"\"\"\n    348     Args:\n    349         img (PIL Image or Tensor): Image to be scaled.\n   (...)    352         PIL Image or Tensor: Rescaled image.\n    353     \"\"\"\n--&gt; 354     return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/functional.py:477, in resize(img, size, interpolation, max_size, antialias)\n    475         warnings.warn(\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\")\n    476     pil_interpolation = pil_modes_mapping[interpolation]\n--&gt; 477     return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)\n    479 return F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py:250, in resize(img, size, interpolation)\n    247 if not (isinstance(size, list) and len(size) == 2):\n    248     raise TypeError(f\"Got inappropriate size arg: {size}\")\n--&gt; 250 return img.resize(tuple(size[::-1]), interpolation)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/PIL/Image.py:2293, in Image.resize(self, size, resample, box, reducing_gap)\n   2290     msg = \"reducing_gap must be 1.0 or greater\"\n   2291     raise ValueError(msg)\n-&gt; 2293 self.load()\n   2294 if box is None:\n   2295     box = (0, 0) + self.size\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/PIL/ImageFile.py:293, in ImageFile.load(self)\n    290         raise OSError(msg)\n    292 b = b + s\n--&gt; 293 n, err_code = decoder.decode(b)\n    294 if n &lt; 0:\n    295     break\n\nKeyboardInterrupt:",
    "crumbs": [
      "Yolo v1 on bacteria"
    ]
  },
  {
    "objectID": "yolov1_miniai_bac.html#testing",
    "href": "yolov1_miniai_bac.html#testing",
    "title": "Yolo v1 on bacteria",
    "section": "Testing",
    "text": "Testing\nLet’s see if the model learned about bounding boxes for the training set.\nLoading the weight from previous one:\n\n# learn.opt_func = torch.optim.Adam(\n#     learn.model.parameters(), lr=lr, weight_decay=0\n# )\n\n# load_checkpoint(torch.load(\"overfit.pth.tar\"), learn.model, learn.opt_func)\n\n\nlearn.model.train(False)\nfor i in range(8):\n    x0, y0 = trn_ds[i]\n    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))\n    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')\n    plot_image(x0, bboxes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.model.train(False)\nfor i in range(8):\n    x0, y0 = val_ds[i]\n    bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))\n    bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')\n    plot_image(x0, bboxes)",
    "crumbs": [
      "Yolo v1 on bacteria"
    ]
  },
  {
    "objectID": "train.html",
    "href": "train.html",
    "title": "train",
    "section": "",
    "text": "# import ultralytics\n\n\n# from ultralytics import YOLO\nfrom IPython.display import display, Image\nfrom minai import *\nimport fastcore.all as fc\nfrom fastcore.utils import L\nfrom pilus_project.core import *\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.transforms import v2\n\n\nset_seed(42)\n\n\npath = fc.Path.home()/'data/pili/training_data'\npath.ls()\n\n(#18) [Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.nd2'),Path('/home/kappa/data/pili/training_data/1hr01002.csv'),Path('/home/kappa/data/pili/training_data/0N01002.nd2'),Path('/home/kappa/data/pili/training_data/7.1- 003.nd2'),Path('/home/kappa/data/pili/training_data/0.1%.004.nd2'),Path('/home/kappa/data/pili/training_data/4hrs incu004.csv'),Path('/home/kappa/data/pili/training_data/1hr01002.nd2'),Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.csv'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.nd2'),Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.csv'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.csv'),Path('/home/kappa/data/pili/training_data/0N01002.csv'),Path('/home/kappa/data/pili/training_data/4hrs incu004.nd2'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.csv'),Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.nd2'),Path('/home/kappa/data/pili/training_data/7.1- 003.csv'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.nd2'),Path('/home/kappa/data/pili/training_data/0.1%.004.csv')]\n\n\n\nims = L(path.glob('*.nd2'))\nims\n\n(#9) [Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.nd2'),Path('/home/kappa/data/pili/training_data/0N01002.nd2'),Path('/home/kappa/data/pili/training_data/7.1- 003.nd2'),Path('/home/kappa/data/pili/training_data/0.1%.004.nd2'),Path('/home/kappa/data/pili/training_data/1hr01002.nd2'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.nd2'),Path('/home/kappa/data/pili/training_data/4hrs incu004.nd2'),Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.nd2'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.nd2')]\n\n\n\npaths = [(im, im.with_suffix('.csv')) for im in ims]\npaths\n\n[(Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.nd2'),\n  Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.csv')),\n (Path('/home/kappa/data/pili/training_data/0N01002.nd2'),\n  Path('/home/kappa/data/pili/training_data/0N01002.csv')),\n (Path('/home/kappa/data/pili/training_data/7.1- 003.nd2'),\n  Path('/home/kappa/data/pili/training_data/7.1- 003.csv')),\n (Path('/home/kappa/data/pili/training_data/0.1%.004.nd2'),\n  Path('/home/kappa/data/pili/training_data/0.1%.004.csv')),\n (Path('/home/kappa/data/pili/training_data/1hr01002.nd2'),\n  Path('/home/kappa/data/pili/training_data/1hr01002.csv')),\n (Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.nd2'),\n  Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.csv')),\n (Path('/home/kappa/data/pili/training_data/4hrs incu004.nd2'),\n  Path('/home/kappa/data/pili/training_data/4hrs incu004.csv')),\n (Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.nd2'),\n  Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.csv')),\n (Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.nd2'),\n  Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.csv'))]\n\n\n\ncsvs = L(path.glob('*.csv'))\ncsvs\n\n(#9) [Path('/home/kappa/data/pili/training_data/1hr01002.csv'),Path('/home/kappa/data/pili/training_data/4hrs incu004.csv'),Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.csv'),Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.csv'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.csv'),Path('/home/kappa/data/pili/training_data/0N01002.csv'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.csv'),Path('/home/kappa/data/pili/training_data/7.1- 003.csv'),Path('/home/kappa/data/pili/training_data/0.1%.004.csv')]\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrn_path, val_path = train_test_split(paths, test_size=0.2, random_state=42)\nval_path\n\n[(Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.nd2'),\n  Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.csv')),\n (Path('/home/kappa/data/pili/training_data/0N01002.nd2'),\n  Path('/home/kappa/data/pili/training_data/0N01002.csv'))]\n\n\n\nclass PiliDataset(Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_files = list(Path(image_dir).glob('*.nd2'))\n        self.transform = transform\n        \n    def __len__(self): return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = get_im(img_path)\n        label_path = img_path.with_suffix('.csv')\n        corners = calc_corners(str(label_path))\n        \n        if self.transform:\n            img = self.transform(img)\n            \n        return img, corners\n\n\nlist(zip(*val_path))\n\n[(Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.nd2'),\n  Path('/home/kappa/data/pili/training_data/0N01002.nd2')),\n (Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.csv'),\n  Path('/home/kappa/data/pili/training_data/0N01002.csv'))]\n\n\nTODO: Normalize with pretrained data statistics\n\ntfm = v2.Compose([\n    get_im,\n    v2.ToImage(),\n    v2.ConvertImageDtype(torch.float32),\n    v2.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0]==1 else x),\n    v2.Normalize(mean=[0.0048, 0.0048, 0.0048], std=[0.0138, 0.0138, 0.0138]), # statistics from data\n])\n\n\ndef process_img(im): return tfm(get_im(im))\n\n\ntrn_ds = TfmDataset(*zip(*trn_path), tfm_x=tfm, tfm_y=calc_corners)\nval_ds = TfmDataset(*zip(*val_path), tfm_x=tfm, tfm_y=calc_corners)\nx, y = next(iter(trn_ds))\nx.shape, y[:5]\n\n(torch.Size([3, 1952, 1952]),\n tensor([[4.0000, 0.2166, 0.8646, 0.1645, 0.8972, 0.1545, 0.8812, 0.2066, 0.8486],\n         [4.0000, 0.2334, 0.6836, 0.1811, 0.6986, 0.1753, 0.6781, 0.2275, 0.6632],\n         [4.0000, 0.2122, 0.4038, 0.1546, 0.3905, 0.1585, 0.3732, 0.2161, 0.3865],\n         [4.0000, 0.2814, 0.2086, 0.2660, 0.1467, 0.2855, 0.1418, 0.3009, 0.2038],\n         [4.0000, 0.3904, 0.3313, 0.3666, 0.2960, 0.3813, 0.2861, 0.4051, 0.3214]], dtype=torch.float64))\n\n\n\nx[0].shape\n\ntorch.Size([1952, 1952])\n\n\n\nx.mean(), x.std()\n\n(tensor(0.0597), tensor(1.4359))\n\n\n\n# Getting the statistics of data\n# imgs = [x for x,y in trn_ds]\n# timgs = torch.stack(imgs)\n# timgs.shape\n# timgs.mean((0,2,3)), timgs.std((0,2,3))\n\n\n@fc.delegates(plt.Axes.imshow)\ndef imshow_with_boxes(im, boxes, figsize=(8,8), **kwargs):\n    import matplotlib.patches as patches\n    colors = {\n        1: 'red',    # GREEN YELLOW\n        2: 'blue',   # TURQUOISE\n        3: 'green',  # THISTLE \n        4: 'yellow', # COrAL\n        5: 'purple', # WHITE\n        6: 'orange'  # MAGENTA\n    }\n    class_names = {\n        1: 'Type 1 - Both poles free',\n        2: 'Type 2 - One pole occluded',\n        3: 'Type 3 - Both poles occluded',\n        4: 'Type 4 - Super bright cell',\n        5: 'Type 5 - Very dim cell',\n        6: 'Type 6 - Partial cell'\n    }\n\n    if im.shape[0]==3: im=im[0]\n    height, width = im.shape[:2]\n    fig, ax = plt.subplots(figsize=figsize)\n    if fc.hasattrs(im, ('cpu','permute','detach')):\n        im = im.detach().cpu()\n        if len(im.shape)==3 and im.shape[0]&lt;5: im=im.permute(1,2,0)\n    elif not isinstance(im,np.ndarray): im=np.array(im)\n    if im.shape[-1]==1: im=im[...,0]\n    if ax is None: _,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, **kwargs)\n    plt.axis('off')\n\n    unique_classes = boxes[:,0].unique().int().tolist()\n    legend_elements = [patches.Patch(facecolor='none',\n                                   edgecolor=colors[class_idx],\n                                   label=class_names[class_idx])\n                      for class_idx in unique_classes]\n\n    for box in boxes:\n        class_idx = box[0].item()\n        corners = [[box[1].item() * width, box[2].item() * height],\n                  [box[3].item() * width, box[4].item() * height],\n                  [box[5].item() * width, box[6].item() * height],\n                  [box[7].item() * width, box[8].item() * height]]\n\n        color = colors.get(box[0].item(), 'white')\n        polygon = patches.Polygon(corners, fill=False,\n                                edgecolor=color,\n                                linewidth=2)\n        ax.add_patch(polygon)\n\n    ax.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\n\nimshow_with_boxes(x, y)\n\n\n\n\n\n\n\n\n\nbs=2\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle variable number of boxes\"\"\"\n    imgs = torch.stack([item[0] for item in batch]) \n    boxes = [item[1] for item in batch]  \n    return imgs, boxes\n\ntrn_dl, val_dl = dls = get_dls(trn_ds, val_ds, bs=bs, collate_fn=collate_fn)\nxb,yb = next(iter(trn_dl))\nxb.shape,yb[0]\n\n(torch.Size([2, 3, 1952, 1952]),\n tensor([[ 4.0000,  0.4952,  0.1168,  ...,  0.0789,  0.5141,  0.1168],\n         [ 6.0000,  0.4814,  0.0082,  ..., -0.0070,  0.4969,  0.0168],\n         [ 1.0000,  0.5268,  0.0588,  ...,  0.0091,  0.5402,  0.0439],\n         ...,\n         [ 1.0000,  0.7748,  0.4930,  ...,  0.4457,  0.7915,  0.4990],\n         [ 1.0000,  0.7768,  0.4339,  ...,  0.4303,  0.7722,  0.4168],\n         [ 2.0000,  0.7423,  0.4943,  ...,  0.4381,  0.7589,  0.5003]], dtype=torch.float64))\n\n\n\nimport sys, gc, traceback, math, typing, random, numpy as np\nfrom collections.abc import Mapping\nfrom copy import copy\nfrom itertools import zip_longest\nfrom functools import partial, wraps\nfrom operator import attrgetter, itemgetter\n\nimport matplotlib.pyplot as plt\nimport fastcore.all as fc\nfrom fastprogress import progress_bar, master_bar\n\nimport torch, torch.nn.functional as F\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import default_collate\n\nfrom torcheval.metrics import Mean\n\n\ndls = DataLoaders(trn_dl, val_dl)\nxb,yb = next(iter(dls.train))\nxb.shape,yb[0]\n\n(torch.Size([2, 3, 1952, 1952]),\n tensor([[4.0000, 0.9067, 0.2497,  ..., 0.2230, 0.9110, 0.2337],\n         [4.0000, 0.4559, 0.3301,  ..., 0.2846, 0.4734, 0.3230],\n         [4.0000, 0.5351, 0.3602,  ..., 0.3862, 0.5183, 0.3547],\n         ...,\n         [2.0000, 0.5490, 0.3630,  ..., 0.3476, 0.5499, 0.3500],\n         [3.0000, 0.5057, 0.3783,  ..., 0.3380, 0.5198, 0.3793],\n         [1.0000, 0.5676, 0.2528,  ..., 0.2573, 0.5592, 0.2413]], dtype=torch.float64))\n\n\n\nclass YOLOWrapper(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = YOLO(\"yolo11n-obb.pt\").model\n        # Keep the original loss\n        self.loss_fn = self.model.loss\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def train(self, mode=True):\n        \"\"\"Override train method\"\"\"\n        super().train(mode)\n        if hasattr(self.model, 'training'):\n            self.model.training = mode\n        return self\n\n\nmodel = YOLOWrapper()\nmodel\n\nYOLOWrapper(\n  (model): OBBModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (4): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (6): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): C3k(\n            (cv1): Conv(\n              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv3): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (m): Sequential(\n              (0): Bottleneck(\n                (cv1): Conv(\n                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n                (cv2): Conv(\n                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n              )\n              (1): Bottleneck(\n                (cv1): Conv(\n                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n                (cv2): Conv(\n                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (8): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): C3k(\n            (cv1): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv3): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (m): Sequential(\n              (0): Bottleneck(\n                (cv1): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n                (cv2): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n              )\n              (1): Bottleneck(\n                (cv1): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n                (cv2): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): C2PSA(\n        (cv1): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): Sequential(\n          (0): PSABlock(\n            (attn): Attention(\n              (qkv): Conv(\n                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): Identity()\n              )\n              (proj): Conv(\n                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): Identity()\n              )\n              (pe): Conv(\n                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): Identity()\n              )\n            )\n            (ffn): Sequential(\n              (0): Conv(\n                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n              (1): Conv(\n                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): Identity()\n              )\n            )\n          )\n        )\n      )\n      (11): Upsample(scale_factor=2.0, mode='nearest')\n      (12): Concat()\n      (13): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (14): Upsample(scale_factor=2.0, mode='nearest')\n      (15): Concat()\n      (16): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (17): Conv(\n        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (18): Concat()\n      (19): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (20): Conv(\n        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (21): Concat()\n      (22): C3k2(\n        (cv1): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0): C3k(\n            (cv1): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv3): Conv(\n              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (m): Sequential(\n              (0): Bottleneck(\n                (cv1): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n                (cv2): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n              )\n              (1): Bottleneck(\n                (cv1): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n                (cv2): Conv(\n                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                  (act): SiLU(inplace=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (23): OBB(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Sequential(\n              (0): DWConv(\n                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n              (1): Conv(\n                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n            )\n            (1): Sequential(\n              (0): DWConv(\n                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n              (1): Conv(\n                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n            )\n            (2): Conv2d(64, 15, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Sequential(\n              (0): DWConv(\n                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n              (1): Conv(\n                (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n            )\n            (1): Sequential(\n              (0): DWConv(\n                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n              (1): Conv(\n                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n            )\n            (2): Conv2d(64, 15, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Sequential(\n              (0): DWConv(\n                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n              (1): Conv(\n                (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n            )\n            (1): Sequential(\n              (0): DWConv(\n                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n              (1): Conv(\n                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n                (act): SiLU(inplace=True)\n              )\n            )\n            (2): Conv2d(64, 15, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n        (cv4): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): Sequential(\n            (0): Conv(\n              (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (2): Sequential(\n            (0): Conv(\n              (conv): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n      )\n    )\n  )\n)\n\n\n\nfrom shapely.geometry import Polygon\n \ndef create_polygon_from_coordinates(coords):\n    \"\"\"Creating a Polygon object from the coordinates of the rotated bounding box.\"\"\"\n    return Polygon([(coords[i], coords[i+1]) for i in range(0, len(coords), 2)])\n \ndef calculate_iou_rotated_from_coords(box1_coords, box2_coords):\n    \"\"\"Calculation of IoU for two rotated bounding boxes with their corner coordinates.\"\"\"\n    poly1 = create_polygon_from_coordinates(box1_coords)\n    poly2 = create_polygon_from_coordinates(box2_coords)\n   \n    intersection = poly1.intersection(poly2).area\n    union = poly1.area + poly2.area - intersection\n    iou = intersection / union if union &gt; 0 else 0.0\n    return iou\n \nclass RotatedBoundingBoxLoss(nn.Module):\n    def __init__(self, num_classes=5, iou_threshold=0.5):\n        super().__init__()\n        self.iou_threshold = iou_threshold\n        self.num_classes = num_classes\n        self.class_loss = nn.CrossEntropyLoss()  # Classification loss\n        self.regression_loss = nn.SmoothL1Loss()  # Bounding box regression loss\n \n    def forward(self, preds, targets):\n        \"\"\"\n        preds: Tensor of shape (N, num_classes + 9) -&gt; (class_logits, x1, y1, ..., x4, y4, confidence)\n        targets: Tensor of shape (M, 9) -&gt; (class_label, x1, y1, ..., x4, y4)\n        \"\"\"\n \n        total_class_loss = 0\n        total_regression_loss = 0\n        total_iou_loss = 0\n        matched_count = 0\n       \n        pred_logits, pred_boxes = preds[:, :self.num_classes], preds[:, self.num_classes:-1]  # Separate logits and coordinates\n        gt_classes, gt_boxes = targets[:, 0], targets[:, 1:]  # Separate class labels and coordinates\n \n        # Ensure proper data types\n        gt_classes = gt_classes.long()  # Convert target class labels to LongTensor\n \n        # Compute pairwise IoU between all predicted and GT boxes\n        iou_matrix = torch.zeros(len(preds), len(targets), dtype=torch.float64)\n \n        for i, pred_box in enumerate(pred_boxes):\n            for j, gt_box in enumerate(gt_boxes):\n                iou_matrix[i, j] = calculate_iou_rotated_from_coords(pred_box.tolist(), gt_box.tolist())\n \n        # Match predictions to GT using max IoU (Greedy Matching)\n        for pred_idx, gt_idx in enumerate(torch.argmax(iou_matrix, dim=1)):\n            max_iou = iou_matrix[pred_idx, gt_idx]\n \n            if max_iou &gt;= self.iou_threshold:\n                # **Fix: Ensure pred_classes is a proper tensor of shape [1, num_classes]**\n                class_loss = self.class_loss(pred_logits[pred_idx].unsqueeze(0).float(), gt_classes[gt_idx].unsqueeze(0))\n \n                # Bounding box regression loss\n                reg_loss = self.regression_loss(pred_boxes[pred_idx], gt_boxes[gt_idx])\n \n                # IoU loss (1 - IoU to minimize)\n                iou_loss = 1 - max_iou\n \n                total_class_loss += class_loss\n                total_regression_loss += reg_loss\n                total_iou_loss += iou_loss\n                matched_count += 1\n \n        # Normalize by the number of matched pairs\n        if matched_count &gt; 0:\n            total_class_loss /= matched_count\n            total_regression_loss /= matched_count\n            total_iou_loss /= matched_count\n \n        return total_class_loss + total_regression_loss + total_iou_loss\n \n \nloss_fn = RotatedBoundingBoxLoss(num_classes=6)  # num_classes\n\n\ncbs = [\n    TrainCB(), # Handles the core steps in the training loop. Can be left out if using TrainLearner\n    DeviceCB(), # Handles making sure data and model are on the right device\n#     MetricsCB(accuracy=MulticlassAccuracy()), # Keep track of any relevant metrics\n    ProgressCB(), # Displays metrics and loss during training, optionally plot=True for a pretty graph\n]\n\n# The learner takes a model, dataloaders and loss function, plus some optional extras like a list of callbacks\nlearn = Learner(model, dls, loss_fn, lr=0.1, cbs=cbs)\nlearn\n\n&lt;minai.core.Learner&gt;\n\n\n\n# learn.lr_find()",
    "crumbs": [
      "train"
    ]
  },
  {
    "objectID": "yolov1.html",
    "href": "yolov1.html",
    "title": "Yolo v1",
    "section": "",
    "text": "Yolo v1 provides many fundamental and important concepts for oriented bounding boxes. It is worth learning. Code adapted from https://github.com/aladdinpersson/Machine-Learning-Collection.\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision.transforms.functional as FT\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\ndata_path = fc.Path('../data')\ndata_path\n\nPath('../data')",
    "crumbs": [
      "Yolo v1"
    ]
  },
  {
    "objectID": "yolov1.html#utils",
    "href": "yolov1.html#utils",
    "title": "Yolo v1",
    "section": "Utils",
    "text": "Utils\nSome helpful utilities that can help us.\nThis intersection_over_union function calculates the IoU metric between predicted bounding boxes and ground truth boxes. It’s essential for:\n\nEvaluating object detection accuracy - IoU measures how well your predicted boxes align with the actual object locations\nNon-max suppression - removing duplicate detections by comparing their overlap\nTraining YOLO - the loss function uses IoU to penalize poor box predictions (not in yolo v1)\n\nThe function handles two box formats: - “midpoint”: (x,y,w,h) where x,y is the center point - “corners”: (x1,y1,x2,y2) representing top-left and bottom-right corners\nIt converts boxes to corner format, calculates intersection area (overlap), calculates union area (total area), then returns intersection/union. The 1e-6 term prevents division by zero.\n\nsource\n\nintersection_over_union\n\n intersection_over_union (boxes_preds, boxes_labels,\n                          box_format='midpoint')\n\n*Calculates intersection over union\nParameters: boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4) boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4) box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\nReturns: tensor: Intersection over union for all examples*\nHere is an example:\n\n# Create example boxes\n# Format: [x, y, width, height] in midpoint format\nbox1 = torch.tensor([0.5, 0.5, 0.4, 0.5])  # Center at (0.5, 0.5) with width=0.4, height=0.5\nbox2 = torch.tensor([0.45, 0.45, 0.3, 0.4])  # Center at (0.45, 0.45) with width=0.3, height=0.4\n\n# Add batch dimension\nboxes_pred = box1.unsqueeze(0)  # Shape: [1, 4]\nboxes_label = box2.unsqueeze(0)  # Shape: [1, 4]\n\n# Calculate IoU\niou = intersection_over_union(boxes_pred, boxes_label, box_format=\"midpoint\")\nprint(f\"IoU: {iou.item():.4f}\")\n\n# Visualize the boxes\nfig, ax = plt.subplots(1)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\n\n# Convert to corner format for plotting\ndef midpoint_to_corners(box):\n    x, y, w, h = box\n    x1 = x - w/2\n    y1 = y - h/2\n    return x1, y1, w, h\n\n# Plot box1 (prediction)\nx1, y1, w, h = midpoint_to_corners(box1)\nrect1 = patches.Rectangle((x1, y1), w, h, linewidth=2, edgecolor='r', facecolor='none', label='Prediction')\nax.add_patch(rect1)\n\n# Plot box2 (ground truth)\nx1, y1, w, h = midpoint_to_corners(box2)\nrect2 = patches.Rectangle((x1, y1), w, h, linewidth=2, edgecolor='g', facecolor='none', label='Ground Truth')\nax.add_patch(rect2)\n\nax.legend()\nplt.title(f\"IoU: {iou.item():.4f}\")\nplt.show()\n\nIoU: 0.6000\n\n\n\n\n\n\n\n\n\nThis non_max_suppression function solves a critical problem in object detection: eliminating duplicate detections of the same object.\nWhen a model detects objects, it often generates multiple overlapping bounding boxes for the same object. Non-Max Suppression (NMS) cleans this up by:\n\nFiltering out boxes with confidence below a threshold\nSorting remaining boxes by confidence (highest first)\nKeeping the highest confidence box and removing others that overlap significantly with it (based on IoU)\nRepeating until no boxes remain\n\nThe function takes: - bboxes: List of detection boxes with format [class_pred, prob_score, x1, y1, x2, y2] - iou_threshold: Maximum allowed overlap between distinct objects - threshold: Minimum confidence score for a detection to be considered - box_format: Whether coordinates are in “midpoint” or “corners” format\nThis is essential for clean, usable object detection results - without NMS, a single object might appear to be detected multiple times, cluttering your output and making it difficult to count or track objects accurately.\n\nsource\n\n\nnon_max_suppression\n\n non_max_suppression (bboxes, iou_threshold, threshold,\n                      box_format='corners')\n\n*Does Non Max Suppression given bboxes\nParameters: bboxes (list): list of lists containing all bboxes with each bboxes specified as [class_pred, prob_score, x1, y1, x2, y2] iou_threshold (float): threshold where predicted bboxes is correct threshold (float): threshold to remove predicted bboxes (independent of IoU) box_format (str): “midpoint” or “corners” used to specify bboxes\nReturns: list: bboxes after performing NMS given a specific IoU threshold*\nHere is an example:\n\nbboxes = [\n    [0, 0.9, 0.2, 0.2, 0.6, 0.6],  # High confidence box\n    [0, 0.8, 0.25, 0.25, 0.65, 0.65],  # Overlapping box, same class\n    [0, 0.7, 0.5, 0.5, 0.9, 0.9],  # Different location, same class\n    [1, 0.95, 0.2, 0.2, 0.6, 0.6],  # Same location, different class\n    [0, 0.3, 0.2, 0.2, 0.6, 0.6],  # Low confidence, will be filtered\n]\nthreshold=0.5\nbboxes = [box for box in bboxes if box[1] &gt; threshold]\nbboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\nbboxes\n\n[[1, 0.95, 0.2, 0.2, 0.6, 0.6],\n [0, 0.9, 0.2, 0.2, 0.6, 0.6],\n [0, 0.8, 0.25, 0.25, 0.65, 0.65],\n [0, 0.7, 0.5, 0.5, 0.9, 0.9]]\n\n\n\nbox1 = torch.tensor([0.2, 0.2, 0.6, 0.6])\nbox2 = torch.tensor([0.2, 0.2, 0.6, 0.6])\nintersection_over_union(box1, box2, box_format=\"corners\")\n\ntensor([1.0000])\n\n\n\nbox_format='corners'\niou_threshold=0.5\nchosen_box = bboxes[0]\n\nfor box in bboxes:\n    if box[0] != chosen_box[0] or intersection_over_union(\n        torch.tensor(chosen_box[2:]),\n        torch.tensor(box[2:]),\n        box_format=box_format,\n    ) &lt; iou_threshold:\n        print(f'if true:{box}')\n    else:\n        print(f'else:{box}')\n\nelse:[1, 0.95, 0.2, 0.2, 0.6, 0.6]\nif true:[0, 0.9, 0.2, 0.2, 0.6, 0.6]\nif true:[0, 0.8, 0.25, 0.25, 0.65, 0.65]\nif true:[0, 0.7, 0.5, 0.5, 0.9, 0.9]\n\n\n\n# Create example bounding boxes\n# Format: [class_pred, prob_score, x1, y1, x2, y2] in corners format\nbboxes = [\n    [0, 0.9, 0.2, 0.2, 0.6, 0.6],  # High confidence box\n    [0, 0.8, 0.25, 0.25, 0.65, 0.65],  # Overlapping box, same class\n    [0, 0.7, 0.5, 0.5, 0.9, 0.9],  # Different location, same class\n    [1, 0.95, 0.2, 0.2, 0.6, 0.6],  # Same location, different class\n    [0, 0.3, 0.2, 0.2, 0.6, 0.6],  # Low confidence, will be filtered\n]\n\n# Apply NMS\niou_threshold = 0.5\nconfidence_threshold = 0.5\nnms_boxes = non_max_suppression(bboxes, iou_threshold, confidence_threshold, box_format=\"corners\")\n\n# Print results\nprint(\"Before NMS:\", len(bboxes), \"boxes\")\nprint(\"After NMS:\", len(nms_boxes), \"boxes\")\nfor i, box in enumerate(nms_boxes):\n    print(f\"Box {i+1}: class={int(box[0])}, confidence={box[1]:.2f}, coords={box[2:]}\")\n\n# Visualize the boxes\nfig, ax = plt.subplots(1, figsize=(8, 8))\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\n\n# Plot original boxes\ncolors = ['r', 'g', 'b', 'c', 'm']\nfor i, box in enumerate(bboxes):\n    if box[1] &gt; confidence_threshold:\n        rect = patches.Rectangle(\n            (box[2], box[3]), \n            box[4]-box[2], \n            box[5]-box[3], \n            linewidth=1, \n            edgecolor=colors[i % len(colors)], \n            facecolor='none', \n            linestyle='--',\n            label=f\"Original {i+1}: class={int(box[0])}, conf={box[1]:.2f}\"\n        )\n        ax.add_patch(rect)\n\n# Plot NMS boxes with solid lines\nfor i, box in enumerate(nms_boxes):\n    rect = patches.Rectangle(\n        (box[2], box[3]), \n        box[4]-box[2], \n        box[5]-box[3], \n        linewidth=2, \n        edgecolor=colors[i % len(colors)], \n        facecolor='none',\n        label=f\"NMS {i+1}: class={int(box[0])}, conf={box[1]:.2f}\"\n    )\n    ax.add_patch(rect)\n\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.title(f\"Non-Max Suppression (IoU threshold={iou_threshold}, Confidence threshold={confidence_threshold})\")\nplt.tight_layout()\nplt.show()\n\nBefore NMS: 5 boxes\nAfter NMS: 3 boxes\nBox 1: class=1, confidence=0.95, coords=[0.2, 0.2, 0.6, 0.6]\nBox 2: class=0, confidence=0.90, coords=[0.2, 0.2, 0.6, 0.6]\nBox 3: class=0, confidence=0.70, coords=[0.5, 0.5, 0.9, 0.9]\n\n\n\n\n\n\n\n\n\nThe mean_average_precision function calculates the mAP (Mean Average Precision) metric, which is the standard evaluation metric for object detection models.\nHere’s what it does: 1. For each class, it separates predictions and ground truths 2. It sorts predictions by confidence (highest first) 3. For each prediction, it: - Finds the best matching ground truth box - Marks it as a True Positive (TP) if IoU &gt; threshold and that ground truth wasn’t matched before - Otherwise marks it as a False Positive (FP) 4. Calculates precision and recall at each detection threshold 5. Computes the area under the precision-recall curve (AP) for each class 6. Returns the mean of these APs across all classes\nWhy we need it: - It provides a single-number evaluation of detection performance across all classes - It accounts for both localization accuracy (via IoU) and classification accuracy - It handles the trade-off between precision and recall - It penalizes models for: - Missing objects (lower recall) - False detections (lower precision) - Poor localization (lower IoU) - Confidence miscalibration (via precision-recall curve)\nThis metric is essential for comparing different object detection models and is used in benchmarks like COCO and PASCAL VOC.\n\nsource\n\n\nmean_average_precision\n\n mean_average_precision (pred_boxes, true_boxes, iou_threshold=0.5,\n                         box_format='midpoint', num_classes=20)\n\n*Calculates mean average precision\nParameters: pred_boxes (list): list of lists containing all bboxes with each bboxes specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2] true_boxes (list): Similar as pred_boxes except all the correct ones iou_threshold (float): threshold where predicted bboxes is correct box_format (str): “midpoint” or “corners” used to specify bboxes num_classes (int): number of classes\nReturns: float: mAP value across all classes given a specific IoU threshold*\n\n# Create sample predictions and ground truths\n# Format: [image_idx, class_id, confidence, x1, y1, x2, y2]\npred_boxes = [\n    [0, 1, 0.9, 0.2, 0.3, 0.7, 0.8],  # Image 0, class 1, high confidence\n    [0, 0, 0.8, 0.1, 0.1, 0.3, 0.3],  # Image 0, class 0\n    [1, 1, 0.7, 0.5, 0.4, 0.9, 0.7],  # Image 1, class 1\n    [1, 0, 0.85, 0.3, 0.25, 0.6, 0.5], # Image 1, class 0\n    [2, 0, 0.6, 0.2, 0.2, 0.5, 0.5],   # Image 2, class 0\n]\n\ntrue_boxes = [\n    [0, 1, 1.0, 0.25, 0.35, 0.65, 0.75],  # Image 0, class 1\n    [0, 0, 1.0, 0.1, 0.1, 0.3, 0.3],      # Image 0, class 0\n    [1, 1, 1.0, 0.5, 0.45, 0.85, 0.75],   # Image 1, class 1\n    [1, 0, 1.0, 0.3, 0.2, 0.6, 0.5],      # Image 1, class 0\n    [2, 0, 1.0, 0.2, 0.2, 0.55, 0.55],    # Image 2, class 0\n]\n\n# Calculate mAP\nmAP = mean_average_precision(\n    pred_boxes=pred_boxes,\n    true_boxes=true_boxes,\n    iou_threshold=0.5,\n    box_format=\"corners\",\n    num_classes=2\n)\n\nprint(f\"Mean Average Precision: {mAP:.4f}\")\n\nMean Average Precision: 1.0000\n\n\n\nsource\n\n\nplot_image\n\n plot_image (image, boxes)\n\nPlots predicted bounding boxes on the image\n\nsource\n\n\nget_bboxes\n\n get_bboxes (loader, model, iou_threshold, threshold, pred_format='cells',\n             box_format='midpoint', device='cpu')\n\n\nsource\n\n\nload_checkpoint\n\n load_checkpoint (checkpoint, model, optimizer)\n\n\nsource\n\n\nsave_checkpoint\n\n save_checkpoint (state, filename='my_checkpoint.pth.tar')\n\n\nsource\n\n\ncellboxes_to_boxes\n\n cellboxes_to_boxes (out, S=7)\n\n\nsource\n\n\nconvert_cellboxes\n\n convert_cellboxes (predictions, S=7)\n\nConverts bounding boxes output from Yolo with an image split size of S into entire image ratios rather than relative to cell ratios. Tried to do this vectorized, but this resulted in quite difficult to read code… Use as a black box? Or implement a more intuitive, using 2 for loops iterating range(S) and convert them one by one, resulting in a slower but more readable implementation.",
    "crumbs": [
      "Yolo v1"
    ]
  },
  {
    "objectID": "yolov1.html#data-loader",
    "href": "yolov1.html#data-loader",
    "title": "Yolo v1",
    "section": "Data Loader",
    "text": "Data Loader\n\ntrain_csv = pd.read_csv(fc.Path('../train.csv'))\ntrain_csv.head()\n\n\n\n\n\n\n\n\n000012.jpg\n000012.txt\n\n\n\n\n0\n000017.jpg\n000017.txt\n\n\n1\n000023.jpg\n000023.txt\n\n\n2\n000026.jpg\n000026.txt\n\n\n3\n000032.jpg\n000032.txt\n\n\n4\n000033.jpg\n000033.txt\n\n\n\n\n\n\n\n\nIMG_DIR = \"../data/images\"\nLABEL_DIR = \"../data/labels\"\n\nImage.open(fc.Path(IMG_DIR)/'000017.jpg')\n\n\n\n\n\n\n\n\n\nsource\n\nVOCDataset\n\n VOCDataset (csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\npd.read_csv(\"../8examples.csv\")\n\n\n\n\n\n\n\n\nimg\nlabel\n\n\n\n\n0\n000007.jpg\n000007.txt\n\n\n1\n000009.jpg\n000009.txt\n\n\n2\n000016.jpg\n000016.txt\n\n\n3\n000019.jpg\n000019.txt\n\n\n4\n000020.jpg\n000020.txt\n\n\n5\n000021.jpg\n000021.txt\n\n\n6\n000122.jpg\n000122.txt\n\n\n7\n000129.jpg\n000129.txt\n\n\n\n\n\n\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n\ntrain_dataset = VOCDataset(\n    \"../8examples.csv\",\n    transform=transform,\n    img_dir=IMG_DIR,\n    label_dir=LABEL_DIR,\n)\nx0, y0 = train_dataset[0]\nx0.shape, y0.shape\n\n(torch.Size([3, 448, 448]), torch.Size([7, 7, 30]))\n\n\n\n@fc.delegates(plt.Axes.imshow)\ndef plot_boxes(im, boxes, figsize=(8,8), ax=None, legend=None, **kwargs):\n    \"Plot boxes on image with automatic format detection\"\n    if boxes.shape[-1] == 9 or (len(boxes.shape) &gt; 1 and boxes.shape[1] == 9): \n        return imshow_with_boxes(im, boxes, figsize=figsize, ax=ax, legend=legend, **kwargs)\n    return plot_image(im, boxes)\n\n\nplot_boxes(x0, y0)",
    "crumbs": [
      "Yolo v1"
    ]
  },
  {
    "objectID": "yolov1.html#architecture",
    "href": "yolov1.html#architecture",
    "title": "Yolo v1",
    "section": "Architecture",
    "text": "Architecture\n\nsource\n\nCNNBlock\n\n CNNBlock (in_channels, out_channels, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nCNNBlock(1, 32, kernel_size=1, stride=1, padding=0)\n\nCNNBlock(\n  (conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (leakyrelu): LeakyReLU(negative_slope=0.1)\n)\n\n\n\nsource\n\n\nYolov1\n\n Yolov1 (in_channels=3, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = Yolov1(split_size=7, num_boxes=2, num_classes=20)\nmodel\n\nYolov1(\n  (darknet): Sequential(\n    (0): CNNBlock(\n      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (2): CNNBlock(\n      (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (4): CNNBlock(\n      (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (5): CNNBlock(\n      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (6): CNNBlock(\n      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (7): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (9): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (10): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (11): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (12): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (13): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (14): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (15): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (16): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (17): CNNBlock(\n      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (18): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (19): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (20): CNNBlock(\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (21): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (22): CNNBlock(\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (23): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (24): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (25): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (26): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (27): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n  )\n  (fcs): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=50176, out_features=496, bias=True)\n    (2): Dropout(p=0.0, inplace=False)\n    (3): LeakyReLU(negative_slope=0.1)\n    (4): Linear(in_features=496, out_features=1470, bias=True)\n  )\n)",
    "crumbs": [
      "Yolo v1"
    ]
  },
  {
    "objectID": "yolov1.html#loss-function",
    "href": "yolov1.html#loss-function",
    "title": "Yolo v1",
    "section": "Loss function",
    "text": "Loss function\n\nsource\n\nYoloLoss\n\n YoloLoss (S=7, B=2, C=20)\n\nCalculate the loss for yolo (v1) model\n\nS=7; B=2; C=20\nlambda_noobj = 0.5\nlambda_coord = 5\npredictions = torch.randn(1, 7, 7, 30)\ntarget = torch.randn(1, 7, 7, 25)\n\npredictions = predictions.reshape(-1, S, S, C + B * 5)  # (bs, S, S, 30)\npredictions.shape\n\ntorch.Size([1, 7, 7, 30])",
    "crumbs": [
      "Yolo v1"
    ]
  },
  {
    "objectID": "yolov1.html#train",
    "href": "yolov1.html#train",
    "title": "Yolo v1",
    "section": "Train",
    "text": "Train\n\nseed = 123\ntorch.manual_seed(seed)\n\nLEARNING_RATE = 2e-5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16 \nWEIGHT_DECAY = 0\nEPOCHS = 150\nNUM_WORKERS = 4\nPIN_MEMORY = True\nLOAD_MODEL = True\nLOAD_MODEL_FILE = \"overfit.pth.tar\"\nIMG_DIR = \"../data/images\"\nLABEL_DIR = \"../data/labels\"\n\n\nIMG_DIR\n\n'../data/images'\n\n\n\nsource\n\nCompose\n\n Compose (transforms)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n\n\ndef train_fn(train_loader, model, optimizer, loss_fn):\n    loop = tqdm(train_loader, leave=True)\n    mean_loss = []\n\n    for batch_idx, (x, y) in enumerate(loop):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        out = model(x)\n        loss = loss_fn(out, y)\n        mean_loss.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # update progress bar\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n\n\ntrain_dataset = VOCDataset(\n    \"../8examples.csv\",\n    transform=transform,\n    img_dir=IMG_DIR,\n    label_dir=LABEL_DIR,\n)\nx0, y0 = train_dataset[0]\nx0.shape, y0.shape\n\n(torch.Size([3, 448, 448]), torch.Size([7, 7, 30]))\n\n\n\ncellboxes_to_boxes(y0.unsqueeze(0))\n\n[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n  [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n  [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n  [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n  [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n  [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n  [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n  [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n  [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n  [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n  [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n  [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n  [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n  [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n  [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n  [0.0, 0.0, 0.4285714626312256, 0.4285714626312256, 0.0, 0.0],\n  [6.0,\n   1.0,\n   0.6390000581741333,\n   0.5675675868988037,\n   0.718000054359436,\n   0.8408408761024475],\n  [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n  [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n  [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n  [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n  [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n  [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n  [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n  [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n  [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n  [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n  [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n  [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n  [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n  [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n  [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n  [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n  [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n  [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n  [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n  [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]]]\n\n\n\ntorch.tensor(cellboxes_to_boxes(y0.unsqueeze(0))).shape\n\ntorch.Size([1, 49, 6])\n\n\n\nplot_image(train_dataset[0][0], y0)\n\n\n\n\n\n\n\n\n\ndef main():\n    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    loss_fn = YoloLoss()\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n\n    train_dataset = VOCDataset(\n        \"../8examples.csv\",\n        transform=transform,\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n    )\n\n    test_dataset = VOCDataset(\n        \"../test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n    )\n\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY,\n        shuffle=True,\n        drop_last=False,\n    )\n\n    for epoch in range(EPOCHS):\n#         for x, y in train_loader:\n#             x = x.to(DEVICE)\n#             for idx in range(8):\n#                 bboxes = cellboxes_to_boxes(model(x))\n#                 bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n#                 plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n            \n#             import sys\n#             sys.exit()\n\n        pred_boxes, target_boxes = get_bboxes(\n            train_loader, model, iou_threshold=0.02, threshold=0.01\n        )\n\n        mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.02, box_format=\"midpoint\"\n        )\n        print(f\"Train mAP: {mean_avg_prec}\")\n\n        if mean_avg_prec &gt; 0.9:\n            checkpoint = {\n               \"state_dict\": model.state_dict(),\n               \"optimizer\": optimizer.state_dict(),\n            }\n            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n            import time\n            time.sleep(10)\n            return\n            \n\n        train_fn(train_loader, model, optimizer, loss_fn)\n\n\nmain()\n\n=&gt; Loading checkpoint\n\n\n/tmp/ipykernel_9519/1525986096.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n  im = np.array(image)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit\n\n\n\n\n/home/galopy/miniforge3/envs/fromscratch/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3554: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n\n\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    pin_memory=PIN_MEMORY,\n    shuffle=True,\n    drop_last=False,\n)\nxb, yb = next(iter(train_loader))\nxb.shape\n\ntorch.Size([8, 3, 448, 448])\n\n\n\nmodel = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nloss_fn = YoloLoss()\n\nInside get_bboxes.\n\nall_pred_boxes = []\nall_true_boxes = []\ntrain_idx = 0\n\n\nxb, yb = next(iter(train_loader))\n\nwith torch.no_grad(): \n    preds = model(xb)\n\nbs = xb.shape[0]\nbs\n\n8\n\n\n\nyb.shape\n\ntorch.Size([8, 7, 7, 30])\n\n\n\nyb[0][0]\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0.]])\n\n\n\ntrue_bboxes = cellboxes_to_boxes(yb)\ntorch.tensor(true_bboxes).shape, true_bboxes\n\n(torch.Size([8, 49, 6]),\n [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n   [7.0,\n    1.0,\n    0.48842108249664307,\n    0.512499988079071,\n    0.8294737339019775,\n    0.9458333849906921],\n   [0.0, 0.0, 0.5714285969734192, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.4285714626312256, 0.0, 0.0],\n   [6.0,\n    1.0,\n    0.6390000581741333,\n    0.5675675868988037,\n    0.718000054359436,\n    0.8408408761024475],\n   [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [7.0,\n    1.0,\n    0.27500003576278687,\n    0.4933333694934845,\n    0.5099999904632568,\n    0.3893333673477173],\n   [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.4285714626312256, 0.0, 0.0],\n   [7.0,\n    1.0,\n    0.7120000720024109,\n    0.4560000002384186,\n    0.5040000677108765,\n    0.4480000436306],\n   [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n   [14.0,\n    1.0,\n    0.37700000405311584,\n    0.5640000104904175,\n    0.15800002217292786,\n    0.3813333213329315],\n   [0.0, 0.0, 0.4285714626312256, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n   [12.0,\n    1.0,\n    0.3370000422000885,\n    0.6666667461395264,\n    0.4020000398159027,\n    0.421333372592926],\n   [14.0,\n    1.0,\n    0.5529999732971191,\n    0.7000000476837158,\n    0.07800000160932541,\n    0.34933337569236755],\n   [14.0,\n    1.0,\n    0.6100000143051147,\n    0.7066667079925537,\n    0.08400000631809235,\n    0.34666669368743896],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n   [14.0,\n    1.0,\n    0.3184524178504944,\n    0.445000022649765,\n    0.369047611951828,\n    0.5659999847412109],\n   [0.0, 0.0, 0.4285714626312256, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.4285714626312256, 0.0, 0.0],\n   [14.0,\n    1.0,\n    0.80952388048172,\n    0.515999972820282,\n    0.3750000298023224,\n    0.8920000195503235],\n   [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [11.0,\n    1.0,\n    0.269345223903656,\n    0.6210000514984131,\n    0.538690447807312,\n    0.3059999942779541],\n   [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n   [6.0,\n    1.0,\n    0.5360000133514404,\n    0.5619999766349792,\n    0.9013333916664124,\n    0.5360000133514404],\n   [0.0, 0.0, 0.5714285969734192, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.4285714626312256, 0.0, 0.0],\n   [1.0,\n    1.0,\n    0.5913174152374268,\n    0.5429999828338623,\n    0.6377246379852295,\n    0.8019999861717224],\n   [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]],\n  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n   [14.0,\n    1.0,\n    0.514970064163208,\n    0.4610000252723694,\n    0.5928143858909607,\n    0.9220000505447388],\n   [0.0, 0.0, 0.5714285969734192, 0.4285714626312256, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n   [14.0,\n    1.0,\n    0.8742515444755554,\n    0.5040000677108765,\n    0.24550898373126984,\n    0.9360000491142273],\n   [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n   [1.0,\n    1.0,\n    0.48353296518325806,\n    0.7000000476837158,\n    0.5538922548294067,\n    0.5960000157356262],\n   [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n   [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n   [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]]])\n\n\n\nbboxes = cellboxes_to_boxes(preds)\ntorch.tensor(bboxes).shape, bboxes[0]\n\n(torch.Size([8, 49, 6]),\n [[3.0,\n   0.05791360139846802,\n   0.03071294166147709,\n   -0.002517323475331068,\n   -0.031728584319353104,\n   0.006539730355143547],\n  [19.0,\n   0.10828172415494919,\n   0.14756271243095398,\n   -0.016582747921347618,\n   -0.008299557492136955,\n   0.01615554839372635],\n  [13.0,\n   0.11980977654457092,\n   0.274875670671463,\n   -0.018651630729436874,\n   0.002111813984811306,\n   -0.0005199417355470359],\n  [8.0,\n   -0.017549633979797363,\n   0.4419444799423218,\n   0.017682278528809547,\n   -0.02853921242058277,\n   -0.005208893679082394],\n  [14.0,\n   -0.09938158094882965,\n   0.5583432912826538,\n   0.012327528558671474,\n   0.0098693473264575,\n   -0.005102234892547131],\n  [0.0,\n   0.043295688927173615,\n   0.7186535596847534,\n   -0.02858358807861805,\n   0.01633009873330593,\n   0.01354031078517437],\n  [7.0,\n   0.10642781108617783,\n   0.8594925999641418,\n   0.023684890940785408,\n   0.000150596461025998,\n   0.01613001897931099],\n  [12.0,\n   -0.038706738501787186,\n   -0.004942918661981821,\n   0.14616219699382782,\n   0.004356366582214832,\n   0.010153576731681824],\n  [14.0,\n   0.21250739693641663,\n   0.16930755972862244,\n   0.1576453000307083,\n   0.012522163800895214,\n   0.007855117321014404],\n  [18.0,\n   0.27751728892326355,\n   0.2996690273284912,\n   0.14025703072547913,\n   0.007530116941779852,\n   0.05020694062113762],\n  [18.0,\n   -0.09983375668525696,\n   0.422551691532135,\n   0.1549568474292755,\n   -0.018472790718078613,\n   -0.013049599714577198],\n  [15.0,\n   0.15368522703647614,\n   0.5565087795257568,\n   0.1547100841999054,\n   -0.02206183224916458,\n   -0.023390311747789383],\n  [17.0,\n   0.03610345348715782,\n   0.7166946530342102,\n   0.1297713816165924,\n   -0.022036131471395493,\n   -0.021519459784030914],\n  [6.0,\n   -0.05443115159869194,\n   0.8743669986724854,\n   0.1744801551103592,\n   -0.03472127765417099,\n   -0.0062321024015545845],\n  [17.0,\n   0.10258059948682785,\n   -0.010206078179180622,\n   0.313591867685318,\n   -4.80264461657498e-05,\n   -0.003143869573250413],\n  [0.0,\n   -0.08548567444086075,\n   0.12012486159801483,\n   0.2796362042427063,\n   0.004561031237244606,\n   0.002124300692230463],\n  [10.0,\n   0.03864534944295883,\n   0.27717122435569763,\n   0.27682626247406006,\n   0.008075060322880745,\n   -0.023886417970061302],\n  [5.0,\n   0.2954280972480774,\n   0.45058658719062805,\n   0.27115803956985474,\n   -0.010309206321835518,\n   -0.0008604415343143046],\n  [14.0,\n   0.14327624440193176,\n   0.5665556788444519,\n   0.29179930686950684,\n   -0.007074080873280764,\n   0.0006725640851072967],\n  [4.0,\n   0.15265095233917236,\n   0.7188976407051086,\n   0.2754635214805603,\n   0.052837397903203964,\n   0.009571452625095844],\n  [19.0,\n   0.033910904079675674,\n   0.8498724102973938,\n   0.2897896468639374,\n   -0.00971283484250307,\n   0.0009967128280550241],\n  [0.0,\n   0.0817124992609024,\n   -0.03051653690636158,\n   0.4207684397697449,\n   0.03483124449849129,\n   -0.03289015218615532],\n  [1.0,\n   0.1646699458360672,\n   0.144572451710701,\n   0.4251202344894409,\n   -0.025422310456633568,\n   0.012480674311518669],\n  [5.0,\n   0.022091789171099663,\n   0.30300432443618774,\n   0.4436083436012268,\n   0.00196900125592947,\n   0.008539533242583275],\n  [9.0,\n   0.219450443983078,\n   0.41262227296829224,\n   0.40972524881362915,\n   0.00021286522678565234,\n   0.0021240711212158203],\n  [5.0,\n   0.17189453542232513,\n   0.5787466168403625,\n   0.4193178713321686,\n   -0.0028356367256492376,\n   0.014560356736183167],\n  [14.0,\n   0.07297457009553909,\n   0.7370981574058533,\n   0.43873345851898193,\n   0.037759847939014435,\n   -0.01427136454731226],\n  [0.0,\n   0.05240419879555702,\n   0.8634119033813477,\n   0.41575559973716736,\n   0.006404500920325518,\n   -0.04177410528063774],\n  [2.0,\n   -0.004385566338896751,\n   -0.008994175121188164,\n   0.5881887674331665,\n   0.0008479586103931069,\n   0.0074990433640778065],\n  [10.0,\n   0.06491167843341827,\n   0.13655878603458405,\n   0.5514402389526367,\n   0.010271313600242138,\n   -0.010734865441918373],\n  [9.0,\n   0.14303606748580933,\n   0.2990753650665283,\n   0.5501272082328796,\n   -0.020976949483156204,\n   0.0012839926639571786],\n  [5.0,\n   0.07218076288700104,\n   0.4433569014072418,\n   0.6036580204963684,\n   0.010148609057068825,\n   -0.027454784139990807],\n  [13.0,\n   -0.07835759222507477,\n   0.564740777015686,\n   0.5779457092285156,\n   -0.017660876736044884,\n   0.015681717544794083],\n  [19.0,\n   -0.09923414885997772,\n   0.7411959767341614,\n   0.5936504602432251,\n   -0.0007167485309764743,\n   0.026969926431775093],\n  [11.0,\n   -0.048919014632701874,\n   0.8472048044204712,\n   0.5756462216377258,\n   -0.01894926279783249,\n   -0.006600611377507448],\n  [19.0,\n   -0.013136878609657288,\n   0.017893746495246887,\n   0.7153540849685669,\n   0.0030052566435188055,\n   0.01996099390089512],\n  [10.0,\n   0.04731687158346176,\n   0.12658247351646423,\n   0.7114074230194092,\n   0.03029739484190941,\n   0.03333675116300583],\n  [3.0,\n   0.08107434213161469,\n   0.2924124598503113,\n   0.7183541655540466,\n   0.0018420539563521743,\n   0.009252648800611496],\n  [14.0,\n   0.03232689946889877,\n   0.4424069821834564,\n   0.7288557291030884,\n   0.012216808274388313,\n   0.013487100601196289],\n  [15.0,\n   -0.03617287054657936,\n   0.558650016784668,\n   0.6989643573760986,\n   0.021594980731606483,\n   -0.007387370802462101],\n  [1.0,\n   0.08664897829294205,\n   0.6988940834999084,\n   0.7042855024337769,\n   0.0118038235232234,\n   0.02734706737101078],\n  [8.0,\n   -0.012789484113454819,\n   0.857683002948761,\n   0.7068489789962769,\n   0.010791255161166191,\n   0.01531668845564127],\n  [11.0,\n   -0.013756664469838142,\n   -0.00892128050327301,\n   0.8778007626533508,\n   -0.00830551516264677,\n   -0.03716181591153145],\n  [18.0,\n   0.079897940158844,\n   0.1574002504348755,\n   0.8666889071464539,\n   0.0021867933683097363,\n   0.012636973522603512],\n  [10.0,\n   -0.0983617976307869,\n   0.3015250861644745,\n   0.852695107460022,\n   0.012542926706373692,\n   -0.0005891781183890998],\n  [17.0,\n   -0.036439429968595505,\n   0.4322212040424347,\n   0.8630614280700684,\n   0.0073834192007780075,\n   -0.009754224680364132],\n  [14.0,\n   -0.013987917453050613,\n   0.5865940451622009,\n   0.8550401329994202,\n   0.014219784177839756,\n   -0.01609755866229534],\n  [13.0,\n   -0.011934658512473106,\n   0.6608523726463318,\n   0.8428880572319031,\n   -0.013302735053002834,\n   -0.009596243500709534],\n  [17.0,\n   0.0019228123128414154,\n   0.8482473492622375,\n   0.840347409248352,\n   0.006790795363485813,\n   0.0356440469622612]])\n\n\n\nlen(bboxes[0]) # 1 per each cell\n\n49\n\n\n\nbox_format = 'midpoint'\nthreshold = 0.1\niou_threshold = 0.1\n\n\nidx = 0\n\nnms_boxes = non_max_suppression(\n    bboxes[idx],\n    iou_threshold=iou_threshold,\n    threshold=threshold,\n    box_format=box_format,\n)\nnms_boxes\n\n[[5.0,\n  0.2954280972480774,\n  0.45058658719062805,\n  0.27115803956985474,\n  -0.010309206321835518,\n  -0.0008604415343143046],\n [18.0,\n  0.27751728892326355,\n  0.2996690273284912,\n  0.14025703072547913,\n  0.007530116941779852,\n  0.05020694062113762],\n [9.0,\n  0.219450443983078,\n  0.41262227296829224,\n  0.40972524881362915,\n  0.00021286522678565234,\n  0.0021240711212158203],\n [14.0,\n  0.21250739693641663,\n  0.16930755972862244,\n  0.1576453000307083,\n  0.012522163800895214,\n  0.007855117321014404],\n [5.0,\n  0.17189453542232513,\n  0.5787466168403625,\n  0.4193178713321686,\n  -0.0028356367256492376,\n  0.014560356736183167],\n [1.0,\n  0.1646699458360672,\n  0.144572451710701,\n  0.4251202344894409,\n  -0.025422310456633568,\n  0.012480674311518669],\n [15.0,\n  0.15368522703647614,\n  0.5565087795257568,\n  0.1547100841999054,\n  -0.02206183224916458,\n  -0.023390311747789383],\n [4.0,\n  0.15265095233917236,\n  0.7188976407051086,\n  0.2754635214805603,\n  0.052837397903203964,\n  0.009571452625095844],\n [14.0,\n  0.14327624440193176,\n  0.5665556788444519,\n  0.29179930686950684,\n  -0.007074080873280764,\n  0.0006725640851072967],\n [9.0,\n  0.14303606748580933,\n  0.2990753650665283,\n  0.5501272082328796,\n  -0.020976949483156204,\n  0.0012839926639571786],\n [13.0,\n  0.11980977654457092,\n  0.274875670671463,\n  -0.018651630729436874,\n  0.002111813984811306,\n  -0.0005199417355470359],\n [19.0,\n  0.10828172415494919,\n  0.14756271243095398,\n  -0.016582747921347618,\n  -0.008299557492136955,\n  0.01615554839372635],\n [7.0,\n  0.10642781108617783,\n  0.8594925999641418,\n  0.023684890940785408,\n  0.000150596461025998,\n  0.01613001897931099],\n [17.0,\n  0.10258059948682785,\n  -0.010206078179180622,\n  0.313591867685318,\n  -4.80264461657498e-05,\n  -0.003143869573250413]]\n\n\n\nplot_image(xb[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\nprint(nms_boxes)\n\n\n\n\n\n\n\n\n[[5.0, 0.2954280972480774, 0.45058658719062805, 0.27115803956985474, -0.010309206321835518, -0.0008604415343143046], [18.0, 0.27751728892326355, 0.2996690273284912, 0.14025703072547913, 0.007530116941779852, 0.05020694062113762], [9.0, 0.219450443983078, 0.41262227296829224, 0.40972524881362915, 0.00021286522678565234, 0.0021240711212158203], [14.0, 0.21250739693641663, 0.16930755972862244, 0.1576453000307083, 0.012522163800895214, 0.007855117321014404], [5.0, 0.17189453542232513, 0.5787466168403625, 0.4193178713321686, -0.0028356367256492376, 0.014560356736183167], [1.0, 0.1646699458360672, 0.144572451710701, 0.4251202344894409, -0.025422310456633568, 0.012480674311518669], [15.0, 0.15368522703647614, 0.5565087795257568, 0.1547100841999054, -0.02206183224916458, -0.023390311747789383], [4.0, 0.15265095233917236, 0.7188976407051086, 0.2754635214805603, 0.052837397903203964, 0.009571452625095844], [14.0, 0.14327624440193176, 0.5665556788444519, 0.29179930686950684, -0.007074080873280764, 0.0006725640851072967], [9.0, 0.14303606748580933, 0.2990753650665283, 0.5501272082328796, -0.020976949483156204, 0.0012839926639571786], [13.0, 0.11980977654457092, 0.274875670671463, -0.018651630729436874, 0.002111813984811306, -0.0005199417355470359], [19.0, 0.10828172415494919, 0.14756271243095398, -0.016582747921347618, -0.008299557492136955, 0.01615554839372635], [7.0, 0.10642781108617783, 0.8594925999641418, 0.023684890940785408, 0.000150596461025998, 0.01613001897931099], [17.0, 0.10258059948682785, -0.010206078179180622, 0.313591867685318, -4.80264461657498e-05, -0.003143869573250413]]\n\n\n\nfor nms_box in nms_boxes:\n    all_pred_boxes.append([train_idx] + nms_box)\n\n\nall_pred_boxes\n\n[[0,\n  5.0,\n  0.2954280972480774,\n  0.45058658719062805,\n  0.27115803956985474,\n  -0.010309206321835518,\n  -0.0008604415343143046],\n [0,\n  18.0,\n  0.27751728892326355,\n  0.2996690273284912,\n  0.14025703072547913,\n  0.007530116941779852,\n  0.05020694062113762],\n [0,\n  9.0,\n  0.219450443983078,\n  0.41262227296829224,\n  0.40972524881362915,\n  0.00021286522678565234,\n  0.0021240711212158203],\n [0,\n  14.0,\n  0.21250739693641663,\n  0.16930755972862244,\n  0.1576453000307083,\n  0.012522163800895214,\n  0.007855117321014404],\n [0,\n  5.0,\n  0.17189453542232513,\n  0.5787466168403625,\n  0.4193178713321686,\n  -0.0028356367256492376,\n  0.014560356736183167],\n [0,\n  1.0,\n  0.1646699458360672,\n  0.144572451710701,\n  0.4251202344894409,\n  -0.025422310456633568,\n  0.012480674311518669],\n [0,\n  15.0,\n  0.15368522703647614,\n  0.5565087795257568,\n  0.1547100841999054,\n  -0.02206183224916458,\n  -0.023390311747789383],\n [0,\n  4.0,\n  0.15265095233917236,\n  0.7188976407051086,\n  0.2754635214805603,\n  0.052837397903203964,\n  0.009571452625095844],\n [0,\n  14.0,\n  0.14327624440193176,\n  0.5665556788444519,\n  0.29179930686950684,\n  -0.007074080873280764,\n  0.0006725640851072967],\n [0,\n  9.0,\n  0.14303606748580933,\n  0.2990753650665283,\n  0.5501272082328796,\n  -0.020976949483156204,\n  0.0012839926639571786],\n [0,\n  13.0,\n  0.11980977654457092,\n  0.274875670671463,\n  -0.018651630729436874,\n  0.002111813984811306,\n  -0.0005199417355470359],\n [0,\n  19.0,\n  0.10828172415494919,\n  0.14756271243095398,\n  -0.016582747921347618,\n  -0.008299557492136955,\n  0.01615554839372635],\n [0,\n  7.0,\n  0.10642781108617783,\n  0.8594925999641418,\n  0.023684890940785408,\n  0.000150596461025998,\n  0.01613001897931099],\n [0,\n  17.0,\n  0.10258059948682785,\n  -0.010206078179180622,\n  0.313591867685318,\n  -4.80264461657498e-05,\n  -0.003143869573250413]]\n\n\n\ntrue_bboxes[idx]\n\n[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n [7.0,\n  1.0,\n  0.48842108249664307,\n  0.512499988079071,\n  0.8294737339019775,\n  0.9458333849906921],\n [0.0, 0.0, 0.5714285969734192, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]]\n\n\n\nfor box in true_bboxes[idx]:\n    # many will get converted to 0 pred\n    if box[1] &gt; threshold:\n        all_true_boxes.append([train_idx] + box)\n\nAdditional train_idx is added for all_true_boxes.\n\nall_true_boxes\n\n[[0,\n  7.0,\n  1.0,\n  0.48842108249664307,\n  0.512499988079071,\n  0.8294737339019775,\n  0.9458333849906921]]\n\n\n\nlen(true_bboxes[0])\n\n49\n\n\n\ntrue_bboxes[0]\n\n[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.1428571492433548, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.2857142984867096, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.4285714626312256, 0.0, 0.0],\n [7.0,\n  1.0,\n  0.48842108249664307,\n  0.512499988079071,\n  0.8294737339019775,\n  0.9458333849906921],\n [0.0, 0.0, 0.5714285969734192, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.4285714626312256, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.5714285969734192, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.7142857313156128, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.1428571492433548, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.2857142984867096, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.4285714626312256, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.5714285969734192, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.7142857313156128, 0.8571429252624512, 0.0, 0.0],\n [0.0, 0.0, 0.8571429252624512, 0.8571429252624512, 0.0, 0.0]]\n\n\n\nplot_image(xb[0], true_bboxes[0])\n\n\n\n\n\n\n\n\n\nplot_image(xb[0], [all_true_boxes[0][1:]])\n\n\n\n\n\n\n\n\n\npred_boxes, target_boxes = get_bboxes(\n    train_loader, model, iou_threshold=0.5, threshold=0.\n)\npred_boxes\n\n[[0,\n  16.0,\n  0.04880784451961517,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014819064177573,\n  -0.0005609018262475729],\n [0,\n  18.0,\n  0.04596563056111336,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246645352803171,\n  0.0013500609202310443],\n [0,\n  5.0,\n  0.04222836345434189,\n  0.43169596791267395,\n  0.852234423160553,\n  0.0015447199111804366,\n  -2.0928358935634606e-05],\n [0,\n  17.0,\n  0.04152437299489975,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269533753395,\n  0.002315293299034238],\n [0,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.0015880215214565396,\n  -0.0007841718033887446],\n [0,\n  10.0,\n  0.03657899051904678,\n  0.28723227977752686,\n  0.0019326733890920877,\n  0.006220615468919277,\n  0.00047305849147960544],\n [0,\n  11.0,\n  0.03548029810190201,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.00324416090734303,\n  0.0029634027741849422],\n [0,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.00097929616458714,\n  -0.002543338108807802],\n [0,\n  2.0,\n  0.034323837608098984,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.002076603937894106,\n  -0.0033999979496002197],\n [0,\n  1.0,\n  0.03337988629937172,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973265981301665,\n  -0.002992372727021575],\n [0,\n  1.0,\n  0.03324149176478386,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.006441280711442232,\n  -0.005959640722721815],\n [0,\n  0.0,\n  0.03252032771706581,\n  0.720090925693512,\n  0.2873821258544922,\n  0.0030895781237632036,\n  0.0051577650010585785],\n [0,\n  5.0,\n  0.03218614682555199,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.0023512018378823996,\n  0.004679890815168619],\n [0,\n  0.0,\n  0.030840162187814713,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847196161747,\n  -0.007043744437396526],\n [0,\n  17.0,\n  0.02896946668624878,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.005785795394331217,\n  -0.0018508632201701403],\n [0,\n  19.0,\n  0.02833571285009384,\n  0.42402884364128113,\n  0.0008106034365482628,\n  -0.005690244026482105,\n  -0.005403212737292051],\n [0,\n  7.0,\n  0.027755150571465492,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.0028729636687785387,\n  0.0061393859796226025],\n [0,\n  11.0,\n  0.02739958092570305,\n  -0.0033466939348727465,\n  0.43197140097618103,\n  0.0022143691312521696,\n  0.005311156623065472],\n [0,\n  17.0,\n  0.027385598048567772,\n  0.002377033932134509,\n  -0.007107321172952652,\n  -0.004745279438793659,\n  0.005192494951188564],\n [0,\n  10.0,\n  0.02640070579946041,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.001242348924279213,\n  0.0039032401982694864],\n [0,\n  3.0,\n  0.02587714046239853,\n  0.5747456550598145,\n  -0.0007388428202830255,\n  0.002270226599648595,\n  -0.004265402909368277],\n [0,\n  10.0,\n  0.025490777567029,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.00406848918646574,\n  0.0031593304593116045],\n [0,\n  1.0,\n  0.02135799638926983,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.005209512542933226,\n  0.004185027908533812],\n [0,\n  4.0,\n  0.02040451392531395,\n  0.14101380109786987,\n  -0.005262696649879217,\n  0.0020898065995424986,\n  -0.0055968184024095535],\n [0,\n  15.0,\n  0.018854616209864616,\n  -0.0022594714537262917,\n  0.1384880095720291,\n  0.002785480348393321,\n  0.005815902724862099],\n [0,\n  12.0,\n  0.018164731562137604,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829452658072114,\n  0.002220866968855262],\n [0,\n  10.0,\n  0.017963578924536705,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879379637539387,\n  0.0050342148169875145],\n [0,\n  19.0,\n  0.01641383208334446,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.0001622691488591954,\n  -0.005205052439123392],\n [0,\n  3.0,\n  0.013968080282211304,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.0054604667238891125,\n  0.003514510579407215],\n [0,\n  19.0,\n  0.010167022235691547,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.001429215306416154,\n  -0.004541652277112007],\n [0,\n  11.0,\n  0.009437758475542068,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314248450100422,\n  -0.005225767381489277],\n [0,\n  5.0,\n  0.00882111955434084,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.0027583171613514423,\n  0.0005774053279310465],\n [0,\n  4.0,\n  0.007675145287066698,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203427946195006,\n  -0.0008360185893252492],\n [0,\n  17.0,\n  0.007499024271965027,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.0012506203493103385,\n  -0.0004903242806904018],\n [0,\n  15.0,\n  0.0074683791026473045,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.0004536565684247762,\n  -0.002246222924441099],\n [0,\n  17.0,\n  0.0066661536693573,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.0025538974441587925,\n  0.002486427780240774],\n [0,\n  2.0,\n  0.004264902323484421,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.0026889850851148367,\n  0.006471899338066578],\n [0,\n  5.0,\n  0.004250847734510899,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021460285410285],\n [0,\n  17.0,\n  0.0005488770548254251,\n  0.0021971387322992086,\n  0.5736400485038757,\n  -0.002658810233697295,\n  -0.0026629820931702852],\n [1,\n  16.0,\n  0.04880784451961517,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014785594772547,\n  -0.0005609014187939465],\n [1,\n  18.0,\n  0.04596563056111336,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246644770726562,\n  0.0013500612694770098],\n [1,\n  5.0,\n  0.04222835972905159,\n  0.43169596791267395,\n  0.852234423160553,\n  0.001544719678349793,\n  -2.092861723212991e-05],\n [1,\n  17.0,\n  0.04152438044548035,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269533753395,\n  0.002315293299034238],\n [1,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.0015880215214565396,\n  -0.0007841720362193882],\n [1,\n  10.0,\n  0.03657899051904678,\n  0.28723227977752686,\n  0.0019326736219227314,\n  0.00622061500325799,\n  0.00047305799671448767],\n [1,\n  11.0,\n  0.035480301827192307,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.0032441611401736736,\n  0.0029634027741849422],\n [1,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.0009792958153411746,\n  -0.002543337643146515],\n [1,\n  2.0,\n  0.03432384133338928,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.002076603937894106,\n  -0.0033999979496002197],\n [1,\n  1.0,\n  0.03337988257408142,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973264817148447,\n  -0.0029923724941909313],\n [1,\n  1.0,\n  0.03324149176478386,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.006441280711442232,\n  -0.005959640722721815],\n [1,\n  0.0,\n  0.03252032771706581,\n  0.720090925693512,\n  0.2873821258544922,\n  0.0030895783565938473,\n  0.0051577650010585785],\n [1,\n  5.0,\n  0.03218614682555199,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.0023512018378823996,\n  0.004679890815168619],\n [1,\n  0.0,\n  0.030840162187814713,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847196161747,\n  -0.007043743971735239],\n [1,\n  17.0,\n  0.02896946482360363,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.0057857949286699295,\n  -0.001850863453000784],\n [1,\n  19.0,\n  0.02833571657538414,\n  0.42402884364128113,\n  0.0008106034947559237,\n  -0.005690244026482105,\n  -0.005403212737292051],\n [1,\n  7.0,\n  0.027755150571465492,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.0028729632031172514,\n  0.0061393859796226025],\n [1,\n  11.0,\n  0.02739957720041275,\n  -0.00334669416770339,\n  0.43197140097618103,\n  0.0022143693640828133,\n  0.005311157088726759],\n [1,\n  17.0,\n  0.027385596185922623,\n  0.0023770341649651527,\n  -0.007107321638613939,\n  -0.004745279438793659,\n  0.005192494951188564],\n [1,\n  10.0,\n  0.02640070579946041,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.001242348924279213,\n  0.0039032401982694864],\n [1,\n  3.0,\n  0.02587714046239853,\n  0.5747456550598145,\n  -0.000738843169528991,\n  0.0022702268324792385,\n  -0.004265402909368277],\n [1,\n  10.0,\n  0.025490777567029,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.004068488720804453,\n  0.0031593304593116045],\n [1,\n  1.0,\n  0.02135799266397953,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.005209512542933226,\n  0.004185027908533812],\n [1,\n  4.0,\n  0.02040451392531395,\n  0.14101380109786987,\n  -0.005262696649879217,\n  0.002089806366711855,\n  -0.0055968184024095535],\n [1,\n  15.0,\n  0.018854618072509766,\n  -0.0022594714537262917,\n  0.1384880095720291,\n  0.002785480348393321,\n  0.005815902724862099],\n [1,\n  12.0,\n  0.018164731562137604,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829451493918896,\n  0.002220866270363331],\n [1,\n  10.0,\n  0.017963578924536705,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879374980926514,\n  0.0050342148169875145],\n [1,\n  19.0,\n  0.01641383022069931,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.00016226922161877155,\n  -0.005205052439123392],\n [1,\n  3.0,\n  0.013968082144856453,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.0054604667238891125,\n  0.003514510579407215],\n [1,\n  19.0,\n  0.010167023167014122,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.001429215190000832,\n  -0.00454165181145072],\n [1,\n  11.0,\n  0.00943775661289692,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314248450100422,\n  -0.005225767381489277],\n [1,\n  5.0,\n  0.008821118623018265,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.002758317394182086,\n  0.000577404978685081],\n [1,\n  4.0,\n  0.007675145752727985,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203430274501443,\n  -0.0008360186475329101],\n [1,\n  17.0,\n  0.007499022409319878,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.0012506204657256603,\n  -0.0004903244553133845],\n [1,\n  15.0,\n  0.007468376308679581,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.00045365694677457213,\n  -0.0022462226916104555],\n [1,\n  17.0,\n  0.0066661536693573,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.002553897676989436,\n  0.0024864287115633488],\n [1,\n  2.0,\n  0.004264902789145708,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.002688985550776124,\n  0.006471899803727865],\n [1,\n  5.0,\n  0.004250846803188324,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021459121257067],\n [1,\n  17.0,\n  0.0005488756578415632,\n  0.0021971389651298523,\n  0.5736400485038757,\n  -0.002658810233697295,\n  -0.0026629820931702852],\n [2,\n  16.0,\n  0.04880784451961517,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014816153794527,\n  -0.0005609015352092683],\n [2,\n  18.0,\n  0.04596563056111336,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246645352803171,\n  0.0013500605709850788],\n [2,\n  5.0,\n  0.04222836345434189,\n  0.43169596791267395,\n  0.852234423160553,\n  0.0015447199111804366,\n  -2.092866816383321e-05],\n [2,\n  17.0,\n  0.04152437299489975,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269068092108,\n  0.0023152935318648815],\n [2,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.0015880210557952523,\n  -0.0007841719198040664],\n [2,\n  10.0,\n  0.03657899424433708,\n  0.28723227977752686,\n  0.0019326737383380532,\n  0.006220615468919277,\n  0.00047305840416811407],\n [2,\n  11.0,\n  0.035480301827192307,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.00324416090734303,\n  0.002963403007015586],\n [2,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.0009792958153411746,\n  -0.002543338807299733],\n [2,\n  2.0,\n  0.034323837608098984,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.0020766041707247496,\n  -0.0033999981824308634],\n [2,\n  1.0,\n  0.03337988257408142,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973268309608102,\n  -0.0029923722613602877],\n [2,\n  1.0,\n  0.03324149176478386,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.0064412811771035194,\n  -0.00595964165404439],\n [2,\n  0.0,\n  0.03252032771706581,\n  0.720090925693512,\n  0.2873821258544922,\n  0.0030895783565938473,\n  0.0051577650010585785],\n [2,\n  5.0,\n  0.03218614682555199,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.002351201605051756,\n  0.0046798912808299065],\n [2,\n  0.0,\n  0.030840158462524414,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847196161747,\n  -0.007043743971735239],\n [2,\n  17.0,\n  0.02896946296095848,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.005785795394331217,\n  -0.001850863336585462],\n [2,\n  19.0,\n  0.02833571471273899,\n  0.42402884364128113,\n  0.0008106032619252801,\n  -0.005690244026482105,\n  -0.005403212737292051],\n [2,\n  7.0,\n  0.027755148708820343,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.0028729632031172514,\n  0.0061393859796226025],\n [2,\n  11.0,\n  0.0273995790630579,\n  -0.0033466939348727465,\n  0.43197140097618103,\n  0.0022143691312521696,\n  0.005311156623065472],\n [2,\n  17.0,\n  0.027385596185922623,\n  0.002377033932134509,\n  -0.007107321172952652,\n  -0.004745279438793659,\n  0.005192495416849852],\n [2,\n  10.0,\n  0.026400703936815262,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.0012423492735251784,\n  0.00390324043110013],\n [2,\n  3.0,\n  0.02587713859975338,\n  0.5747456550598145,\n  -0.0007388427620753646,\n  0.002270226599648595,\n  -0.004265402909368277],\n [2,\n  10.0,\n  0.02549077942967415,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.00406848918646574,\n  0.0031593304593116045],\n [2,\n  1.0,\n  0.02135799452662468,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.0052095134742558,\n  0.004185027442872524],\n [2,\n  4.0,\n  0.02040451392531395,\n  0.14101380109786987,\n  -0.005262696649879217,\n  0.0020898065995424986,\n  -0.005596817936748266],\n [2,\n  15.0,\n  0.018854618072509766,\n  -0.002259471220895648,\n  0.1384880095720291,\n  0.0027854801155626774,\n  0.005815903190523386],\n [2,\n  12.0,\n  0.018164733424782753,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829451493918896,\n  0.002220866736024618],\n [2,\n  10.0,\n  0.017963578924536705,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879372652620077,\n  0.0050342148169875145],\n [2,\n  19.0,\n  0.01641383022069931,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.00016226903244387358,\n  -0.005205052439123392],\n [2,\n  3.0,\n  0.013968079350888729,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.0054604667238891125,\n  0.003514510579407215],\n [2,\n  19.0,\n  0.010167024098336697,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.001429215306416154,\n  -0.004541652277112007],\n [2,\n  11.0,\n  0.00943775661289692,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314247984439135,\n  -0.005225767381489277],\n [2,\n  5.0,\n  0.008821121416985989,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.002758317394182086,\n  0.0005774049204774201],\n [2,\n  4.0,\n  0.007675147615373135,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203427946195006,\n  -0.0008360185311175883],\n [2,\n  17.0,\n  0.007499023340642452,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.0012506205821409822,\n  -0.0004903243388980627],\n [2,\n  15.0,\n  0.00746837817132473,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.00045365680125541985,\n  -0.002246222924441099],\n [2,\n  17.0,\n  0.006666155532002449,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.00255389790982008,\n  0.0024864282459020615],\n [2,\n  2.0,\n  0.004264901392161846,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.002688985550776124,\n  0.006471899803727865],\n [2,\n  5.0,\n  0.004250846803188324,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021460867486894],\n [2,\n  17.0,\n  0.0005488754250109196,\n  0.0021971387322992086,\n  0.5736400485038757,\n  -0.002658810233697295,\n  -0.002662982326000929],\n [3,\n  16.0,\n  0.04880784451961517,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014829250518233,\n  -0.0005609014770016074],\n [3,\n  18.0,\n  0.04596563056111336,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246644770726562,\n  0.0013500606874004006],\n [3,\n  5.0,\n  0.04222835972905159,\n  0.43169596791267395,\n  0.852234423160553,\n  0.001544719678349793,\n  -2.0928830053890124e-05],\n [3,\n  17.0,\n  0.04152437672019005,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269068092108,\n  0.0023152935318648815],\n [3,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.001588021288625896,\n  -0.00078417151235044],\n [3,\n  10.0,\n  0.03657899051904678,\n  0.28723227977752686,\n  0.0019326735055074096,\n  0.00622061500325799,\n  0.00047305820044130087],\n [3,\n  11.0,\n  0.035480301827192307,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.0032441606745123863,\n  0.002963403007015586],\n [3,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.0009792959317564964,\n  -0.002543338341638446],\n [3,\n  2.0,\n  0.034323837608098984,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.0020766044035553932,\n  -0.0033999981824308634],\n [3,\n  1.0,\n  0.03337988257408142,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973265981301665,\n  -0.0029923722613602877],\n [3,\n  1.0,\n  0.03324149176478386,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.0064412811771035194,\n  -0.005959640257060528],\n [3,\n  0.0,\n  0.03252032399177551,\n  0.720090925693512,\n  0.2873821258544922,\n  0.0030895783565938473,\n  0.0051577650010585785],\n [3,\n  5.0,\n  0.03218614682555199,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.0023512018378823996,\n  0.0046798912808299065],\n [3,\n  0.0,\n  0.030840162187814713,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847196161747,\n  -0.007043743971735239],\n [3,\n  17.0,\n  0.02896946296095848,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.005785795394331217,\n  -0.001850863336585462],\n [3,\n  19.0,\n  0.02833571657538414,\n  0.42402884364128113,\n  0.0008106034365482628,\n  -0.005690244026482105,\n  -0.005403212737292051],\n [3,\n  7.0,\n  0.027755148708820343,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.0028729632031172514,\n  0.006139385513961315],\n [3,\n  11.0,\n  0.0273995790630579,\n  -0.0033466939348727465,\n  0.43197140097618103,\n  0.0022143693640828133,\n  0.005311156157404184],\n [3,\n  17.0,\n  0.027385598048567772,\n  0.002377033932134509,\n  -0.007107321172952652,\n  -0.004745279438793659,\n  0.005192495416849852],\n [3,\n  10.0,\n  0.02640070579946041,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.0012423486914485693,\n  0.0039032401982694864],\n [3,\n  3.0,\n  0.02587713673710823,\n  0.5747456550598145,\n  -0.0007388429949060082,\n  0.002270226599648595,\n  -0.004265402909368277],\n [3,\n  10.0,\n  0.025490777567029,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.004068488720804453,\n  0.003159330226480961],\n [3,\n  1.0,\n  0.02135799452662468,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.005209512542933226,\n  0.004185027442872524],\n [3,\n  4.0,\n  0.02040451392531395,\n  0.14101380109786987,\n  -0.00526269618421793,\n  0.002089806366711855,\n  -0.0055968184024095535],\n [3,\n  15.0,\n  0.018854614347219467,\n  -0.002259471220895648,\n  0.1384880095720291,\n  0.0027854805812239647,\n  0.005815903190523386],\n [3,\n  12.0,\n  0.018164735287427902,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829456150531769,\n  0.002220866736024618],\n [3,\n  10.0,\n  0.017963577061891556,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879379637539387,\n  0.005034215748310089],\n [3,\n  19.0,\n  0.01641383022069931,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.00016226954176090658,\n  -0.005205052439123392],\n [3,\n  3.0,\n  0.013968078419566154,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.005460468120872974,\n  0.0035145103465765715],\n [3,\n  19.0,\n  0.010167025029659271,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.0014292156556621194,\n  -0.00454165181145072],\n [3,\n  11.0,\n  0.00943775661289692,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314247984439135,\n  -0.005225767381489277],\n [3,\n  5.0,\n  0.008821120485663414,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.0027583171613514423,\n  0.0005774050951004028],\n [3,\n  4.0,\n  0.007675145287066698,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203426782041788,\n  -0.0008360186475329101],\n [3,\n  17.0,\n  0.007499019615352154,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.0012506204657256603,\n  -0.0004903243971057236],\n [3,\n  15.0,\n  0.00746837817132473,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.0004536568303592503,\n  -0.002246223157271743],\n [3,\n  17.0,\n  0.006666154600679874,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.002553898375481367,\n  0.002486428013071418],\n [3,\n  2.0,\n  0.00426490418612957,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.002688985550776124,\n  0.006471899803727865],\n [3,\n  5.0,\n  0.004250845406204462,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021460867486894],\n [3,\n  17.0,\n  0.0005488754250109196,\n  0.0021971389651298523,\n  0.5736400485038757,\n  -0.0026588100008666515,\n  -0.0026629820931702852],\n [4,\n  16.0,\n  0.048807840794324875,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014808877836913,\n  -0.000560901768039912],\n [4,\n  18.0,\n  0.04596562683582306,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246648263186216,\n  0.0013500606874004006],\n [4,\n  5.0,\n  0.04222836345434189,\n  0.43169596791267395,\n  0.852234423160553,\n  0.001544719678349793,\n  -2.0928697267663665e-05],\n [4,\n  17.0,\n  0.04152437299489975,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269068092108,\n  0.002315293066203594],\n [4,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.0015880210557952523,\n  -0.0007841719780117273],\n [4,\n  10.0,\n  0.03657899051904678,\n  0.28723227977752686,\n  0.0019326736219227314,\n  0.00622061500325799,\n  0.00047305814223363996],\n [4,\n  11.0,\n  0.03548029810190201,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.00324416090734303,\n  0.0029634027741849422],\n [4,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.0009792960481718183,\n  -0.002543338341638446],\n [4,\n  2.0,\n  0.034323837608098984,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.0020766041707247496,\n  -0.0033999981824308634],\n [4,\n  1.0,\n  0.03337988629937172,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973268309608102,\n  -0.0029923724941909313],\n [4,\n  1.0,\n  0.03324149176478386,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.0064412811771035194,\n  -0.005959640722721815],\n [4,\n  0.0,\n  0.03252032771706581,\n  0.720090925693512,\n  0.2873821258544922,\n  0.0030895781237632036,\n  0.0051577650010585785],\n [4,\n  5.0,\n  0.03218614682555199,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.002351201605051756,\n  0.004679890815168619],\n [4,\n  0.0,\n  0.030840162187814713,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847661823034,\n  -0.007043743971735239],\n [4,\n  17.0,\n  0.02896946109831333,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.005785795394331217,\n  -0.001850863453000784],\n [4,\n  19.0,\n  0.02833571471273899,\n  0.42402884364128113,\n  0.0008106034947559237,\n  -0.005690243560820818,\n  -0.005403212737292051],\n [4,\n  7.0,\n  0.027755150571465492,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.002872963435947895,\n  0.0061393859796226025],\n [4,\n  11.0,\n  0.02739957720041275,\n  -0.0033466939348727465,\n  0.43197140097618103,\n  0.0022143691312521696,\n  0.005311157088726759],\n [4,\n  17.0,\n  0.027385596185922623,\n  0.0023770336993038654,\n  -0.007107321172952652,\n  -0.0047452799044549465,\n  0.005192495416849852],\n [4,\n  10.0,\n  0.02640070579946041,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.001242348924279213,\n  0.0039032401982694864],\n [4,\n  3.0,\n  0.02587714232504368,\n  0.5747456550598145,\n  -0.0007388429366983473,\n  0.002270226599648595,\n  -0.004265402909368277],\n [4,\n  10.0,\n  0.025490777567029,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.00406848918646574,\n  0.0031593304593116045],\n [4,\n  1.0,\n  0.02135799452662468,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.0052095134742558,\n  0.004185027442872524],\n [4,\n  4.0,\n  0.02040451392531395,\n  0.14101380109786987,\n  -0.005262696649879217,\n  0.0020898065995424986,\n  -0.005596817936748266],\n [4,\n  15.0,\n  0.018854618072509766,\n  -0.0022594714537262917,\n  0.1384880095720291,\n  0.002785480348393321,\n  0.005815902724862099],\n [4,\n  12.0,\n  0.018164735287427902,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829456150531769,\n  0.0022208665031939745],\n [4,\n  10.0,\n  0.017963577061891556,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879374980926514,\n  0.005034215282648802],\n [4,\n  19.0,\n  0.01641383022069931,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.00016226941079366952,\n  -0.005205052439123392],\n [4,\n  3.0,\n  0.013968080282211304,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.005460467655211687,\n  0.0035145108122378588],\n [4,\n  19.0,\n  0.010167024098336697,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.0014292149571701884,\n  -0.004541652277112007],\n [4,\n  11.0,\n  0.009437755681574345,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314248450100422,\n  -0.005225767847150564],\n [4,\n  5.0,\n  0.008821118623018265,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.002758317394182086,\n  0.0005774050951004028],\n [4,\n  4.0,\n  0.007675147615373135,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203429110348225,\n  -0.0008360188221558928],\n [4,\n  17.0,\n  0.007499021012336016,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.0012506203493103385,\n  -0.000490324106067419],\n [4,\n  15.0,\n  0.007468376774340868,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.0004536567721515894,\n  -0.002246222924441099],\n [4,\n  17.0,\n  0.006666155066341162,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.00255389790982008,\n  0.002486428013071418],\n [4,\n  2.0,\n  0.004264899529516697,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.0026889857836067677,\n  0.006471899803727865],\n [4,\n  5.0,\n  0.004250845871865749,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021461449563503],\n [4,\n  17.0,\n  0.0005488754250109196,\n  0.0021971389651298523,\n  0.5736400485038757,\n  -0.002658810233697295,\n  -0.002662982326000929],\n [5,\n  16.0,\n  0.04880784451961517,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014821974560618,\n  -0.0005609014770016074],\n [5,\n  18.0,\n  0.04596563056111336,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246648263186216,\n  0.0013500608038157225],\n [5,\n  5.0,\n  0.04222836345434189,\n  0.43169596791267395,\n  0.852234423160553,\n  0.0015447194455191493,\n  -2.0928522644680925e-05],\n [5,\n  17.0,\n  0.04152437672019005,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269533753395,\n  0.0023152935318648815],\n [5,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.0015880210557952523,\n  -0.0007841716869734228],\n [5,\n  10.0,\n  0.03657899051904678,\n  0.28723227977752686,\n  0.0019326733890920877,\n  0.00622061500325799,\n  0.00047305834596045315],\n [5,\n  11.0,\n  0.035480301827192307,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.0032441611401736736,\n  0.0029634027741849422],\n [5,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.0009792959317564964,\n  -0.002543338341638446],\n [5,\n  2.0,\n  0.034323837608098984,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.0020766041707247496,\n  -0.0033999981824308634],\n [5,\n  1.0,\n  0.03337988629937172,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973267145454884,\n  -0.0029923722613602877],\n [5,\n  1.0,\n  0.03324148803949356,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.0064412811771035194,\n  -0.005959640257060528],\n [5,\n  0.0,\n  0.03252032771706581,\n  0.720090925693512,\n  0.2873821258544922,\n  0.00308957789093256,\n  0.0051577650010585785],\n [5,\n  5.0,\n  0.03218614682555199,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.0023512018378823996,\n  0.0046798912808299065],\n [5,\n  0.0,\n  0.030840162187814713,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847661823034,\n  -0.007043743971735239],\n [5,\n  17.0,\n  0.02896946109831333,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.0057857949286699295,\n  -0.0018508632201701403],\n [5,\n  19.0,\n  0.02833571471273899,\n  0.42402884364128113,\n  0.000810603320132941,\n  -0.005690243560820818,\n  -0.005403212737292051],\n [5,\n  7.0,\n  0.027755146846175194,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.0028729636687785387,\n  0.0061393859796226025],\n [5,\n  11.0,\n  0.0273995790630579,\n  -0.0033466939348727465,\n  0.43197140097618103,\n  0.0022143691312521696,\n  0.005311156157404184],\n [5,\n  17.0,\n  0.027385596185922623,\n  0.002377033932134509,\n  -0.007107321172952652,\n  -0.0047452799044549465,\n  0.005192495416849852],\n [5,\n  10.0,\n  0.02640070579946041,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.0012423492735251784,\n  0.00390324043110013],\n [5,\n  3.0,\n  0.02587714046239853,\n  0.5747456550598145,\n  -0.0007388429949060082,\n  0.002270226599648595,\n  -0.004265402909368277],\n [5,\n  10.0,\n  0.02549077570438385,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.00406848918646574,\n  0.003159330692142248],\n [5,\n  1.0,\n  0.02135799452662468,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.005209512542933226,\n  0.004185027442872524],\n [5,\n  4.0,\n  0.0204045157879591,\n  0.14101380109786987,\n  -0.005262696649879217,\n  0.0020898061338812113,\n  -0.005596817936748266],\n [5,\n  15.0,\n  0.018854616209864616,\n  -0.0022594714537262917,\n  0.1384880095720291,\n  0.002785480348393321,\n  0.005815903190523386],\n [5,\n  12.0,\n  0.018164731562137604,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829452658072114,\n  0.0022208665031939745],\n [5,\n  10.0,\n  0.017963578924536705,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879381965845823,\n  0.005034215282648802],\n [5,\n  19.0,\n  0.01641383022069931,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.00016226967272814363,\n  -0.005205052439123392],\n [5,\n  3.0,\n  0.013968081213533878,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.005460468120872974,\n  0.0035145115107297897],\n [5,\n  19.0,\n  0.010167024098336697,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.0014292154228314757,\n  -0.004541652277112007],\n [5,\n  11.0,\n  0.009437757544219494,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314248450100422,\n  -0.005225767847150564],\n [5,\n  5.0,\n  0.008821120485663414,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.0027583171613514423,\n  0.0005774052697233856],\n [5,\n  4.0,\n  0.007675145752727985,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203426782041788,\n  -0.0008360188221558928],\n [5,\n  17.0,\n  0.007499020081013441,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.001250620000064373,\n  -0.000490324106067419],\n [5,\n  15.0,\n  0.0074683791026473045,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.00045365694677457213,\n  -0.002246222924441099],\n [5,\n  17.0,\n  0.0066661536693573,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.00255389790982008,\n  0.0024864282459020615],\n [5,\n  2.0,\n  0.004264900926500559,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.0026889850851148367,\n  0.006471899803727865],\n [5,\n  5.0,\n  0.0042508491314947605,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021460867486894],\n [5,\n  17.0,\n  0.0005488761235028505,\n  0.0021971387322992086,\n  0.5736400485038757,\n  -0.002658810233697295,\n  -0.0026629820931702852],\n [6,\n  16.0,\n  0.048807837069034576,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014816153794527,\n  -0.0005609015352092683],\n [6,\n  18.0,\n  0.045965634286403656,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246647681109607,\n  0.0013500606874004006],\n [6,\n  5.0,\n  0.04222835972905159,\n  0.43169596791267395,\n  0.852234423160553,\n  0.001544719678349793,\n  -2.092823342536576e-05],\n [6,\n  17.0,\n  0.04152437299489975,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269533753395,\n  0.0023152935318648815],\n [6,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.0015880210557952523,\n  -0.0007841713959351182],\n [6,\n  10.0,\n  0.03657899051904678,\n  0.28723227977752686,\n  0.001932673272676766,\n  0.006220615468919277,\n  0.00047305814223363996],\n [6,\n  11.0,\n  0.035480305552482605,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.00324416090734303,\n  0.0029634025413542986],\n [6,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.00097929616458714,\n  -0.002543338341638446],\n [6,\n  2.0,\n  0.034323837608098984,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.0020766041707247496,\n  -0.0033999979496002197],\n [6,\n  1.0,\n  0.03337988257408142,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973263652995229,\n  -0.0029923722613602877],\n [6,\n  1.0,\n  0.03324148803949356,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.0064412811771035194,\n  -0.00595964165404439],\n [6,\n  0.0,\n  0.03252032771706581,\n  0.720090925693512,\n  0.2873821258544922,\n  0.0030895783565938473,\n  0.0051577650010585785],\n [6,\n  5.0,\n  0.03218614310026169,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.0023512018378823996,\n  0.004679890815168619],\n [6,\n  0.0,\n  0.030840162187814713,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847661823034,\n  -0.007043744437396526],\n [6,\n  17.0,\n  0.02896946296095848,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.005785795394331217,\n  -0.0018508632201701403],\n [6,\n  19.0,\n  0.02833571471273899,\n  0.42402884364128113,\n  0.0008106034365482628,\n  -0.005690244026482105,\n  -0.005403212737292051],\n [6,\n  7.0,\n  0.027755148708820343,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.0028729632031172514,\n  0.006139386910945177],\n [6,\n  11.0,\n  0.0273995790630579,\n  -0.003346693702042103,\n  0.43197140097618103,\n  0.0022143693640828133,\n  0.005311157088726759],\n [6,\n  17.0,\n  0.027385596185922623,\n  0.002377033932134509,\n  -0.007107322104275227,\n  -0.004745279438793659,\n  0.005192494951188564],\n [6,\n  10.0,\n  0.02640070579946041,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.001242348924279213,\n  0.0039032401982694864],\n [6,\n  3.0,\n  0.02587714046239853,\n  0.5747456550598145,\n  -0.0007388432277366519,\n  0.002270226366817951,\n  -0.004265402443706989],\n [6,\n  10.0,\n  0.02549077570438385,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.00406848918646574,\n  0.0031593304593116045],\n [6,\n  1.0,\n  0.021357998251914978,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.005209512542933226,\n  0.004185027908533812],\n [6,\n  4.0,\n  0.02040451392531395,\n  0.14101380109786987,\n  -0.00526269618421793,\n  0.0020898065995424986,\n  -0.0055968184024095535],\n [6,\n  15.0,\n  0.018854619935154915,\n  -0.0022594714537262917,\n  0.1384880095720291,\n  0.0027854801155626774,\n  0.005815902724862099],\n [6,\n  12.0,\n  0.018164733424782753,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829456150531769,\n  0.002220866270363331],\n [6,\n  10.0,\n  0.017963580787181854,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879379637539387,\n  0.005034215282648802],\n [6,\n  19.0,\n  0.01641383208334446,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.00016226971638388932,\n  -0.005205052904784679],\n [6,\n  3.0,\n  0.013968080282211304,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.0054604667238891125,\n  0.003514510579407215],\n [6,\n  19.0,\n  0.010167023167014122,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.001429215306416154,\n  -0.004541652277112007],\n [6,\n  11.0,\n  0.00943775661289692,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314247518777847,\n  -0.005225767847150564],\n [6,\n  5.0,\n  0.008821118623018265,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.0027583171613514423,\n  0.0005774052115157247],\n [6,\n  4.0,\n  0.00767514668405056,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203430274501443,\n  -0.0008360182982869446],\n [6,\n  17.0,\n  0.007499022409319878,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.0012506204657256603,\n  -0.0004903242806904018],\n [6,\n  15.0,\n  0.007468376774340868,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.0004536568303592503,\n  -0.002246223157271743],\n [6,\n  17.0,\n  0.006666154600679874,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.00255389790982008,\n  0.002486428013071418],\n [6,\n  2.0,\n  0.004264900926500559,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.0026889857836067677,\n  0.006471899803727865],\n [6,\n  5.0,\n  0.004250847734510899,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021462613716722],\n [6,\n  17.0,\n  0.0005488742608577013,\n  0.0021971389651298523,\n  0.5736400485038757,\n  -0.002658810233697295,\n  -0.0026629818603396416],\n [7,\n  16.0,\n  0.048807840794324875,\n  0.28639811277389526,\n  0.1367788016796112,\n  0.00024014808877836913,\n  -0.0005609017098322511],\n [7,\n  18.0,\n  0.04596562683582306,\n  0.14614132046699524,\n  0.14432477951049805,\n  -0.0006246645352803171,\n  0.0013500613858923316],\n [7,\n  5.0,\n  0.04222835600376129,\n  0.43169596791267395,\n  0.852234423160553,\n  0.0015447194455191493,\n  -2.0928748199366964e-05],\n [7,\n  17.0,\n  0.04152437299489975,\n  0.2838396430015564,\n  0.5691403150558472,\n  -0.003964269533753395,\n  0.002315293299034238],\n [7,\n  19.0,\n  0.03887373208999634,\n  0.853602945804596,\n  0.2924693822860718,\n  0.001588021288625896,\n  -0.0007841718033887446],\n [7,\n  10.0,\n  0.03657899051904678,\n  0.28723227977752686,\n  0.0019326735055074096,\n  0.0062206159345805645,\n  0.00047305854968726635],\n [7,\n  11.0,\n  0.035480301827192307,\n  0.5692073702812195,\n  0.28337833285331726,\n  0.0032441611401736736,\n  0.002963402308523655],\n [7,\n  0.0,\n  0.034344859421253204,\n  0.715355634689331,\n  0.5746159553527832,\n  -0.0009792960481718183,\n  -0.002543338807299733],\n [7,\n  2.0,\n  0.034323837608098984,\n  0.2915676236152649,\n  0.27988576889038086,\n  0.0020766041707247496,\n  -0.0033999981824308634],\n [7,\n  1.0,\n  0.03337988257408142,\n  0.8600233197212219,\n  0.854455292224884,\n  -0.0015973268309608102,\n  -0.002992372028529644],\n [7,\n  1.0,\n  0.03324149176478386,\n  0.5760607123374939,\n  0.13815787434577942,\n  -0.0064412811771035194,\n  -0.005959640722721815],\n [7,\n  0.0,\n  0.03252032771706581,\n  0.720090925693512,\n  0.2873821258544922,\n  0.0030895783565938473,\n  0.0051577650010585785],\n [7,\n  5.0,\n  0.03218614682555199,\n  0.29011401534080505,\n  0.43338435888290405,\n  0.0023512018378823996,\n  0.004679890815168619],\n [7,\n  0.0,\n  0.030840162187814713,\n  0.7094765901565552,\n  0.4298780560493469,\n  -0.004852847661823034,\n  -0.007043743971735239],\n [7,\n  17.0,\n  0.02896946482360363,\n  0.5695548057556152,\n  0.7205935716629028,\n  -0.0057857949286699295,\n  -0.0018508631037548184],\n [7,\n  19.0,\n  0.02833571657538414,\n  0.42402884364128113,\n  0.0008106033783406019,\n  -0.005690244026482105,\n  -0.005403212737292051],\n [7,\n  7.0,\n  0.027755150571465492,\n  0.14687421917915344,\n  0.7170413136482239,\n  -0.002872963435947895,\n  0.006139386910945177],\n [7,\n  11.0,\n  0.0273995790630579,\n  -0.0033466939348727465,\n  0.43197140097618103,\n  0.0022143691312521696,\n  0.005311156623065472],\n [7,\n  17.0,\n  0.027385598048567772,\n  0.002377033932134509,\n  -0.007107321638613939,\n  -0.0047452799044549465,\n  0.005192495416849852],\n [7,\n  10.0,\n  0.026400703936815262,\n  0.7067244052886963,\n  0.855017364025116,\n  -0.0012423486914485693,\n  0.0039032401982694864],\n [7,\n  3.0,\n  0.02587713859975338,\n  0.5747456550598145,\n  -0.0007388429949060082,\n  0.002270226599648595,\n  -0.004265402909368277],\n [7,\n  10.0,\n  0.02549077570438385,\n  0.2899598479270935,\n  0.8518151640892029,\n  0.00406848918646574,\n  0.0031593304593116045],\n [7,\n  1.0,\n  0.021357998251914978,\n  0.13988235592842102,\n  0.4246824383735657,\n  -0.005209512542933226,\n  0.004185027908533812],\n [7,\n  4.0,\n  0.02040451392531395,\n  0.14101380109786987,\n  -0.005262696649879217,\n  0.0020898065995424986,\n  -0.0055968184024095535],\n [7,\n  15.0,\n  0.018854618072509766,\n  -0.0022594714537262917,\n  0.1384880095720291,\n  0.0027854805812239647,\n  0.005815903190523386],\n [7,\n  12.0,\n  0.018164731562137604,\n  0.7163902521133423,\n  0.7119909524917603,\n  -0.0010829456150531769,\n  0.002220866270363331],\n [7,\n  10.0,\n  0.017963577061891556,\n  0.13925801217556,\n  0.5659564137458801,\n  0.0021879379637539387,\n  0.005034215282648802],\n [7,\n  19.0,\n  0.01641383022069931,\n  0.42223626375198364,\n  0.4327896535396576,\n  -0.0001622694981051609,\n  -0.005205052439123392],\n [7,\n  3.0,\n  0.013968080282211304,\n  0.5662983655929565,\n  0.8560060262680054,\n  0.005460467655211687,\n  0.003514510579407215],\n [7,\n  19.0,\n  0.010167023167014122,\n  0.5683696269989014,\n  0.5717867016792297,\n  -0.001429215190000832,\n  -0.004541652277112007],\n [7,\n  11.0,\n  0.00943775661289692,\n  0.8632266521453857,\n  0.42669495940208435,\n  -0.004314247518777847,\n  -0.005225767847150564],\n [7,\n  5.0,\n  0.008821118623018265,\n  0.14858995378017426,\n  0.8628853559494019,\n  -0.0027583171613514423,\n  0.0005774050951004028],\n [7,\n  4.0,\n  0.007675145752727985,\n  0.8546355366706848,\n  0.7188422083854675,\n  -0.0019203427946195006,\n  -0.0008360186475329101],\n [7,\n  17.0,\n  0.007499019615352154,\n  0.42597904801368713,\n  0.7159280180931091,\n  -0.0012506203493103385,\n  -0.0004903243388980627],\n [7,\n  15.0,\n  0.007468379568308592,\n  0.2914716899394989,\n  0.7104690074920654,\n  -0.0004536565684247762,\n  -0.002246222924441099],\n [7,\n  17.0,\n  0.006666154600679874,\n  0.7089016437530518,\n  0.14547641575336456,\n  -0.002553897676989436,\n  0.0024864282459020615],\n [7,\n  2.0,\n  0.004264899529516697,\n  0.5711851119995117,\n  0.43296027183532715,\n  0.0026889850851148367,\n  0.006471899803727865],\n [7,\n  5.0,\n  0.0042508440092206,\n  0.4341600239276886,\n  0.5686967372894287,\n  -0.0048612928949296474,\n  -0.0006021459703333676],\n [7,\n  17.0,\n  0.0005488756578415632,\n  0.002197139197960496,\n  0.5736400485038757,\n  -0.002658810233697295,\n  -0.002662982326000929]]\n\n\n\ntarget_boxes\n\n[[0,\n  6.0,\n  1.0,\n  0.5360000133514404,\n  0.5619999766349792,\n  0.9013333916664124,\n  0.5360000133514404],\n [1,\n  7.0,\n  1.0,\n  0.48842108249664307,\n  0.512499988079071,\n  0.8294737339019775,\n  0.9458333849906921],\n [2,\n  7.0,\n  1.0,\n  0.27500003576278687,\n  0.4933333694934845,\n  0.5099999904632568,\n  0.3893333673477173],\n [2,\n  7.0,\n  1.0,\n  0.7120000720024109,\n  0.4560000002384186,\n  0.5040000677108765,\n  0.4480000436306],\n [3,\n  1.0,\n  1.0,\n  0.5913174152374268,\n  0.5429999828338623,\n  0.6377246379852295,\n  0.8019999861717224],\n [4,\n  14.0,\n  1.0,\n  0.514970064163208,\n  0.4610000252723694,\n  0.5928143858909607,\n  0.9220000505447388],\n [4,\n  14.0,\n  1.0,\n  0.8742515444755554,\n  0.5040000677108765,\n  0.24550898373126984,\n  0.9360000491142273],\n [4,\n  1.0,\n  1.0,\n  0.48353296518325806,\n  0.7000000476837158,\n  0.5538922548294067,\n  0.5960000157356262],\n [5,\n  6.0,\n  1.0,\n  0.6390000581741333,\n  0.5675675868988037,\n  0.718000054359436,\n  0.8408408761024475],\n [6,\n  14.0,\n  1.0,\n  0.37700000405311584,\n  0.5640000104904175,\n  0.15800002217292786,\n  0.3813333213329315],\n [6,\n  12.0,\n  1.0,\n  0.3370000422000885,\n  0.6666667461395264,\n  0.4020000398159027,\n  0.421333372592926],\n [6,\n  14.0,\n  1.0,\n  0.5529999732971191,\n  0.7000000476837158,\n  0.07800000160932541,\n  0.34933337569236755],\n [6,\n  14.0,\n  1.0,\n  0.6100000143051147,\n  0.7066667079925537,\n  0.08400000631809235,\n  0.34666669368743896],\n [7,\n  14.0,\n  1.0,\n  0.3184524178504944,\n  0.445000022649765,\n  0.369047611951828,\n  0.5659999847412109],\n [7,\n  14.0,\n  1.0,\n  0.80952388048172,\n  0.515999972820282,\n  0.3750000298023224,\n  0.8920000195503235],\n [7,\n  11.0,\n  1.0,\n  0.269345223903656,\n  0.6210000514984131,\n  0.538690447807312,\n  0.3059999942779541]]\n\n\nInside of mean_average_precision",
    "crumbs": [
      "Yolo v1"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "TODO:",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#image-processing",
    "href": "core.html#image-processing",
    "title": "core",
    "section": "Image processing",
    "text": "Image processing\n\nBacteria image\nWe have videos of bacteria. However, let’s work with images with bounding boxes first. We save first frame as images. Something was wrong with 20mins021.nd2 file, so we won’t use it.\n\nvideo_path = Path.home()/'data/pili/training_videos'\nvideo_path.ls()\n\n(#9) [Path('/home/kappa/data/pili/training_videos/200ms-0.4%-005.nd2'),Path('/home/kappa/data/pili/training_videos/0N01002.nd2'),Path('/home/kappa/data/pili/training_videos/7.1- 003.nd2'),Path('/home/kappa/data/pili/training_videos/0.1%.004.nd2'),Path('/home/kappa/data/pili/training_videos/1hr01002.nd2'),Path('/home/kappa/data/pili/training_videos/dCpdA R1 FH 017.nd2'),Path('/home/kappa/data/pili/training_videos/4hrs incu004.nd2'),Path('/home/kappa/data/pili/training_videos/WT-A86C-LB-ice-002.nd2'),Path('/home/kappa/data/pili/training_videos/Chp B Replicate 2 200 MS060.nd2')]\n\n\nLet’s take a look at an image of the first video.\n\nvp = video_path.ls()[0]\nvp\n\nPath('/home/kappa/data/pili/training_videos/200ms-0.4%-005.nd2')\n\n\nFor each video, we want to only use the first frame for now. This is what it looks like:\n\nwith nd2.ND2File(vp) as nd2_file:\n    first_frame = nd2_file.read_frame(0)\n    plt.imshow(first_frame)\n    plt.axis('off')\n\n\n\n\n\n\n\n\nNow we turn the video into an image.\n\ndef save_first_frame(input_path, output_path, extension = '.png') -&gt; None:\n    \"\"\"\n    Save the first frame of an ND2 file as an image, using the same name as input file.\n    \"\"\"\n    filename = input_path.name.removesuffix('.nd2')  # Some names have . in the filename\n    output_path = (output_path/f'{filename}{extension}')\n    Image.fromarray(nd2.imread(input_path)[0]).save(output_path)\n\n\n# Save videos as images\n# for p in video_path.ls(): \n#     if p.suffix == '.nd2': save_first_frame(p, path)\n\n\npath = Path.home()/'data/pili/training_data'\npath.ls()\n\n(#18) [Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.png'),Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.png'),Path('/home/kappa/data/pili/training_data/1hr01002.csv'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.png'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.png'),Path('/home/kappa/data/pili/training_data/4hrs incu004.csv'),Path('/home/kappa/data/pili/training_data/7.1- 003.png'),Path('/home/kappa/data/pili/training_data/4hrs incu004.png'),Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.csv'),Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.csv'),Path('/home/kappa/data/pili/training_data/0.1%.004.png'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.csv'),Path('/home/kappa/data/pili/training_data/0N01002.csv'),Path('/home/kappa/data/pili/training_data/0N01002.png'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.csv'),Path('/home/kappa/data/pili/training_data/7.1- 003.csv'),Path('/home/kappa/data/pili/training_data/1hr01002.png'),Path('/home/kappa/data/pili/training_data/0.1%.004.csv')]\n\n\nWe can now use show_image to display images.\n\nimg_path = path/'0N01002.png'\nbac_im = np.array(Image.open(img_path))\nshow_image(bac_im, figsize=(4,4));\n\n\n\n\n\n\n\n\nNow, we want to take care of bounding boxes. We first turn them into YOLO bounding box format because they are in angle format.\nYOLO bounding box format:\n'class_index', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4'\nwhere x1, …, y4 are edge points for each box.\ncsv files have information about the box. Length, Width, Position X and Position Y are in nanometer(?).\n\ndf = pd.read_csv(path/'0N01002.csv')\ndf.head()\n\n\n\n\n\n\n\n\nName\nLength\nWidth\nAngle\nPosition X\nPosition Y\nColor R\nColor G\nColor B\nType\n\n\n\n\n0\nBox 1\n2.100000e-06\n0.000001\n0.733038\n0.000011\n0.000082\n0.509804\n0.901961\n0.509804\n1\n\n\n1\nBox 2\n2.300000e-06\n0.000001\n0.401426\n0.000015\n0.000083\n0.509804\n0.901961\n0.509804\n1\n\n\n2\nBox 3\n2.000000e-06\n0.000001\n0.837758\n0.000013\n0.000072\n0.509804\n0.901961\n0.509804\n1\n\n\n3\nBox 4\n2.100000e-06\n0.000001\n1.832596\n0.000029\n0.000075\n0.509804\n0.901961\n0.509804\n1\n\n\n4\nBox 5\n9.000000e-07\n0.000001\n-1.553343\n0.000026\n0.000084\n1.000000\n0.000000\n0.549020\n6\n\n\n\n\n\n\n\nTODO: make class_index 0-based.\n\nsource\n\n\ncalc_corners\n\n calc_corners (csv, max_pos=8.458666666666666e-05)\n\n\nbac_boxes = calc_corners(path/'0N01002.csv')\nbac_boxes[:5]\n\ntensor([[1.0000, 0.1295, 0.9836, 0.1110, 0.9670, 0.1205, 0.9565, 0.1390, 0.9731],\n        [1.0000, 0.1901, 0.9989, 0.1651, 0.9883, 0.1706, 0.9752, 0.1957, 0.9859],\n        [1.0000, 0.1547, 0.8677, 0.1389, 0.8501, 0.1494, 0.8406, 0.1652, 0.8582],\n        [1.0000, 0.3313, 0.8914, 0.3377, 0.8674, 0.3514, 0.8710, 0.3450, 0.8950],\n        [6.0000, 0.3144, 0.9890, 0.3142, 0.9996, 0.3000, 0.9994, 0.3002, 0.9887]],\n       dtype=torch.float64)\n\n\nWe want to take a look at images with bounding boxes.\n/opt/hostedtoolcache/Python/3.10.17/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Other Parameters\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.17/x64/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: Unknown section See Also\n  else: warn(msg)\n\n\n\nshow_bac_img_with_boxes\n\n show_bac_img_with_boxes (im, boxes, figsize=(8, 8), title=None, ax=None,\n                          legend=None, legend_loc='upper left', cmap=None,\n                          norm=None, aspect=None, interpolation=None,\n                          alpha=None, vmin=None, vmax=None,\n                          colorizer=None, origin=None, extent=None,\n                          interpolation_stage=None, filternorm=True,\n                          filterrad=4.0, resample=None, url=None,\n                          data=None)\n\nDisplay image with bounding boxes for different cell types, returns fig and ax for further customization\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nim\n\n\n\n\n\nboxes\n\n\n\n\n\nfigsize\ntuple\n(8, 8)\n\n\n\ntitle\nNoneType\nNone\n\n\n\nax\nNoneType\nNone\n\n\n\nlegend\nNoneType\nNone\n\n\n\nlegend_loc\nstr\nupper left\n\n\n\ncmap\nNoneType\nNone\nThe Colormap instance or registered colormap name used to map scalar datato colors.This parameter is ignored if X is RGB(A).\n\n\nnorm\nNoneType\nNone\nThe normalization method used to scale scalar data to the [0, 1] rangebefore mapping to colors using cmap. By default, a linear scaling isused, mapping the lowest value to 0 and the highest to 1.If given, this can be one of the following:- An instance of .Normalize or one of its subclasses (see :ref:colormapnorms).- A scale name, i.e. one of “linear”, “log”, “symlog”, “logit”, etc. For a list of available scales, call matplotlib.scale.get_scale_names(). In that case, a suitable .Normalize subclass is dynamically generated and instantiated.This parameter is ignored if X is RGB(A).\n\n\naspect\nNoneType\nNone\nThe aspect ratio of the Axes. This parameter is particularlyrelevant for images since it determines whether data pixels aresquare.This parameter is a shortcut for explicitly calling.Axes.set_aspect. See there for further details.- ‘equal’: Ensures an aspect ratio of 1. Pixels will be square (unless pixel sizes are explicitly made non-square in data coordinates using extent).- ‘auto’: The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels.Normally, None (the default) means to use :rc:image.aspect. However, ifthe image uses a transform that does not contain the axes data transform,then None means to not modify the axes aspect at all (in that case, directlycall .Axes.set_aspect if desired).\n\n\ninterpolation\nNoneType\nNone\nThe interpolation method used.Supported values are ‘none’, ‘auto’, ‘nearest’, ‘bilinear’,‘bicubic’, ‘spline16’, ‘spline36’, ‘hanning’, ‘hamming’, ‘hermite’,‘kaiser’, ‘quadric’, ‘catrom’, ‘gaussian’, ‘bessel’, ‘mitchell’,‘sinc’, ‘lanczos’, ‘blackman’.The data X is resampled to the pixel size of the image on thefigure canvas, using the interpolation method to either up- ordownsample the data.If interpolation is ‘none’, then for the ps, pdf, and svgbackends no down- or upsampling occurs, and the image data ispassed to the backend as a native image. Note that different ps,pdf, and svg viewers may display these raw pixels differently. Onother backends, ‘none’ is the same as ‘nearest’.If interpolation is the default ‘auto’, then ‘nearest’interpolation is used if the image is upsampled by more than afactor of three (i.e. the number of display pixels is at leastthree times the size of the data array). If the upsampling rate issmaller than 3, or the image is downsampled, then ‘hanning’interpolation is used to act as an anti-aliasing filter, unless theimage happens to be upsampled by exactly a factor of two or one.See:doc:/gallery/images_contours_and_fields/interpolation_methodsfor an overview of the supported interpolation methods, and:doc:/gallery/images_contours_and_fields/image_antialiasing fora discussion of image antialiasing.Some interpolation methods require an additional radius parameter,which can be set by filterrad. Additionally, the antigrain imageresize filter is controlled by the parameter filternorm.\n\n\nalpha\nNoneType\nNone\nThe alpha blending value, between 0 (transparent) and 1 (opaque).If alpha is an array, the alpha blending values are applied pixelby pixel, and alpha must have the same shape as X.\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\ncolorizer\nNoneType\nNone\nThe Colorizer object used to map color to data. If None, a Colorizerobject is created from a norm and cmap.This parameter is ignored if X is RGB(A).\n\n\norigin\nNoneType\nNone\nPlace the [0, 0] index of the array in the upper left or lowerleft corner of the Axes. The convention (the default) ‘upper’ istypically used for matrices and images.Note that the vertical axis points upward for ‘lower’but downward for ‘upper’.See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\nextent\nNoneType\nNone\nThe bounding box in data coordinates that the image will fill.These values may be unitful and match the units of the Axes.The image is stretched individually along x and y to fill the box.The default extent is determined by the following conditions.Pixels have unit size in data coordinates. Their centers are oninteger coordinates, and their center coordinates range from 0 tocolumns-1 horizontally and from 0 to rows-1 vertically.Note that the direction of the vertical axis and thus the defaultvalues for top and bottom depend on origin:- For origin == 'upper' the default is (-0.5, numcols-0.5, numrows-0.5, -0.5).- For origin == 'lower' the default is (-0.5, numcols-0.5, -0.5, numrows-0.5).See the :ref:imshow_extent tutorial forexamples and a more detailed description.\n\n\ninterpolation_stage\nNoneType\nNone\nSupported values:- ‘data’: Interpolation is carried out on the data provided by the user This is useful if interpolating between pixels during upsampling.- ‘rgba’: The interpolation is carried out in RGBA-space after the color-mapping has been applied. This is useful if downsampling and combining pixels visually.- ‘auto’: Select a suitable interpolation stage automatically. This uses ‘rgba’ when downsampling, or upsampling at a rate less than 3, and ‘data’ when upsampling at a higher rate.See :doc:/gallery/images_contours_and_fields/image_antialiasing fora discussion of image antialiasing.\n\n\nfilternorm\nbool\nTrue\nA parameter for the antigrain image resize filter (see theantigrain documentation). If filternorm is set, the filternormalizes integer values and corrects the rounding errors. Itdoesn’t do anything with the source floating point values, itcorrects only integers according to the rule of 1.0 which meansthat any sum of pixel weights must be equal to 1.0. So, thefilter function must produce a graph of the proper shape.\n\n\nfilterrad\nfloat\n4.0\nThe filter radius for filters that have a radius parameter, i.e.when interpolation is one of: ‘sinc’, ‘lanczos’ or ‘blackman’.\n\n\nresample\nNoneType\nNone\nWhen True, use a full resampling method. When False, onlyresample when the output image is larger than the input image.\n\n\nurl\nNoneType\nNone\nSet the url of the created .AxesImage. See .Artist.set_url.\n\n\ndata\nNoneType\nNone\n\n\n\n\n\nshow_bac_img_with_boxes(bac_im, bac_boxes, legend=True, legend_loc='best');\n\n\n\n\n\n\n\n\nWe want to train with yolov1, which has the ['class_index', 'x_mid', 'y_mid', 'w', 'h']. So we change the format of our coordinates.\n\ndef calc_yolo_boxes(csv, max_pos=8.458666666666666e-05):\n    df = pd.read_csv(csv)\n    results = []\n    for _, row in df.iterrows():\n        length, width, angle = row['Length'], row['Width'], row['Angle']\n        pos_x, pos_y = row['Position X'], row['Position Y']\n\n        x1 = pos_x + length/2 * np.cos(angle) - width/2 * np.sin(angle)\n        y1 = pos_y + length/2 * np.sin(angle) + width/2 * np.cos(angle)\n        x2 = pos_x - length/2 * np.cos(angle) - width/2 * np.sin(angle)\n        y2 = pos_y - length/2 * np.sin(angle) + width/2 * np.cos(angle)\n        x3 = pos_x - length/2 * np.cos(angle) + width/2 * np.sin(angle)\n        y3 = pos_y - length/2 * np.sin(angle) - width/2 * np.cos(angle)\n        x4 = pos_x + length/2 * np.cos(angle) + width/2 * np.sin(angle)\n        y4 = pos_y + length/2 * np.sin(angle) - width/2 * np.cos(angle)\n        \n        x_coords, y_coords = [x1, x2, x3, x4], [y1, y2, y3, y4]\n        x_center, y_center = sum(x_coords) / 4, sum(y_coords) / 4\n        bbox_width, bbox_height = max(x_coords) - min(x_coords), max(y_coords) - min(y_coords)\n        \n        results.append([row['Type'], x_center / max_pos, y_center / max_pos, bbox_width / max_pos, bbox_height / max_pos])\n\n    return torch.tensor(results)\n\n\nyolo_bac_boxes = calc_yolo_boxes(path/'0N01002.csv')\nyolo_bac_boxes[:5]\n\ntensor([[1.0000, 0.1250, 0.9701, 0.0279, 0.0272],\n        [1.0000, 0.1804, 0.9871, 0.0306, 0.0237],\n        [1.0000, 0.1521, 0.8542, 0.0264, 0.0271],\n        [1.0000, 0.3414, 0.8812, 0.0201, 0.0277],\n        [6.0000, 0.3072, 0.9942, 0.0144, 0.0109]], dtype=torch.float64)\n\n\nWe can use show_image_with_boxes to show an image with boxes. Whether it is bacteria image or VOC image.\n\nVOC_CLASSES = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \n           \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \n           \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n\n\n@fc.delegates(show_image)\ndef show_image_with_boxes(im, boxes, ax=None, figsize=(8,8), box_color='red', thickness=1, **kwargs):\n    \"Show image with bounding boxes with `[class_id, x_center, y_center, width, height]` format.\"\n    ax = show_image(im, ax=ax, figsize=figsize, **kwargs)\n    if isinstance(im, (PIL.JpegImagePlugin.JpegImageFile, PIL.PngImagePlugin.PngImageFile)):  # PIL.Image\n        h, w = im.height, im.width\n    elif len(im.shape) == 2:                               # Bac image w/o channel\n        h, w = im.shape\n    else:                                                  # VOC image\n        h, w = im.shape[1:]\n    \n    for box in boxes:\n        class_id, x_center, y_center, width, height = box\n        x1, y1 = int((x_center - width/2) * w), int((y_center - height/2) * h)\n        x2, y2 = int((x_center + width/2) * w), int((y_center + height/2) * h)\n        \n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=thickness, edgecolor=box_color, facecolor='none')\n        ax.add_patch(rect)\n        \n        ax.text(x1, y1-5, f\"{VOC_CLASSES[int(class_id)]}\", color=box_color)\n    \n    return ax\n\nTODO: Classes need to be changed for bac images.\n\nshow_image_with_boxes(bac_im, yolo_bac_boxes, figsize=(8,8));\n\n\n\n\n\n\n\n\n\n\nVOC image\nWe will also work with PASCAL VOC dataset. VOC dataset has images and bounding boxes for corresponding objects. We can train our model on this dataset before training on bacteria dataset as we don’t have too many samples for bacteria images.\n\nvoc_img_path = Path('../data/images')\nvoc_img_path\n\nPath('../data/images')\n\n\ncsv file has img and label.\n\ndf = pd.read_csv(\"../8examples.csv\")\ndf\n\n\n\n\n\n\n\n\nimg\nlabel\n\n\n\n\n0\n000007.jpg\n000007.txt\n\n\n1\n000009.jpg\n000009.txt\n\n\n2\n000016.jpg\n000016.txt\n\n\n3\n000019.jpg\n000019.txt\n\n\n4\n000020.jpg\n000020.txt\n\n\n5\n000021.jpg\n000021.txt\n\n\n6\n000122.jpg\n000122.txt\n\n\n7\n000129.jpg\n000129.txt\n\n\n\n\n\n\n\n\ndf.iloc[3]\n\nimg      000019.jpg\nlabel    000019.txt\nName: 3, dtype: object\n\n\nLet’s take a look at an image of two cats chilling or dreaming on the ground. 😺🐈\n\nvoc_im = Image.open(voc_img_path/df.iloc[3,0])\nvoc_im\n\n\n\n\n\n\n\n\nNow we get get the boxes. We also call boxes as labels because they are the targets we are trying to predict.\n\nvoc_lbl_path = Path('../data/labels')\nvoc_lbl_path\n\nPath('../data/labels')\n\n\nLabel file has boxes for each row. Each row has the shape of ['class_index', 'x_mid', 'y_mid', 'w', 'h']. Those coordinates for each box are normalized from 0 to 1, which makes them great for working with different sized images. It is important to keep in mind that x_mid and y_mid start from top left corner.\n\nwith open(voc_lbl_path/df.iloc[3,1]) as f:\n    for label in f.readlines():\n        print(label)\n\n7 0.712 0.45599999999999996 0.504 0.448\n\n7 0.275 0.4933333333333333 0.51 0.3893333333333333\n\n\n\nLet’s turn those into tensors.\n\ndef get_boxes(label_path):\n    with open(label_path) as f: \n        boxes = [[float(x) if '.' in x else int(x) for x in line.strip().split()] for line in f]\n    return torch.tensor(boxes)\n\n\nvoc_boxes = get_boxes(voc_lbl_path/df.iloc[3,1])\nvoc_boxes\n\ntensor([[7.0000, 0.7120, 0.4560, 0.5040, 0.4480],\n        [7.0000, 0.2750, 0.4933, 0.5100, 0.3893]])\n\n\nWe have 2 boxes, which have the same classes (cats) and bounding boxes.\n\nvoc_boxes.shape\n\ntorch.Size([2, 5])\n\n\n\nshow_image_with_boxes(voc_im, voc_boxes, figsize=(6,6));\n\n\n\n\n\n\n\n\n\n\nyolo utils\n\nseed = 123\ntorch.manual_seed(seed)\n\nLEARNING_RATE = 2e-5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 4 \nWEIGHT_DECAY = 0\nNUM_WORKERS = 4\nPIN_MEMORY = True\nLOAD_MODEL = False\nLOAD_MODEL_FILE = \"overfit.pth.tar\"\nIMG_DIR = \"../data/images\"\nLABEL_DIR = \"../data/labels\"\n\nWe have some utility functions we need. Let’s get the dataset for VOC. For bounding boxes, we are converting coordinates based on each image into split cells.\n\nclass VOCDataset(Dataset):\n    def __init__(self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None):\n        self.annotations, self.img_dir = pd.read_csv(csv_file), fc.Path(img_dir)\n        self.label_dir, self.transform = fc.Path(label_dir), transform\n        self.S, self.B, self.C = S, B, C\n\n    def __len__(self): return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = self.label_dir / self.annotations.iloc[index, 1]\n        img_path = self.img_dir / self.annotations.iloc[index, 0]\n        image = Image.open(img_path)\n        boxes = get_boxes(label_path)\n\n        if self.transform: image, boxes = self.transform(image, boxes)\n\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box\n            class_label = int(class_label)\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n            width_cell, height_cell = width * self.S, height * self.S\n\n            if label_matrix[i, j, 20] == 0:\n                label_matrix[i, j, 20] = 1\n                label_matrix[i, j, 21:25] = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n                label_matrix[i, j, class_label] = 1\n\n        return image, label_matrix\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        if img.mode != 'RGB': img = img.convert('RGB')\n        for t in self.transforms: img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\ntransform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n\ntrain_dataset = VOCDataset(\n    \"../8examples.csv\",\n    transform=transform,\n    img_dir=IMG_DIR,\n    label_dir=LABEL_DIR,\n)\nx0, y0 = train_dataset[3]\nx0.shape, y0.shape\n\n(torch.Size([3, 448, 448]), torch.Size([7, 7, 30]))\n\n\n\ny0[3]\n\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.9250, 0.4533, 3.5700, 2.7253, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 1.0000, 0.9840, 0.1920, 3.5280, 3.1360, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000]])\n\n\nTo be able to visualize this, we have to convert it back to previous format.\n\ndef cellboxes_to_boxes(predictions, S=7, conf_threshold=0.1):\n    \"Convert YOLO predictions to list of bounding boxes for each image\"\n    predictions = predictions.cpu().reshape(-1, S, S, 30)  # (bs, S, S, 2*B + C)\n    bs = predictions.shape[0]\n    \n    bboxes1, bboxes2 = predictions[..., 21:25], predictions[..., 26:30]  # (bs, S, S, 4)\n    scores = torch.stack([predictions[..., 20], predictions[..., 25]])   # (2, bs, S, S)\n    best_box = scores.argmax(0).bool()\n    best_boxes = torch.where(best_box.unsqueeze(-1), bboxes2, bboxes1)   # (bs, S, S, 4)\n    \n    i, j = torch.meshgrid(torch.arange(S), torch.arange(S), indexing='ij')\n    grid = torch.stack([j, i], dim=-1).unsqueeze(0).repeat(bs, 1, 1, 1).to(predictions.device)\n    x = (best_boxes[..., 0:1] + grid[..., 0:1]) / S                      # (bs, S, S, 2)\n    y = (best_boxes[..., 1:2] + grid[..., 1:2]) / S                      # (bs, S, S, 2)\n    w_h = best_boxes[..., 2:4] / S\n    \n    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(-1)\n    \n    result = torch.cat([predicted_class, best_confidence, x, y, w_h], dim=-1)\n    mask = best_confidence.squeeze(-1) &lt; conf_threshold                  \n    # remove xy cooordinates without object\n    result[mask] = 0                                                     # (bs, S, S, 6)\n    \n    converted_pred = result.reshape(result.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    return converted_pred.tolist()\n\n\ncellboxes_to_boxes(y0)\n\n[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [7.0,\n   1.0,\n   0.2750000059604645,\n   0.4933333396911621,\n   0.5099999904632568,\n   0.3893333375453949],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [7.0,\n   1.0,\n   0.7120000123977661,\n   0.4560000002384186,\n   0.5040000081062317,\n   0.4480000138282776],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]]\n\n\n\nlen(cellboxes_to_boxes(y0)[0])\n\n49\n\n\n\nmy_boxes = cellboxes_to_boxes(y0)[0]\nmy_boxes[:4]\n\n[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n\n\nmy_boxes has a shape of [class, confidence, x, y, w, h]. But we don’t need confidence for showing an image. We could add that option in the future.\n\n[[boxes[0]] + boxes[2:] for boxes in my_boxes][:4]\n\n[[0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0]]\n\n\n\n[[[boxes[0]] + boxes[2:] for boxes in my_boxes][25]]\n\n[[7.0,\n  0.7120000123977661,\n  0.4560000002384186,\n  0.5040000081062317,\n  0.4480000138282776]]\n\n\n\n[[[boxes[0]] + boxes[2:] for boxes in my_boxes]][0]\n\n[[0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [7.0,\n  0.2750000059604645,\n  0.4933333396911621,\n  0.5099999904632568,\n  0.3893333375453949],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [7.0,\n  0.7120000123977661,\n  0.4560000002384186,\n  0.5040000081062317,\n  0.4480000138282776],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0],\n [0.0, 0.0, 0.0, 0.0, 0.0]]\n\n\n\nshow_image_with_boxes(x0, [[[boxes[0]] + boxes[2:] for boxes in my_boxes]][0], figsize=(8,8));\n\n\n\n\n\n\n\n\n\nshow_image_with_boxes(x0, [[[boxes[0]] + boxes[2:] for boxes in my_boxes][25]], figsize=(8,8));\n\n\n\n\n\n\n\n\n\ndef shape_bb(bb, **kwargs): return [[[boxes[0]] + boxes[2:] for boxes in cellboxes_to_boxes(bb, **kwargs)[0]]][0]\n\n\nshow_image_with_boxes(x0, shape_bb(y0), figsize=(8,8));\n\n\n\n\n\n\n\n\n\n\nNotes to myself\n\ntest_preds = y0\n\n\nS = 7\npreds = test_preds.reshape(-1, S, S, 30)\nbs = preds.shape[0]\npreds.shape\n\ntorch.Size([1, 7, 7, 30])\n\n\n\nbboxes1, bboxes2 = preds[..., 21:25], preds[..., 26:30]\nbboxes1.shape, bboxes2.shape\n\n(torch.Size([1, 7, 7, 4]), torch.Size([1, 7, 7, 4]))\n\n\n\npreds[..., 20].shape, preds[..., 25].shape\n\n(torch.Size([1, 7, 7]), torch.Size([1, 7, 7]))\n\n\n\nscores = torch.stack([preds[..., 20], preds[..., 25]])\nscores.shape\n\ntorch.Size([2, 1, 7, 7])\n\n\n\nbest_box = scores.argmax(0).bool()\nbest_box.shape\n\ntorch.Size([1, 7, 7])\n\n\n\nbest_boxes = torch.where(best_box.unsqueeze(-1), bboxes2, bboxes1)  # (bs, S, S, 4)\nbest_boxes.shape\n\ntorch.Size([1, 7, 7, 4])\n\n\n\ni, j = torch.meshgrid(torch.arange(S), torch.arange(S), indexing='ij')\ni\n\ntensor([[0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1],\n        [2, 2, 2, 2, 2, 2, 2],\n        [3, 3, 3, 3, 3, 3, 3],\n        [4, 4, 4, 4, 4, 4, 4],\n        [5, 5, 5, 5, 5, 5, 5],\n        [6, 6, 6, 6, 6, 6, 6]])\n\n\n\nj\n\ntensor([[0, 1, 2, 3, 4, 5, 6],\n        [0, 1, 2, 3, 4, 5, 6],\n        [0, 1, 2, 3, 4, 5, 6],\n        [0, 1, 2, 3, 4, 5, 6],\n        [0, 1, 2, 3, 4, 5, 6],\n        [0, 1, 2, 3, 4, 5, 6],\n        [0, 1, 2, 3, 4, 5, 6]])\n\n\n\ngrid = torch.stack([j, i], dim=-1).unsqueeze(0).repeat(bs, 1, 1, 1).to(preds.device)\ngrid\n\ntensor([[[[0, 0],\n          [1, 0],\n          [2, 0],\n          [3, 0],\n          [4, 0],\n          [5, 0],\n          [6, 0]],\n\n         [[0, 1],\n          [1, 1],\n          [2, 1],\n          [3, 1],\n          [4, 1],\n          [5, 1],\n          [6, 1]],\n\n         [[0, 2],\n          [1, 2],\n          [2, 2],\n          [3, 2],\n          [4, 2],\n          [5, 2],\n          [6, 2]],\n\n         [[0, 3],\n          [1, 3],\n          [2, 3],\n          [3, 3],\n          [4, 3],\n          [5, 3],\n          [6, 3]],\n\n         [[0, 4],\n          [1, 4],\n          [2, 4],\n          [3, 4],\n          [4, 4],\n          [5, 4],\n          [6, 4]],\n\n         [[0, 5],\n          [1, 5],\n          [2, 5],\n          [3, 5],\n          [4, 5],\n          [5, 5],\n          [6, 5]],\n\n         [[0, 6],\n          [1, 6],\n          [2, 6],\n          [3, 6],\n          [4, 6],\n          [5, 6],\n          [6, 6]]]])\n\n\n\n(best_boxes[..., :2] + grid) / S\n\ntensor([[[[0.0000, 0.0000],\n          [0.1429, 0.0000],\n          [0.2857, 0.0000],\n          [0.4286, 0.0000],\n          [0.5714, 0.0000],\n          [0.7143, 0.0000],\n          [0.8571, 0.0000]],\n\n         [[0.0000, 0.1429],\n          [0.1429, 0.1429],\n          [0.2857, 0.1429],\n          [0.4286, 0.1429],\n          [0.5714, 0.1429],\n          [0.7143, 0.1429],\n          [0.8571, 0.1429]],\n\n         [[0.0000, 0.2857],\n          [0.1429, 0.2857],\n          [0.2857, 0.2857],\n          [0.4286, 0.2857],\n          [0.5714, 0.2857],\n          [0.7143, 0.2857],\n          [0.8571, 0.2857]],\n\n         [[0.0000, 0.4286],\n          [0.2750, 0.4933],\n          [0.2857, 0.4286],\n          [0.4286, 0.4286],\n          [0.7120, 0.4560],\n          [0.7143, 0.4286],\n          [0.8571, 0.4286]],\n\n         [[0.0000, 0.5714],\n          [0.1429, 0.5714],\n          [0.2857, 0.5714],\n          [0.4286, 0.5714],\n          [0.5714, 0.5714],\n          [0.7143, 0.5714],\n          [0.8571, 0.5714]],\n\n         [[0.0000, 0.7143],\n          [0.1429, 0.7143],\n          [0.2857, 0.7143],\n          [0.4286, 0.7143],\n          [0.5714, 0.7143],\n          [0.7143, 0.7143],\n          [0.8571, 0.7143]],\n\n         [[0.0000, 0.8571],\n          [0.1429, 0.8571],\n          [0.2857, 0.8571],\n          [0.4286, 0.8571],\n          [0.5714, 0.8571],\n          [0.7143, 0.8571],\n          [0.8571, 0.8571]]]])\n\n\n\nxy = (best_boxes[..., :2] + grid) / S\nxy.shape\n\ntorch.Size([1, 7, 7, 2])\n\n\n\nxy\n\ntensor([[[[0.0000, 0.0000],\n          [0.1429, 0.0000],\n          [0.2857, 0.0000],\n          [0.4286, 0.0000],\n          [0.5714, 0.0000],\n          [0.7143, 0.0000],\n          [0.8571, 0.0000]],\n\n         [[0.0000, 0.1429],\n          [0.1429, 0.1429],\n          [0.2857, 0.1429],\n          [0.4286, 0.1429],\n          [0.5714, 0.1429],\n          [0.7143, 0.1429],\n          [0.8571, 0.1429]],\n\n         [[0.0000, 0.2857],\n          [0.1429, 0.2857],\n          [0.2857, 0.2857],\n          [0.4286, 0.2857],\n          [0.5714, 0.2857],\n          [0.7143, 0.2857],\n          [0.8571, 0.2857]],\n\n         [[0.0000, 0.4286],\n          [0.2750, 0.4933],\n          [0.2857, 0.4286],\n          [0.4286, 0.4286],\n          [0.7120, 0.4560],\n          [0.7143, 0.4286],\n          [0.8571, 0.4286]],\n\n         [[0.0000, 0.5714],\n          [0.1429, 0.5714],\n          [0.2857, 0.5714],\n          [0.4286, 0.5714],\n          [0.5714, 0.5714],\n          [0.7143, 0.5714],\n          [0.8571, 0.5714]],\n\n         [[0.0000, 0.7143],\n          [0.1429, 0.7143],\n          [0.2857, 0.7143],\n          [0.4286, 0.7143],\n          [0.5714, 0.7143],\n          [0.7143, 0.7143],\n          [0.8571, 0.7143]],\n\n         [[0.0000, 0.8571],\n          [0.1429, 0.8571],\n          [0.2857, 0.8571],\n          [0.4286, 0.8571],\n          [0.5714, 0.8571],\n          [0.7143, 0.8571],\n          [0.8571, 0.8571]]]])\n\n\n\nw_h = best_boxes[..., 2:4] / S\nw_h.shape\n\ntorch.Size([1, 7, 7, 2])\n\n\n\nw_h\n\ntensor([[[[0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000]],\n\n         [[0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000]],\n\n         [[0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000]],\n\n         [[0.0000, 0.0000],\n          [0.5100, 0.3893],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.5040, 0.4480],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000]],\n\n         [[0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000]],\n\n         [[0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000]],\n\n         [[0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000],\n          [0.0000, 0.0000]]]])\n\n\n\npred_class = preds[..., :20].argmax(-1).unsqueeze(-1)\npred_class\n\ntensor([[[[0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0]],\n\n         [[0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0]],\n\n         [[0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0]],\n\n         [[0],\n          [7],\n          [0],\n          [0],\n          [7],\n          [0],\n          [0]],\n\n         [[0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0]],\n\n         [[0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0]],\n\n         [[0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0],\n          [0]]]])\n\n\n\nbest_confidence = torch.max(preds[..., 20], preds[..., 25]).unsqueeze(-1)\nbest_confidence.shape\n\ntorch.Size([1, 7, 7, 1])\n\n\n\ntorch.cat([pred_class, best_confidence, xy, w_h], dim=-1).shape\n\ntorch.Size([1, 7, 7, 6])\n\n\n\n\nother utils\n\ndef non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n    assert type(bboxes) == list\n    bboxes = sorted([box for box in bboxes if box[1] &gt; threshold], key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n        bboxes = [box for box in bboxes if box[0] != chosen_box[0] or \n                 intersection_over_union(torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format=box_format) &lt; iou_threshold]\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms\n\n\ndef intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n    if box_format == \"midpoint\":\n        box1_x1, box1_y1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2, boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n        box1_x2, box1_y2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2, boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n        box2_x1, box2_y1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2, boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n        box2_x2, box2_y2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2, boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n    else:\n        box1_x1, box1_y1, box1_x2, box1_y2 = boxes_preds[..., 0:1], boxes_preds[..., 1:2], boxes_preds[..., 2:3], boxes_preds[..., 3:4]\n        box2_x1, box2_y1, box2_x2, box2_y2 = boxes_labels[..., 0:1], boxes_labels[..., 1:2], boxes_labels[..., 2:3], boxes_labels[..., 3:4]\n\n    x1, y1 = torch.max(box1_x1, box2_x1), torch.max(box1_y1, box2_y1)\n    x2, y2 = torch.min(box1_x2, box2_x2), torch.min(box1_y2, box2_y2)\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    box1_area, box2_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1)), abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n    return intersection / (box1_area + box2_area - intersection + 1e-6)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#model-architecture",
    "href": "core.html#model-architecture",
    "title": "core",
    "section": "Model Architecture",
    "text": "Model Architecture\n\nsource\n\nCNNBlock\n\n CNNBlock (in_channels, out_channels, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nCNNBlock(1, 32, kernel_size=1, stride=1, padding=0)\n\nCNNBlock(\n  (conv): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (leakyrelu): LeakyReLU(negative_slope=0.1)\n)\n\n\n\nsource\n\n\nYolov1\n\n Yolov1 (in_channels=3, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nmodel = Yolov1(split_size=7, num_boxes=2, num_classes=20)\nmodel\n\nYolov1(\n  (darknet): Sequential(\n    (0): CNNBlock(\n      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (2): CNNBlock(\n      (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (4): CNNBlock(\n      (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (5): CNNBlock(\n      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (6): CNNBlock(\n      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (7): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (9): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (10): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (11): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (12): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (13): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (14): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (15): CNNBlock(\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (16): CNNBlock(\n      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (17): CNNBlock(\n      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (18): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (19): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (20): CNNBlock(\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (21): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (22): CNNBlock(\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (23): CNNBlock(\n      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (24): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (25): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (26): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n    (27): CNNBlock(\n      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (leakyrelu): LeakyReLU(negative_slope=0.1)\n    )\n  )\n  (fcs): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=50176, out_features=496, bias=True)\n    (2): Dropout(p=0.0, inplace=False)\n    (3): LeakyReLU(negative_slope=0.1)\n    (4): Linear(in_features=496, out_features=1470, bias=True)\n  )\n)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#loss-function",
    "href": "core.html#loss-function",
    "title": "core",
    "section": "Loss function",
    "text": "Loss function\n\nsource\n\nYoloLoss\n\n YoloLoss (S=7, B=2, C=20)\n\nCalculate the loss for yolo (v1) model\n\nS=7; B=2; C=20\nlambda_noobj = 0.5\nlambda_coord = 5\npredictions = torch.randn(1, 7, 7, 30)\ntarget = torch.randn(1, 7, 7, 25)\n\npredictions = predictions.reshape(-1, S, S, C + B * 5)  # (bs, S, S, 30)\npredictions.shape\n\ntorch.Size([1, 7, 7, 30])",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#dataset-and-dataloader",
    "href": "core.html#dataset-and-dataloader",
    "title": "core",
    "section": "DataSet and DataLoader",
    "text": "DataSet and DataLoader\n\nbs = 2\n\n\ntrn_ds = VOCDataset(\"../8examples.csv\", \n                    transform=transform,\n                    img_dir=IMG_DIR,\n                    label_dir=LABEL_DIR)\nx0, y0 = trn_ds[0]\nshow_image_with_boxes(x0, shape_bb(y0, conf_threshold=0.4));\n\n\n\n\n\n\n\n\n\nval_ds = VOCDataset('../8examples_val.csv', transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR)\nx0, y0 = val_ds[0]\nshow_image_with_boxes(x0, shape_bb(y0, conf_threshold=0.4));\n\n\n\n\n\n\n\n\n\ntrn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\nxb, yb = next(iter(trn_dl))\nxb.shape, yb.shape\n\n(torch.Size([2, 3, 448, 448]), torch.Size([2, 7, 7, 30]))\n\n\n\ndls = DataLoaders(trn_dl, val_dl)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#learner",
    "href": "core.html#learner",
    "title": "core",
    "section": "Learner",
    "text": "Learner\n\nclass MeanAP:\n    def __init__(self, num_classes=1, epsilon=1e-6, threshold=0.4, iou_threshold=0.5, box_format='midpoint'):\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n        self.box_format = box_format\n        self.average_precisions = []\n    \n    def reset(self):\n        self.average_precisions = []\n    \n    def compute(self):\n        return sum(self.average_precisions) / len(self.average_precisions)\n    \n    def update(self, pred, label):\n        # `get_bboxes` part\n        all_pred_boxes = []\n        all_true_boxes = []\n        train_idx = 0\n        batch_size = pred.shape[0]\n        pred_boxes = cellboxes_to_boxes(pred)\n        true_boxes = cellboxes_to_boxes(label)\n        \n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                pred_boxes[idx],\n                iou_threshold=self.iou_threshold,\n                threshold=self.threshold,\n                box_format=self.box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_boxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] &gt; self.threshold:\n                    all_true_boxes.append([train_idx] + box)\n        \n        pred_boxes = all_pred_boxes\n        true_boxes = all_true_boxes\n        \n        for c in range(self.num_classes):\n            detections = []\n            ground_truths = []\n\n            # Go through all predictions and targets,\n            # and only add the ones that belong to the\n            # current class c\n            for detection in pred_boxes:\n                if detection[1] == c:\n                    detections.append(detection)\n\n            for true_box in true_boxes:\n                if true_box[1] == c:\n                    ground_truths.append(true_box)\n\n            # find the amount of bboxes for each training example\n            # Counter here finds how many ground truth bboxes we get\n            # for each training example, so let's say img 0 has 3,\n            # img 1 has 5 then we will obtain a dictionary with:\n            # amount_bboxes = {0:3, 1:5}\n            amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n            # We then go through each key, val in this dictionary\n            # and convert to the following (w.r.t same example):\n            # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n            for key, val in amount_bboxes.items():\n                amount_bboxes[key] = torch.zeros(val)\n\n            # sort by box probabilities which is index 2\n            detections.sort(key=lambda x: x[2], reverse=True)\n            TP = torch.zeros((len(detections)))\n            FP = torch.zeros((len(detections)))\n            total_true_bboxes = len(ground_truths)\n\n            # If none exists for this class then we can safely skip\n            if total_true_bboxes == 0:\n                continue\n\n            for detection_idx, detection in enumerate(detections):\n                # Only take out the ground_truths that have the same\n                # training idx as detection\n                ground_truth_img = [\n                    bbox for bbox in ground_truths if bbox[0] == detection[0]\n                ]\n\n                num_gts = len(ground_truth_img)\n                best_iou = 0\n\n                for idx, gt in enumerate(ground_truth_img):\n                    iou = intersection_over_union(\n                        torch.tensor(detection[3:]),\n                        torch.tensor(gt[3:]),\n                        box_format=self.box_format,\n                    )\n\n                    if iou &gt; best_iou:\n                        best_iou = iou\n                        best_gt_idx = idx\n\n                if best_iou &gt; self.iou_threshold:\n                    # only detect ground truth detection once\n                    if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                        # true positive and add this bounding box to seen\n                        TP[detection_idx] = 1\n                        amount_bboxes[detection[0]][best_gt_idx] = 1\n                    else:\n                        FP[detection_idx] = 1\n\n                # if IOU is lower then the detection is a false positive\n                else:\n                    FP[detection_idx] = 1\n\n            TP_cumsum = torch.cumsum(TP, dim=0)\n            FP_cumsum = torch.cumsum(FP, dim=0)\n            recalls = TP_cumsum / (total_true_bboxes + self.epsilon)\n            precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + self.epsilon))\n            precisions = torch.cat((torch.tensor([1]), precisions))\n            recalls = torch.cat((torch.tensor([0]), recalls))\n            # torch.trapz for numerical integration\n            self.average_precisions.append(torch.trapz(precisions, recalls))\n\n\ncbs = [\n    TrainCB(),\n    DeviceCB(),\n    MetricsCB(MeanAP(num_classes=20)),\n]\nopt = partial(torch.optim.AdamW, betas=(0.9,0.95), eps=1e-5)\n\n\nmodel = Yolov1(split_size=7, num_boxes=2, num_classes=20)\n\nlr, epochs = 1e-4, 20\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\n\nlearn = Learner(model, dls, YoloLoss(), lr=lr, cbs=cbs+xtra, opt_func=torch.optim.AdamW)\n\n\nlearn.show_image_batch(max_n=1)\n\n\n\n\n\n\n\n\n\nlearn.fit(epochs, cbs=[ProgressCB(plot=True)])\n\n\n\n\n\n\n\n\nMeanAP\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.000\n164.220\n0\ntrain\n00:04\n\n\n0.000\n211.467\n0\neval\n00:01\n\n\n0.000\n103.889\n1\ntrain\n00:05\n\n\n0.000\n200.495\n1\neval\n00:01\n\n\n0.000\n64.096\n2\ntrain\n00:07\n\n\n0.000\n182.836\n2\neval\n00:01\n\n\n0.000\n60.757\n3\ntrain\n00:07\n\n\n0.000\n172.836\n3\neval\n00:01\n\n\n0.050\n58.652\n4\ntrain\n00:05\n\n\n0.000\n169.163\n4\neval\n00:01\n\n\n0.011\n37.878\n5\ntrain\n00:06\n\n\n0.000\n167.833\n5\neval\n00:01\n\n\n0.129\n87.072\n6\ntrain\n00:07\n\n\n0.000\n192.208\n6\neval\n00:01\n\n\n0.008\n54.140\n7\ntrain\n00:07\n\n\n0.009\n224.684\n7\neval\n00:01\n\n\n0.050\n46.042\n8\ntrain\n00:07\n\n\n0.000\n275.958\n8\neval\n00:01\n\n\n0.045\n32.179\n9\ntrain\n00:06\n\n\n0.000\n286.695\n9\neval\n00:01\n\n\n0.098\n35.831\n10\ntrain\n00:06\n\n\n0.009\n300.782\n10\neval\n00:01\n\n\n0.142\n26.238\n11\ntrain\n00:06\n\n\n0.000\n311.709\n11\neval\n00:01\n\n\n0.245\n28.274\n12\ntrain\n00:06\n\n\n0.000\n323.532\n12\neval\n00:01\n\n\n0.157\n26.449\n13\ntrain\n00:06\n\n\n0.000\n325.302\n13\neval\n00:01\n\n\n0.183\n19.315\n14\ntrain\n00:06\n\n\n0.023\n301.128\n14\neval\n00:01\n\n\n0.450\n14.059\n15\ntrain\n00:06\n\n\n0.023\n289.292\n15\neval\n00:01\n\n\n0.394\n15.945\n16\ntrain\n00:07\n\n\n0.091\n272.089\n16\neval\n00:01\n\n\n0.491\n10.472\n17\ntrain\n00:06\n\n\n0.091\n253.606\n17\neval\n00:01\n\n\n0.450\n19.727\n18\ntrain\n00:06\n\n\n0.091\n250.489\n18\neval\n00:01\n\n\n0.636\n9.860\n19\ntrain\n00:06\n\n\n0.091\n245.718\n19\neval\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nWith a small dataset of 8, batch size is very important factor. When batch size is 2, model gets updated frequently and results in different bounding boxes. However, if it is 4 to 8, it takes more epochs to get close bounding boxes.\nThe images shown are predi\n\nlearn.model.eval()\nfor i in range(8):\n    x0, y0 = trn_ds[i]\n#     bboxes = cellboxes_to_boxes(learn.model(x0.unsqueeze(0).to(DEVICE)))[0]\n#     bboxes = non_max_suppression(bboxes[0], iou_threshold=0.5, threshold=0.4, box_format='midpoint')\n#     bboxes = [[[boxes[0]] + boxes[2:] for boxes in bboxes]][0]\n#     show_image_with_boxes(x0, bboxes, figsize=(8,8))\n    pred = learn.model(x0.unsqueeze(0).to(DEVICE))\n    show_image_with_boxes(x0, shape_bb(pred, conf_threshold=0.4), figsize=(8,8), title='Predictions')\n    show_image_with_boxes(x0, shape_bb(y0, conf_threshold=0.4), figsize=(8,8), title='Targets')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot too bad. Let’s save the model.\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=&gt; Saving checkpoint\")\n    torch.save(state, filename)\n\n\ncheckpoint = {'state_dict': learn.model.state_dict(), 'optimizer': learn.opt.state_dict()}\nsave_checkpoint(checkpoint, '8examples_1e-4_20.pth.tar')\n\n=&gt; Saving checkpoint\n\n\nIt’s almost 1 GB.\n\n!ls -lh\n\ntotal 4.8G\n-rw-r--r-- 1 kappa kappa  17M May 29 20:52 00_core.ipynb\n-rw-r--r-- 1 kappa kappa 464K May 20 21:00 01_train.ipynb\n-rw-r--r-- 1 kappa kappa 400K Mar 25 00:30 50_darknet.ipynb\n-rw-r--r-- 1 kappa kappa 3.9M Mar 26 10:43 51_pascal.ipynb\n-rw-r--r-- 1 kappa kappa 1.6M May 10 18:24 52_detection.ipynb\n-rw-r--r-- 1 kappa kappa 1.5M May 27 16:24 53_yolov1.ipynb\n-rw-r--r-- 1 kappa kappa 6.6M May 20 22:46 54_yolov1_miniai.ipynb\n-rw-r--r-- 1 kappa kappa 7.1M May 14 11:10 55_yolov1_miniai_bac.ipynb\n-rw-r--r-- 1 kappa kappa 982M May 29 20:51 8examples_1e-4_20.pth.tar\n-rw-r--r-- 1 kappa kappa  330 Jan  2 08:13 _quarto.yml\n-rw-r--r-- 1 kappa kappa 2.7K May 23 23:34 fasttransform.ipynb\n-rw-r--r-- 1 kappa kappa 2.5K May 10 18:24 index.ipynb\n-rw-r--r-- 1 kappa kappa  260 Mar 26 10:45 nbdev.yml\n-rw-r--r-- 1 kappa kappa 709M May 20 22:33 overfit.pth.tar\n-rw-r--r-- 1 kappa kappa  175 Mar 26 10:45 sidebar.yml\n-rw-rw-r-- 1 kappa kappa  600 Sep  1  2024 styles.css\n-rw-r--r-- 1 kappa kappa 3.1G May 14 11:42 yolov1_1e-6_20.pth.tar\n\n\n\ndef load_checkpoint(weight_path, model, optimizer):\n    print(\"=&gt; Loading checkpoint\")\n    checkpoint = torch.load(weight_path, weights_only=True)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n\nload_checkpoint('8examples_1e-4_20.pth.tar', learn.model, learn.opt)\n\n=&gt; Loading checkpoint",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#bacteria-images",
    "href": "core.html#bacteria-images",
    "title": "core",
    "section": "Bacteria images",
    "text": "Bacteria images\nOur model on VOC seems okay. Let’s finetune it on bacteria images.\n\npath.ls()\n\n(#18) [Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.png'),Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.png'),Path('/home/kappa/data/pili/training_data/1hr01002.csv'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.png'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.png'),Path('/home/kappa/data/pili/training_data/4hrs incu004.csv'),Path('/home/kappa/data/pili/training_data/7.1- 003.png'),Path('/home/kappa/data/pili/training_data/4hrs incu004.png'),Path('/home/kappa/data/pili/training_data/200ms-0.4%-005.csv'),Path('/home/kappa/data/pili/training_data/WT-A86C-LB-ice-002.csv'),Path('/home/kappa/data/pili/training_data/0.1%.004.png'),Path('/home/kappa/data/pili/training_data/dCpdA R1 FH 017.csv'),Path('/home/kappa/data/pili/training_data/0N01002.csv'),Path('/home/kappa/data/pili/training_data/0N01002.png'),Path('/home/kappa/data/pili/training_data/Chp B Replicate 2 200 MS060.csv'),Path('/home/kappa/data/pili/training_data/7.1- 003.csv'),Path('/home/kappa/data/pili/training_data/1hr01002.png'),Path('/home/kappa/data/pili/training_data/0.1%.004.csv')]\n\n\n\nimg_path = path/'0N01002.png'\nbac_im = np.array(Image.open(img_path))\n\n\nyolo_bac_boxes = calc_yolo_boxes(path/'0N01002.csv')\nyolo_bac_boxes[:5]\n\ntensor([[1.0000, 0.1250, 0.9701, 0.0279, 0.0272],\n        [1.0000, 0.1804, 0.9871, 0.0306, 0.0237],\n        [1.0000, 0.1521, 0.8542, 0.0264, 0.0271],\n        [1.0000, 0.3414, 0.8812, 0.0201, 0.0277],\n        [6.0000, 0.3072, 0.9942, 0.0144, 0.0109]], dtype=torch.float64)\n\n\n\nyolo_bac_boxes.shape\n\ntorch.Size([187, 5])\n\n\n\nshow_image_with_boxes(bac_im, yolo_bac_boxes, figsize=(8,8));\n\n\n\n\n\n\n\n\n\ndef create_bacteria_dataframe(path):\n    files = list(path.glob('*'))\n    png_files = [f for f in files if f.suffix == '.png']\n    csv_files = [f for f in files if f.suffix == '.csv']\n    \n    pairs = []\n    for png_file in png_files:\n        stem = png_file.stem\n        matching_csv = next((f for f in csv_files if f.stem == stem), None)\n        if matching_csv: pairs.append([png_file.name, matching_csv.name])\n    \n    return pd.DataFrame(pairs, columns=['img', 'label'])\n\n\nbac_df = create_bacteria_dataframe(path)\nbac_df\n\n\n\n\n\n\n\n\nimg\nlabel\n\n\n\n\n0\nWT-A86C-LB-ice-002.png\nWT-A86C-LB-ice-002.csv\n\n\n1\n200ms-0.4%-005.png\n200ms-0.4%-005.csv\n\n\n2\ndCpdA R1 FH 017.png\ndCpdA R1 FH 017.csv\n\n\n3\nChp B Replicate 2 200 MS060.png\nChp B Replicate 2 200 MS060.csv\n\n\n4\n7.1- 003.png\n7.1- 003.csv\n\n\n5\n4hrs incu004.png\n4hrs incu004.csv\n\n\n6\n0.1%.004.png\n0.1%.004.csv\n\n\n7\n0N01002.png\n0N01002.csv\n\n\n8\n1hr01002.png\n1hr01002.csv\n\n\n\n\n\n\n\n\ntrn_bac_df, val_bac_df = bac_df.iloc[1:7], bac_df.iloc[7:]\ntrn_bac_df\n\n\n\n\n\n\n\n\nimg\nlabel\n\n\n\n\n1\n200ms-0.4%-005.png\n200ms-0.4%-005.csv\n\n\n2\ndCpdA R1 FH 017.png\ndCpdA R1 FH 017.csv\n\n\n3\nChp B Replicate 2 200 MS060.png\nChp B Replicate 2 200 MS060.csv\n\n\n4\n7.1- 003.png\n7.1- 003.csv\n\n\n5\n4hrs incu004.png\n4hrs incu004.csv\n\n\n6\n0.1%.004.png\n0.1%.004.csv\n\n\n\n\n\n\n\n\npath/trn_bac_df.iloc[0, 0]\n\nPath('/home/kappa/data/pili/training_data/200ms-0.4%-005.png')\n\n\n\ntrn_bac_df.to_csv(\"../bac_train.csv\", index=False)\n\n\nval_bac_df.to_csv('../bac_valid.csv', index=False)\n\n\nclass BacDataset(Dataset):\n    def __init__(self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None):\n        self.annotations, self.img_dir = pd.read_csv(csv_file), fc.Path(img_dir)\n        self.label_dir, self.transform = fc.Path(label_dir), transform\n        self.S, self.B, self.C = S, B, C\n\n    def __len__(self): return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = self.label_dir / self.annotations.iloc[index, 1]\n        img_path = self.img_dir / self.annotations.iloc[index, 0]\n        image = Image.open(img_path)\n        boxes = calc_yolo_boxes(label_path)\n        if self.transform: image, boxes = self.transform(image, boxes)\n\n        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n        for box in boxes:\n            class_label, x, y, width, height = box\n            class_label = int(class_label)\n            i, j = int(self.S * y), int(self.S * x)\n            x_cell, y_cell = self.S * x - j, self.S * y - i\n            width_cell, height_cell = width * self.S, height * self.S\n            \n            if 0 &gt; i or i &gt;= 7 or 0 &gt; j or j &gt;= 7: \n                print(i,j)\n\n                continue\n            if label_matrix[i, j, 20] == 0:\n                label_matrix[i, j, 20] = 1\n                label_matrix[i, j, 21:25] = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n                label_matrix[i, j, class_label] = 1\n\n        return image, label_matrix\n\n\nbac_trn_ds = BacDataset(\"../bac_train.csv\", path, path, C=20, transform=transform)\n# bac_trn_ds = BacDataset(\"../bac_train.csv\", path, path, C=20)\nx0, y0 = bac_trn_ds[5]\nx0.shape, y0.shape\n\n(torch.Size([3, 448, 448]), torch.Size([7, 7, 30]))\n\n\nMany boxes are missing because only one class can be predicted from each cell.\n\nshow_image_with_boxes(x0, shape_bb(y0));\n\n\n\n\n\n\n\n\n\nbac_val_ds = BacDataset('../bac_valid.csv', path, path, C=20, transform=transform)\nx0, y0 = bac_val_ds[0]\nx0.shape, y0.shape\n\n(torch.Size([3, 448, 448]), torch.Size([7, 7, 30]))\n\n\nHmm.\n\nshow_image_with_boxes(x0, shape_bb(y0));\n\n\n\n\n\n\n\n\n\nbac_trn_dl, bac_val_dl = get_dls(bac_trn_ds, bac_val_ds, bs)\nxb, yb = next(iter(bac_trn_dl))\nxb.shape, yb.shape\n\n(torch.Size([2, 3, 448, 448]), torch.Size([2, 7, 7, 30]))\n\n\n\nbac_dls = DataLoaders(bac_trn_dl, bac_val_dl)\n\n\nlr, epochs = 3e-3, 25\ntmax = epochs * len(bac_dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nxtra = [BatchSchedCB(sched)]\n\n# learn = Learner(model, bac_dls, YoloLoss(), lr=lr, cbs=cbs+xtra, opt_func=torch.optim.AdamW)\nlearn.dls = bac_dls\n\n\nlearn.show_image_batch(max_n=4)\n\n\n\n\n\n\n\n\n\nlearn.fit(epochs, cbs=[ProgressCB(plot=True)])\n\n\n\n\n\n\n\n\nMeanAP\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.000\n2110.353\n0\ntrain\n00:03\n\n\n0.000\n2019.032\n0\neval\n00:00\n\n\n0.000\n1616.481\n1\ntrain\n00:03\n\n\n0.000\n1406.707\n1\neval\n00:00\n\n\n0.000\n1043.905\n2\ntrain\n00:04\n\n\n0.000\n748.545\n2\neval\n00:00\n\n\n0.000\n711.525\n3\ntrain\n00:04\n\n\n0.000\n580.770\n3\neval\n00:00\n\n\n0.000\n626.063\n4\ntrain\n00:04\n\n\n0.000\n551.427\n4\neval\n00:00\n\n\n0.000\n529.337\n5\ntrain\n00:05\n\n\n0.000\n572.216\n5\neval\n00:00\n\n\n0.000\n559.300\n6\ntrain\n00:05\n\n\n0.000\n489.061\n6\neval\n00:00\n\n\n0.000\n622.305\n7\ntrain\n00:04\n\n\n0.000\n460.538\n7\neval\n00:00\n\n\n0.000\n513.237\n8\ntrain\n00:05\n\n\n0.000\n416.242\n8\neval\n00:00\n\n\n0.000\n434.381\n9\ntrain\n00:05\n\n\n0.000\n395.815\n9\neval\n00:00\n\n\n0.001\n407.428\n10\ntrain\n00:05\n\n\n0.000\n357.732\n10\neval\n00:00\n\n\n0.000\n317.297\n11\ntrain\n00:05\n\n\n0.000\n333.096\n11\neval\n00:00\n\n\n0.000\n250.294\n12\ntrain\n00:04\n\n\n0.000\n310.736\n12\neval\n00:00\n\n\n0.000\n224.237\n13\ntrain\n00:05\n\n\n0.000\n283.839\n13\neval\n00:00\n\n\n0.000\n226.868\n14\ntrain\n00:05\n\n\n0.000\n267.242\n14\neval\n00:00\n\n\n0.001\n193.923\n15\ntrain\n00:05\n\n\n0.000\n259.993\n15\neval\n00:00\n\n\n0.000\n172.583\n16\ntrain\n00:05\n\n\n0.000\n263.563\n16\neval\n00:00\n\n\n0.004\n183.626\n17\ntrain\n00:04\n\n\n0.000\n259.507\n17\neval\n00:00\n\n\n0.000\n178.855\n18\ntrain\n00:05\n\n\n0.000\n253.352\n18\neval\n00:00\n\n\n0.006\n151.948\n19\ntrain\n00:05\n\n\n0.000\n250.560\n19\neval\n00:00\n\n\n0.005\n146.231\n20\ntrain\n00:05\n\n\n0.000\n249.883\n20\neval\n00:00\n\n\n0.004\n153.878\n21\ntrain\n00:05\n\n\n0.000\n247.891\n21\neval\n00:00\n\n\n0.005\n138.734\n22\ntrain\n00:04\n\n\n0.000\n247.245\n22\neval\n00:00\n\n\n0.004\n147.657\n23\ntrain\n00:05\n\n\n0.000\n245.872\n23\neval\n00:00\n\n\n0.006\n135.498\n24\ntrain\n00:05\n\n\n0.000\n245.713\n24\neval\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n7 0\n\n\n\nlearn.model.eval()\nfor i in range(1):\n    x0, y0 = bac_trn_ds[i]\n    pred = learn.model(x0.unsqueeze(0).to(DEVICE))\n    show_image_with_boxes(x0, shape_bb(pred, conf_threshold=0.4), figsize=(8,8), title='Predictions')\n    show_image_with_boxes(x0, shape_bb(y0, conf_threshold=0.4), figsize=(8,8), title='Targets')",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#image-transform",
    "href": "core.html#image-transform",
    "title": "core",
    "section": "Image transform",
    "text": "Image transform\n\nCLAHE\nTackling low signal problem: Let’s take a look at an image with labels.\nHow CLAHE (Contrast Limited Adaptive Histogram Equalization) works:\n\nBasic Principle:\n\n\nUnlike regular histogram equalization which works on the entire image at once, CLAHE works on small regions (tiles) of the image\nThis local approach helps maintain local details and contrast\n\n\nStep-by-Step Process:\n\nThe image is divided into small tiles (defined by tile_grid_size)\nFor each tile:\n\nA local histogram is computed\nThe histogram is clipped at a predetermined value (clip_limit) to prevent noise amplification\nHistogram equalization is applied to that tile\n\nBilinear interpolation is used to eliminate artificial boundaries between tiles\n\nKey Advantages:\n\nBetter handling of local contrast\nPrevents over-amplification of noise (through clipping)\nPreserves edges and local details\nWorks well with varying brightness levels in different image regions\n\nParameters Impact:\n\nclip_limit: Higher values allow more contrast enhancement but may increase noise\ntile_grid_size: Smaller tiles give more local enhancement but might make the image look “patchy”\n\n\n\nim.shape\n\n(1952, 1952)\n\n\nTODO: Why are we using uint16? Maybe should change it to float32 or bfloat16?\n\nim.dtype\n\ndtype('uint16')\n\n\n\nsource\n\n\napply_clahe\n\n apply_clahe (image, clip_limit=2.0, tile_grid_size=(8, 8))\n\n\nsource\n\n\ncompare_ims\n\n compare_ims (img1, img2, im1_title='img1', im2_title='img2', cmap='gray')\n\n\nsource\n\n\ncompare_ims_with_boxes\n\n compare_ims_with_boxes (img1, img2, boxes1=None, boxes2=None,\n                         im1_title='img1', im2_title='img2', legend=None,\n                         legend_loc='best', bformat=None, **kwargs)\n\n\nenh = apply_clahe(im, clip_limit=10.1, tile_grid_size=(8,8))\ncompare_ims_with_boxes(im, enh);\n\n\n\n\n\n\n\n\nHere’s what it looks like with labels:\n\ncompare_ims_with_boxes(im, enh, boxes2=y);",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "pascal.html",
    "href": "pascal.html",
    "title": "Pascal Darknet Classification",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload",
    "crumbs": [
      "Pascal Darknet Classification"
    ]
  },
  {
    "objectID": "pascal.html#data",
    "href": "pascal.html#data",
    "title": "Pascal Darknet Classification",
    "section": "Data",
    "text": "Data\n\nData loading\n\nset_seed(42)\n\nLet’s take a look at VOC2007.\n\ndata_path = fc.Path.home()/'data/'\ndata_path.ls()\n\n(#3) [Path('/home/kappa/data/VOCtrainval_06-Nov-2007.tar'),Path('/home/kappa/data/VOCdevkit'),Path('/home/kappa/data/pili')]\n\n\n\nds = datasets.VOCDetection(root=data_path, year='2007', image_set='train', download=False)\nds\n\nDataset VOCDetection\n    Number of datapoints: 2501\n    Root location: /home/kappa/data\n\n\n\n\nChecking out data\nWhat’s in the data?\n\nds[0]\n\n(&lt;PIL.Image.Image image mode=RGB size=500x333&gt;,\n {'annotation': {'folder': 'VOC2007',\n   'filename': '000012.jpg',\n   'source': {'database': 'The VOC2007 Database',\n    'annotation': 'PASCAL VOC2007',\n    'image': 'flickr',\n    'flickrid': '207539885'},\n   'owner': {'flickrid': 'KevBow', 'name': '?'},\n   'size': {'width': '500', 'height': '333', 'depth': '3'},\n   'segmented': '0',\n   'object': [{'name': 'car',\n     'pose': 'Rear',\n     'truncated': '0',\n     'difficult': '0',\n     'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}})\n\n\n\nsource\n\n\nshow_voc_sample\n\n show_voc_sample (ds, idx, figsize=(12, 10))\n\n\n# set_seed(42)\n# import random\n# random_indices = random.sample(range(len(ds)), 5)\n# for idx in random_indices:\n#     show_voc_sample(ds, idx, figsize=(5,5))\n\n\n\n\n\n\n\n\nImage size: 500x333\nNumber of objects: 1\nObject 1: aeroplane, Difficult: 0, Truncated: 0\n\n\n\n\n\n\n\n\n\nImage size: 332x500\nNumber of objects: 2\nObject 1: person, Difficult: 0, Truncated: 0\nObject 2: person, Difficult: 0, Truncated: 0\n\n\n\n\n\n\n\n\n\nImage size: 500x375\nNumber of objects: 5\nObject 1: person, Difficult: 0, Truncated: 1\nObject 2: bottle, Difficult: 0, Truncated: 1\nObject 3: bottle, Difficult: 0, Truncated: 1\nObject 4: person, Difficult: 0, Truncated: 1\nObject 5: person, Difficult: 0, Truncated: 1\n\n\n\n\n\n\n\n\n\nImage size: 500x333\nNumber of objects: 1\nObject 1: tvmonitor, Difficult: 0, Truncated: 0\n\n\n\n\n\n\n\n\n\nImage size: 500x281\nNumber of objects: 2\nObject 1: car, Difficult: 0, Truncated: 0\nObject 2: car, Difficult: 0, Truncated: 0\n\n\n\nsource\n\n\nget_class_distribution\n\n get_class_distribution (ds)\n\nGet distribution of classes in the dataset\n\nclass_dist = get_class_distribution(ds)\nplt.figure(figsize=(12, 6))\nclass_dist.plot(kind='bar')\nplt.title('Class Distribution in VOC2007')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_image_sizes\n\n get_image_sizes (ds, n=100)\n\nGet distribution of image sizes in the dataset\n\nsizes = get_image_sizes(ds)\nplt.figure(figsize=(10, 6))\nplt.scatter(sizes['width'], sizes['height'], alpha=0.5)\nplt.title('Image Dimensions')\nplt.xlabel('Width')\nplt.ylabel('Height')\nplt.grid(True, alpha=0.3)\n\n\n\n\n\n\n\n\n\nsource\n\n\nshow_class_examples\n\n show_class_examples (ds, class_name, n=4)\n\nShow examples of a specific class\n\nshow_class_examples(ds, 'cat');\n\n\n\n\n\n\n\n\n\nobjects_per_image = [len(ds[i][1]['annotation']['object']) for i in range(len(ds))]\nplt.figure(figsize=(10, 6))\nplt.hist(objects_per_image, bins=10)\nplt.title('Objects per Image')\nplt.xlabel('Number of Objects')\nplt.ylabel('Number of Images')\n\nText(0, 0.5, 'Number of Images')\n\n\n\n\n\n\n\n\n\n\nsource\n\n\ncalculate_dataset_stats\n\n calculate_dataset_stats (dataloader, max_images=None)\n\n*Calculate mean and std of a dataset using a dataloader.\nArgs: dataloader: DataLoader instance max_images: Maximum number of images to use (None = use all)\nReturns: mean and std per channel*\n\nsource\n\n\nget_stats_dataloader\n\n get_stats_dataloader (data_path, bs=32, year='2007')\n\nCreate a dataloader for calculating dataset statistics\n\nstats_dl = get_stats_dataloader(data_path, bs=32)\n\nmean, std = calculate_dataset_stats(stats_dl, max_images=2500)\n\nprint(f\"Dataset mean: {mean.tolist()}\")\nprint(f\"Dataset std: {std.tolist()}\")\n\n\n\n\nDataset mean: [0.45178133249282837, 0.4230543076992035, 0.39004892110824585]\nDataset std: [0.26676368713378906, 0.261764258146286, 0.2731017470359802]\n\n\n\n\nDataset\nWe create pytorch dataset.\nPytorch has options to add transforms to its dataset, so this is like minai’s TfmDataset.\n\nsource\n\n\ncreate_voc_datasets\n\n create_voc_datasets (data_path, train_tfms=None, valid_tfms=None,\n                      year='2007')\n\nCreate training and validation datasets for VOC\n\ntrn_ds, val_ds = create_voc_datasets(data_path)\ntrn_ds\n\nDataset VOCDetection\n    Number of datapoints: 2501\n    Root location: /home/kappa/data\n    StandardTransform\nTransform: Compose(\n                 RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=InterpolationMode.BILINEAR, antialias=True)\n                 RandomHorizontalFlip(p=0.5)\n                 ToImage()\n                 ToDtype(scale=True)\n                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n           )\n\n\nThe target has many more information than we need. We only need annotation.object’s names for classification purposes.\n\ntrn_ds[0]\n\n(Image([[[-0.8678, -0.9020, -0.8678,  ..., -1.0390, -1.0733, -1.1418],\n         [-0.9020, -0.8849, -0.8849,  ..., -1.1247, -1.1418, -1.1589],\n         [-0.8849, -0.9192, -0.9020,  ..., -1.2103, -1.2103, -1.1418],\n         ...,\n         [-0.4739, -0.4911, -0.5253,  ..., -0.7650, -0.7308, -0.7479],\n         [-0.5938, -0.5253, -0.5767,  ..., -0.7650, -0.7137, -0.7479],\n         [-0.6452, -0.5938, -0.5596,  ..., -0.7993, -0.7822, -0.7822]],\n \n        [[-0.7577, -0.7927, -0.7577,  ..., -0.9328, -0.9678, -1.0378],\n         [-0.7927, -0.7752, -0.7752,  ..., -1.0203, -1.0378, -1.0553],\n         [-0.7752, -0.8102, -0.7927,  ..., -1.1078, -1.1078, -1.0378],\n         ...,\n         [-0.3725, -0.3901, -0.4251,  ..., -0.6352, -0.6001, -0.6176],\n         [-0.4951, -0.4251, -0.4776,  ..., -0.6352, -0.5826, -0.6176],\n         [-0.5476, -0.4951, -0.4601,  ..., -0.6702, -0.6527, -0.6527]],\n \n        [[-0.5321, -0.5670, -0.5321,  ..., -0.7413, -0.7761, -0.8458],\n         [-0.5670, -0.5495, -0.5495,  ..., -0.8284, -0.8458, -0.8633],\n         [-0.5495, -0.5844, -0.5670,  ..., -0.9156, -0.9156, -0.8458],\n         ...,\n         [-0.1835, -0.2010, -0.2358,  ..., -0.3753, -0.3404, -0.3578],\n         [-0.3055, -0.2358, -0.2881,  ..., -0.3753, -0.3230, -0.3578],\n         [-0.3578, -0.3055, -0.2707,  ..., -0.4101, -0.3927, -0.3927]]], ),\n {'annotation': {'folder': 'VOC2007',\n   'filename': '000012.jpg',\n   'source': {'database': 'The VOC2007 Database',\n    'annotation': 'PASCAL VOC2007',\n    'image': 'flickr',\n    'flickrid': '207539885'},\n   'owner': {'flickrid': 'KevBow', 'name': '?'},\n   'size': {'width': '500', 'height': '333', 'depth': '3'},\n   'segmented': '0',\n   'object': [{'name': 'car',\n     'pose': 'Rear',\n     'truncated': '0',\n     'difficult': '0',\n     'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}})\n\n\nWith voc_extract, we can get any field we want from the target.\n\nsource\n\n\nvoc_extract\n\n voc_extract (field='name')\n\nCreate a function that extracts a specific field from VOC annotations\nObject name:\n\nds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='train', download=False, \n    target_transform=voc_extract())\nds[0]\n\n(&lt;PIL.Image.Image image mode=RGB size=500x333&gt;, (#1) ['car'])\n\n\nBound box:\n\nds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='train', download=False, \n    target_transform=voc_extract(field='bndbox'))\nds[0]\n\n(&lt;PIL.Image.Image image mode=RGB size=500x333&gt;,\n (#1) [{'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}])\n\n\nFor training, we actually need one-hot encoded vector because the targets are multi-labels.\n\nVOC_CLASSES\n\n(#20) ['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n\n\n\nnames = ['car', 'dog']\nnames\n\n['car', 'dog']\n\n\n\nlbls = torch.zeros(len(VOC_CLASSES))\nlbls\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\ntorch.scatter is a good way to do this:\n\nonehot = lbls.scatter(0, torch.tensor([1,3,5]), 1)\nonehot\n\ntensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.])\n\n\n\nsource\n\n\nonehot_tfm\n\n onehot_tfm (targ, clss=['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n             'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n             'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n             'sofa', 'train', 'tvmonitor'])\n\n\nds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='train', download=False, \n    target_transform=onehot_tfm)\nds[0]\n\n(&lt;PIL.Image.Image image mode=RGB size=500x333&gt;,\n tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0.]))\n\n\nHow about going back to label from one hot encoding? We use np.where. Why use numpy instead of pytorch? Because rvs_onehot_tfm is used for displaying images. We will never use this during training.\n\nonehot\n\ntensor([0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.])\n\n\n\nnp.where(onehot == 1)[0]\n\narray([1, 3, 5])\n\n\n\nVOC_CLASSES[np.where(onehot == 1)[0]]\n\n(#3) ['bicycle','boat','bus']\n\n\n\n_rvs_onehot_tfm(onehot)\n\n(#3) ['bicycle','boat','bus']\n\n\n\n\nDataLoader\nWe got the dataset, so we are ready to create a dataloader. There are couple transformations we want to apply to images. We have images so far, but we need pytorch tensors with the same image sizes. We also normalize images using imagenet statistics.\n\nto_tensor = v2.Compose([\n    v2.Resize((224, 224)),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\ntrn_ds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='train', download=False, \n    transform=to_tensor, target_transform=onehot_tfm)\nval_ds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='val', download=False, \n    transform=to_tensor, target_transform=onehot_tfm)\n\n\nbs = 64\nmulti_label_loss = nn.BCEWithLogitsLoss()\n\ntrn_dl, val_dl = get_dls(trn_ds, val_ds, bs=bs)\n\n\nxb,yb = next(iter(trn_dl))\nxb.shape,yb[:10]\n\n(torch.Size([64, 3, 224, 224]),\n tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n          0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n          0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n          0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0.],\n         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0.],\n         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n          0., 0.],\n         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n          0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n          0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 1.]]))\n\n\nDenormalize image before display.\n\nsource\n\n\ndenorm\n\n denorm (x)\n\n\nshow_image(xb[0], tfm_x=denorm);\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_classification_model\n\n get_classification_model (num_classes=20)\n\nCreate a multi-label classification model based on darknet19\n\nmodel = get_classification_model()\ndls = DataLoaders(trn_dl, val_dl)\nlearn = TrainLearner(model, dls, multi_label_loss, lr=1e-3, \n                   cbs=[TrainCB(), DeviceCB(), ProgressCB(), MetricsCB()])\nlearn.summary()\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      0.00% [0/40 00:00&lt;?]\n    \n    \n\n\nTot params: 20359636; MFLOPS: 970.9\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nSequential\n(64, 3, 224, 224)\n(64, 1024, 7, 7)\n19824576\n970.4\n\n\nAdaptiveAvgPool2d\n(64, 1024, 7, 7)\n(64, 1024, 1, 1)\n0\n0.0\n\n\nFlatten\n(64, 1024, 1, 1)\n(64, 1024)\n0\n0.0\n\n\nLinear\n(64, 1024)\n(64, 512)\n524800\n0.5\n\n\nReLU\n(64, 512)\n(64, 512)\n0\n0.0\n\n\nDropout\n(64, 512)\n(64, 512)\n0\n0.0\n\n\nLinear\n(64, 512)\n(64, 20)\n10260\n0.0\n\n\n\n\n\nWe also have to reverse the transform for the targets. It is in onehot encoding, but we want class names.\n\nyb\n\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 1.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 1., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\n\n[', '.join(_rvs_onehot_tfm(y)) for y in np.array(yb)][:4]\n\n['person', 'chair, person', 'bird', 'dog, person, sofa']\n\n\n\ndef rvs_onehot_tfm(yb): return [', '.join(_rvs_onehot_tfm(y)) for y in np.array(yb)]\n\n\nimport sys, gc, traceback, math, typing, random, numpy as np\nfrom itertools import zip_longest\n\nWe want to transform x and y.\n\nlearn.show_image_batch(tfm_x=denorm, tfm_y=_rvs_onehot_tfm)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      0.00% [0/40 00:00&lt;?]\n    \n    \n\n\n\n\n\n\n\n\n\n\nlearn.lr_find(gamma=1.4, max_mult=2)\n\n\n\n\n\n\n      \n      10.00% [1/10 00:13&lt;02:04]\n    \n    \n\n\n\n\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.455\n0\ntrain\n00:13\n\n\n\n\n\n    \n      \n      0.00% [0/40 00:00&lt;?]",
    "crumbs": [
      "Pascal Darknet Classification"
    ]
  },
  {
    "objectID": "pascal.html#training-classification",
    "href": "pascal.html#training-classification",
    "title": "Pascal Darknet Classification",
    "section": "Training Classification",
    "text": "Training Classification\n\nmodel = get_classification_model()\nlearn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, \n                   cbs=[TrainCB(), DeviceCB(), ProgressCB(), MetricsCB()])\n\n\nlearn.summary()\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      0.00% [0/40 00:00&lt;?]\n    \n    \n\n\nTot params: 20359636; MFLOPS: 970.9\n\n\n\n\n\nModule\nInput\nOutput\nNum params\nMFLOPS\n\n\n\n\nSequential\n(64, 3, 224, 224)\n(64, 1024, 7, 7)\n19824576\n970.4\n\n\nAdaptiveAvgPool2d\n(64, 1024, 7, 7)\n(64, 1024, 1, 1)\n0\n0.0\n\n\nFlatten\n(64, 1024, 1, 1)\n(64, 1024)\n0\n0.0\n\n\nLinear\n(64, 1024)\n(64, 512)\n524800\n0.5\n\n\nReLU\n(64, 512)\n(64, 512)\n0\n0.0\n\n\nDropout\n(64, 512)\n(64, 512)\n0\n0.0\n\n\nLinear\n(64, 512)\n(64, 20)\n10260\n0.0\n\n\n\n\n\n\nmodel = get_classification_model()\nlearn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, \n                   cbs=[DeviceCB(), ProgressCB(), MetricsCB()])\nlearn.fit(3)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.371\n0\ntrain\n00:13\n\n\n0.261\n0\neval\n00:54\n\n\n0.248\n1\ntrain\n00:13\n\n\n0.237\n1\neval\n00:11\n\n\n0.241\n2\ntrain\n00:13\n\n\n0.237\n2\neval\n00:11\n\n\n\n\n\n\nclass TopKAccuracy(Callback):\n    def __init__(self, k_values=[1, 5], class_names=VOC_CLASSES):\n        \"\"\"\n        Implements Top-K accuracy for multi-label classification\n        \n        Args:\n            k_values: List of k values to compute (e.g., [1, 5] for top-1 and top-5)\n            class_names: List of class names\n        \"\"\"\n        self.k_values = sorted(k_values)\n        self.max_k = max(k_values)\n        self.class_names = class_names\n        \n    def before_fit(self, learn):\n        self.learn = learn\n        \n    def before_epoch(self, learn):\n        # Initialize counters for each k\n        self.correct = {k: 0 for k in self.k_values}\n        self.total = 0\n        \n    def after_batch(self, learn):\n        # Get predictions and targets\n        logits = to_cpu(learn.preds)\n        targets = to_cpu(learn.batch[1])\n        batch_size = targets.size(0)\n        \n        # For each image in the batch\n        for i in range(batch_size):\n            # Get ground truth classes for this image\n            true_classes = torch.where(targets[i] == 1)[0]\n            if len(true_classes) == 0:\n                continue  # Skip images with no labels\n                \n            # Get top-k predicted classes\n            _, top_indices = torch.topk(logits[i], min(self.max_k, len(self.class_names)))\n            \n            # Check if any true class is in top-k predictions\n            for k in self.k_values:\n                top_k_indices = top_indices[:k]\n                # For multi-label: if any true class is in top-k predictions, count as correct\n                if any(cls in top_k_indices for cls in true_classes):\n                    self.correct[k] += 1\n            \n            self.total += 1\n        \n    def after_epoch(self, learn):\n        phase = 'train' if learn.training else 'valid'\n        for k in self.k_values:\n            accuracy = self.correct[k] / self.total if self.total &gt; 0 else 0\n            print(f\"{phase} top-{k} accuracy: {accuracy:.4f}\")\n\n\n# Alternative implementation that considers a prediction correct only if \n# all true classes are in the top-k predictions\nclass StrictTopKAccuracy(Callback):\n    def __init__(self, k_values=[1, 5], class_names=VOC_CLASSES):\n        self.k_values = sorted(k_values)\n        self.max_k = max(k_values)\n        self.class_names = class_names\n        \n    def before_fit(self, learn):\n        self.learn = learn\n        \n    def before_epoch(self, learn):\n        self.correct = {k: 0 for k in self.k_values}\n        self.total = 0\n        \n    def after_batch(self, learn):\n        logits = to_cpu(learn.preds)\n        targets = to_cpu(learn.batch[1])\n        batch_size = targets.size(0)\n        \n        for i in range(batch_size):\n            true_classes = torch.where(targets[i] == 1)[0]\n            if len(true_classes) == 0:\n                continue\n                \n            _, top_indices = torch.topk(logits[i], min(self.max_k, len(self.class_names)))\n            \n            for k in self.k_values:\n                if k &lt; len(true_classes):\n                    continue  # Can't fit all true classes in top-k if k &lt; number of true classes\n                    \n                top_k_indices = set(top_indices[:k].tolist())\n                true_classes_set = set(true_classes.tolist())\n                \n                # Strict version: all true classes must be in top-k predictions\n                if true_classes_set.issubset(top_k_indices):\n                    self.correct[k] += 1\n            \n            self.total += 1\n        \n    def after_epoch(self, learn):\n        phase = 'train' if learn.training else 'valid'\n        for k in self.k_values:\n            accuracy = self.correct[k] / self.total if self.total &gt; 0 else 0\n            print(f\"{phase} strict top-{k} accuracy: {accuracy:.4f}\")\n\n\nmodel = get_classification_model()\nlearn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, \n                   cbs=[DeviceCB(), ProgressCB(), MetricsCB(top5=TopKMultilabelAccuracy(k=5)), \n                        TopKAccuracy(k_values=[1, 5])])\n\n\nlearn.fit(3)\n\n\n\n\n\n\n      \n      66.67% [2/3 00:49&lt;00:24]\n    \n    \n\n\n\n\ntop5\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.000\n0.374\n0\ntrain\n00:13\n\n\n0.000\n0.265\n0\neval\n00:11\n\n\n0.000\n0.247\n1\ntrain\n00:13\n\n\n0.000\n0.238\n1\neval\n00:11\n\n\n0.000\n0.240\n2\ntrain\n00:13\n\n\n\n\n\n    \n      \n      50.00% [10/20 00:05&lt;00:05 0.240]\n    \n\n\ntrain top-1 accuracy: 0.3631\ntrain top-5 accuracy: 0.6234\nvalid top-1 accuracy: 0.4084\nvalid top-5 accuracy: 0.6657\ntrain top-1 accuracy: 0.4266\ntrain top-5 accuracy: 0.6745\nvalid top-1 accuracy: 0.4084\nvalid top-5 accuracy: 0.7032\ntrain top-1 accuracy: 0.4314\ntrain top-5 accuracy: 0.7245\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[75], line 1\n----&gt; 1 learn.fit(3)\n\nFile ~/git/minai/minai/core.py:260, in Learner.fit(self, n_epochs, train, valid, cbs, lr)\n    258     if lr is None: lr = self.lr\n    259     if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n--&gt; 260     self._fit(train, valid)\n    261 finally:\n    262     for cb in cbs: self.cbs.remove(cb)\n\nFile ~/git/minai/minai/core.py:194, in with_cbs.__call__.&lt;locals&gt;._f(o, *args, **kwargs)\n    192 try:\n    193     o.callback(f'before_{self.nm}')\n--&gt; 194     f(o, *args, **kwargs)\n    195     o.callback(f'after_{self.nm}')\n    196 except globals()[f'Cancel{self.nm.title()}Exception']: pass\n\nFile ~/git/minai/minai/core.py:250, in Learner._fit(self, train, valid)\n    248 if train: self.one_epoch(True)\n    249 if valid:\n--&gt; 250     with torch.inference_mode(): self.one_epoch(False)\n\nFile ~/git/minai/minai/core.py:241, in Learner.one_epoch(self, training)\n    239 self.model.train(training)\n    240 self.dl = self.train_dl if training else self.dls.valid\n--&gt; 241 self._one_epoch()\n\nFile ~/git/minai/minai/core.py:194, in with_cbs.__call__.&lt;locals&gt;._f(o, *args, **kwargs)\n    192 try:\n    193     o.callback(f'before_{self.nm}')\n--&gt; 194     f(o, *args, **kwargs)\n    195     o.callback(f'after_{self.nm}')\n    196 except globals()[f'Cancel{self.nm.title()}Exception']: pass\n\nFile ~/git/minai/minai/core.py:236, in Learner._one_epoch(self)\n    234 @with_cbs('epoch')\n    235 def _one_epoch(self):\n--&gt; 236     for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\nFile ~/miniforge3/lib/python3.10/site-packages/fastprogress/fastprogress.py:41, in ProgressBar.__iter__(self)\n     39 if self.total != 0: self.update(0)\n     40 try:\n---&gt; 41     for i,o in enumerate(self.gen):\n     42         if self.total and i &gt;= self.total: break\n     43         yield o\n\nFile ~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701, in _BaseDataLoaderIter.__next__(self)\n    698 if self._sampler_iter is None:\n    699     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    700     self._reset()  # type: ignore[call-arg]\n--&gt; 701 data = self._next_data()\n    702 self._num_yielded += 1\n    703 if (\n    704     self._dataset_kind == _DatasetKind.Iterable\n    705     and self._IterableDataset_len_called is not None\n    706     and self._num_yielded &gt; self._IterableDataset_len_called\n    707 ):\n\nFile ~/miniforge3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757, in _SingleProcessDataLoaderIter._next_data(self)\n    755 def _next_data(self):\n    756     index = self._next_index()  # may raise StopIteration\n--&gt; 757     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    758     if self._pin_memory:\n    759         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\nFile ~/miniforge3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n     50         data = self.dataset.__getitems__(possibly_batched_index)\n     51     else:\n---&gt; 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n\nFile ~/miniforge3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52, in &lt;listcomp&gt;(.0)\n     50         data = self.dataset.__getitems__(possibly_batched_index)\n     51     else:\n---&gt; 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n\nFile ~/miniforge3/lib/python3.10/site-packages/torchvision/datasets/voc.py:201, in VOCDetection.__getitem__(self, index)\n    193 \"\"\"\n    194 Args:\n    195     index (int): Index\n   (...)\n    198     tuple: (image, target) where target is a dictionary of the XML tree.\n    199 \"\"\"\n    200 img = Image.open(self.images[index]).convert(\"RGB\")\n--&gt; 201 target = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n    203 if self.transforms is not None:\n    204     img, target = self.transforms(img, target)\n\nFile ~/miniforge3/lib/python3.10/site-packages/defusedxml/common.py:100, in _generate_etree_functions.&lt;locals&gt;.parse(source, parser, forbid_dtd, forbid_entities, forbid_external)\n     93 if parser is None:\n     94     parser = DefusedXMLParser(\n     95         target=_TreeBuilder(),\n     96         forbid_dtd=forbid_dtd,\n     97         forbid_entities=forbid_entities,\n     98         forbid_external=forbid_external,\n     99     )\n--&gt; 100 return _parse(source, parser)\n\nFile ~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:1222, in parse(source, parser)\n   1213 \"\"\"Parse XML document into element tree.\n   1214 \n   1215 *source* is a filename or file object containing XML data,\n   (...)\n   1219 \n   1220 \"\"\"\n   1221 tree = ElementTree()\n-&gt; 1222 tree.parse(source, parser)\n   1223 return tree\n\nFile ~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:586, in ElementTree.parse(self, source, parser)\n    584     if not data:\n    585         break\n--&gt; 586     parser.feed(data)\n    587 self._root = parser.close()\n    588 return self._root\n\nFile ~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:1713, in XMLParser.feed(self, data)\n   1711 \"\"\"Feed encoded data to parser.\"\"\"\n   1712 try:\n-&gt; 1713     self.parser.Parse(data, False)\n   1714 except self._error as v:\n   1715     self._raiseerror(v)\n\nFile /home/conda/feedstock_root/build_artifacts/python-split_1687559129017/work/Modules/pyexpat.c:416, in StartElement()\n\nFile ~/miniforge3/lib/python3.10/xml/etree/ElementTree.py:1641, in XMLParser._start(self, tag, attr_list)\n   1638 def _end_ns(self, prefix):\n   1639     return self.target.end_ns(prefix or '')\n-&gt; 1641 def _start(self, tag, attr_list):\n   1642     # Handler for expat's StartElementHandler. Since ordered_attributes\n   1643     # is set, the attributes are reported as a list of alternating\n   1644     # attribute name,value.\n   1645     fixname = self._fixname\n   1646     tag = fixname(tag)\n\nKeyboardInterrupt: \n\n\n\n\nmodel = get_classification_model()\nlearn = TrainLearner(model, dls, multi_label_loss, lr=1e-1, \n                   cbs=[DeviceCB(), ProgressCB(), \n                        MetricsCB(mAP=MultilabelAUPRC(num_labels=20), hamming=MultilabelAccuracy(criteria='hamming'), overlap=MultilabelAccuracy(criteria='overlap'), contain=MultilabelAccuracy(criteria='contain'), belong=MultilabelAccuracy(criteria='belong'),top1=MultilabelAccuracy(criteria='exact_match'), top5=TopKMultilabelAccuracy(criteria='contain', k=5), ), \n                        TopKAccuracy(k_values=[1, 5]), \n                        StrictTopKAccuracy(k_values=[1, 5])])\n\n\nlearn.fit(5)\n\n\n\n\n\n\n      \n      40.00% [2/5 09:29&lt;14:14]\n    \n    \n\n\n\n\nmAP\nhamming\noverlap\ncontain\nbelong\ntop1\ntop5\nloss\nepoch\ntrain\ntime\n\n\n\n\n0.085\n0.920\n0.002\n0.001\n0.998\n0.001\n0.299\n0.374\n0\ntrain\n03:34\n\n\n0.103\n0.922\n0.000\n0.000\n1.000\n0.000\n0.362\n0.263\n0\neval\n01:20\n\n\n0.099\n0.920\n0.006\n0.001\n0.995\n0.001\n0.355\n0.249\n1\ntrain\n03:20\n\n\n0.149\n0.922\n0.000\n0.000\n1.000\n0.000\n0.423\n0.238\n1\neval\n01:14\n\n\n\n\n\n    \n      \n      15.00% [6/40 00:31&lt;02:58 0.237]\n    \n\n\ntrain top-1 accuracy: 0.3894\ntrain top-5 accuracy: 0.6301\ntrain strict top-1 accuracy: 0.0768\ntrain strict top-5 accuracy: 0.2991\nvalid top-1 accuracy: 0.4084\nvalid top-5 accuracy: 0.7040\nvalid strict top-1 accuracy: 0.0797\nvalid strict top-5 accuracy: 0.3622\ntrain top-1 accuracy: 0.4278\ntrain top-5 accuracy: 0.6853\ntrain strict top-1 accuracy: 0.0832\ntrain strict top-5 accuracy: 0.3555\nvalid top-1 accuracy: 0.4084\nvalid top-5 accuracy: 0.7104\nvalid strict top-1 accuracy: 0.0797\nvalid strict top-5 accuracy: 0.4231\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[117], line 1\n----&gt; 1 learn.fit(5)\n\nFile ~/git/minai/minai/core.py:260, in Learner.fit(self, n_epochs, train, valid, cbs, lr)\n    258     if lr is None: lr = self.lr\n    259     if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n--&gt; 260     self._fit(train, valid)\n    261 finally:\n    262     for cb in cbs: self.cbs.remove(cb)\n\nFile ~/git/minai/minai/core.py:194, in with_cbs.__call__.&lt;locals&gt;._f(o, *args, **kwargs)\n    192 try:\n    193     o.callback(f'before_{self.nm}')\n--&gt; 194     f(o, *args, **kwargs)\n    195     o.callback(f'after_{self.nm}')\n    196 except globals()[f'Cancel{self.nm.title()}Exception']: pass\n\nFile ~/git/minai/minai/core.py:248, in Learner._fit(self, train, valid)\n    246 if self.epoch_sz is not None: self.train_dl = CycleDL(self.train_dl, self.epoch_sz)\n    247 for self.epoch in self.epochs:\n--&gt; 248     if train: self.one_epoch(True)\n    249     if valid:\n    250         with torch.inference_mode(): self.one_epoch(False)\n\nFile ~/git/minai/minai/core.py:241, in Learner.one_epoch(self, training)\n    239 self.model.train(training)\n    240 self.dl = self.train_dl if training else self.dls.valid\n--&gt; 241 self._one_epoch()\n\nFile ~/git/minai/minai/core.py:194, in with_cbs.__call__.&lt;locals&gt;._f(o, *args, **kwargs)\n    192 try:\n    193     o.callback(f'before_{self.nm}')\n--&gt; 194     f(o, *args, **kwargs)\n    195     o.callback(f'after_{self.nm}')\n    196 except globals()[f'Cancel{self.nm.title()}Exception']: pass\n\nFile ~/git/minai/minai/core.py:236, in Learner._one_epoch(self)\n    234 @with_cbs('epoch')\n    235 def _one_epoch(self):\n--&gt; 236     for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/fastprogress/fastprogress.py:41, in ProgressBar.__iter__(self)\n     39 if self.total != 0: self.update(0)\n     40 try:\n---&gt; 41     for i,o in enumerate(self.gen):\n     42         if self.total and i &gt;= self.total: break\n     43         yield o\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701, in _BaseDataLoaderIter.__next__(self)\n    698 if self._sampler_iter is None:\n    699     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    700     self._reset()  # type: ignore[call-arg]\n--&gt; 701 data = self._next_data()\n    702 self._num_yielded += 1\n    703 if (\n    704     self._dataset_kind == _DatasetKind.Iterable\n    705     and self._IterableDataset_len_called is not None\n    706     and self._num_yielded &gt; self._IterableDataset_len_called\n    707 ):\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757, in _SingleProcessDataLoaderIter._next_data(self)\n    755 def _next_data(self):\n    756     index = self._next_index()  # may raise StopIteration\n--&gt; 757     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    758     if self._pin_memory:\n    759         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n     50         data = self.dataset.__getitems__(possibly_batched_index)\n     51     else:\n---&gt; 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52, in &lt;listcomp&gt;(.0)\n     50         data = self.dataset.__getitems__(possibly_batched_index)\n     51     else:\n---&gt; 52         data = [self.dataset[idx] for idx in possibly_batched_index]\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/datasets/voc.py:204, in VOCDetection.__getitem__(self, index)\n    201 target = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n    203 if self.transforms is not None:\n--&gt; 204     img, target = self.transforms(img, target)\n    206 return img, target\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/datasets/vision.py:95, in StandardTransform.__call__(self, input, target)\n     93 def __call__(self, input: Any, target: Any) -&gt; Tuple[Any, Any]:\n     94     if self.transform is not None:\n---&gt; 95         input = self.transform(input)\n     96     if self.target_transform is not None:\n     97         target = self.target_transform(target)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py:51, in Compose.forward(self, *inputs)\n     49 needs_unpacking = len(inputs) &gt; 1\n     50 for transform in self.transforms:\n---&gt; 51     outputs = transform(*inputs)\n     52     inputs = outputs if needs_unpacking else (outputs,)\n     53 return outputs\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:50, in Transform.forward(self, *inputs)\n     45 needs_transform_list = self._needs_transform_list(flat_inputs)\n     46 params = self._get_params(\n     47     [inpt for (inpt, needs_transform) in zip(flat_inputs, needs_transform_list) if needs_transform]\n     48 )\n---&gt; 50 flat_outputs = [\n     51     self._transform(inpt, params) if needs_transform else inpt\n     52     for (inpt, needs_transform) in zip(flat_inputs, needs_transform_list)\n     53 ]\n     55 return tree_unflatten(flat_outputs, spec)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:51, in &lt;listcomp&gt;(.0)\n     45 needs_transform_list = self._needs_transform_list(flat_inputs)\n     46 params = self._get_params(\n     47     [inpt for (inpt, needs_transform) in zip(flat_inputs, needs_transform_list) if needs_transform]\n     48 )\n     50 flat_outputs = [\n---&gt; 51     self._transform(inpt, params) if needs_transform else inpt\n     52     for (inpt, needs_transform) in zip(flat_inputs, needs_transform_list)\n     53 ]\n     55 return tree_unflatten(flat_outputs, spec)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_geometry.py:160, in Resize._transform(self, inpt, params)\n    159 def _transform(self, inpt: Any, params: Dict[str, Any]) -&gt; Any:\n--&gt; 160     return self._call_kernel(\n    161         F.resize,\n    162         inpt,\n    163         self.size,\n    164         interpolation=self.interpolation,\n    165         max_size=self.max_size,\n    166         antialias=self.antialias,\n    167     )\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:35, in Transform._call_kernel(self, functional, inpt, *args, **kwargs)\n     33 def _call_kernel(self, functional: Callable, inpt: Any, *args: Any, **kwargs: Any) -&gt; Any:\n     34     kernel = _get_kernel(functional, type(inpt), allow_passthrough=True)\n---&gt; 35     return kernel(inpt, *args, **kwargs)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_geometry.py:310, in __resize_image_pil_dispatch(image, size, interpolation, max_size, antialias)\n    308 if antialias is False:\n    309     warnings.warn(\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\")\n--&gt; 310 return _resize_image_pil(image, size=size, interpolation=interpolation, max_size=max_size)\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_geometry.py:297, in _resize_image_pil(image, size, interpolation, max_size)\n    294 if (new_height, new_width) == (old_height, old_width):\n    295     return image\n--&gt; 297 return image.resize((new_width, new_height), resample=pil_modes_mapping[interpolation])\n\nFile ~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/PIL/Image.py:2328, in Image.resize(self, size, resample, box, reducing_gap)\n   2316         self = (\n   2317             self.reduce(factor, box=reduce_box)\n   2318             if callable(self.reduce)\n   2319             else Image.reduce(self, factor, box=reduce_box)\n   2320         )\n   2321         box = (\n   2322             (box[0] - reduce_box[0]) / factor_x,\n   2323             (box[1] - reduce_box[1]) / factor_y,\n   2324             (box[2] - reduce_box[0]) / factor_x,\n   2325             (box[3] - reduce_box[1]) / factor_y,\n   2326         )\n-&gt; 2328 return self._new(self.im.resize(size, resample, box))\n\nKeyboardInterrupt: \n\n\n\n\n# Code from https://github.com/pytorch/vision/blob/main/gallery/transforms/helpers.py\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\n\ndef plot(imgs, row_title=None, **imshow_kwargs):\n    if not isinstance(imgs[0], list):\n        # Make a 2d grid even if there's just 1 row\n        imgs = [imgs]\n\n    num_rows = len(imgs)\n    num_cols = len(imgs[0])\n    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n    for row_idx, row in enumerate(imgs):\n        for col_idx, img in enumerate(row):\n            boxes = None\n            masks = None\n            if isinstance(img, tuple):\n                img, target = img\n                if isinstance(target, dict):\n                    boxes = target.get(\"boxes\")\n                    masks = target.get(\"masks\")\n                elif isinstance(target, tv_tensors.BoundingBoxes):\n                    boxes = target\n                else:\n                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n            img = F.to_image(img)\n            if img.dtype.is_floating_point and img.min() &lt; 0:\n                # Poor man's re-normalization for the colors to be OK-ish. This\n                # is useful for images coming out of Normalize()\n                img -= img.min()\n                img /= img.max()\n\n            img = F.to_dtype(img, torch.uint8, scale=True)\n            if boxes is not None:\n                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n            if masks is not None:\n                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n\n            ax = axs[row_idx, col_idx]\n            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n    if row_title is not None:\n        for row_idx in range(num_rows):\n            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n\n    plt.tight_layout()",
    "crumbs": [
      "Pascal Darknet Classification"
    ]
  },
  {
    "objectID": "detection.html",
    "href": "detection.html",
    "title": "Pascal Darknet Detection",
    "section": "",
    "text": "from minai import *\n\nimport torch\nimport torch.nn as nn\nfrom torch import tensor\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nimport torchvision.transforms.v2.functional as TF\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torchvision.transforms import v2\n\nfrom torcheval.metrics import MulticlassAccuracy\n\nimport fastcore.all as fc\nfrom fastcore.utils import L\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom datasets import load_dataset, load_dataset_builder\n\nfrom IPython.display import display, Image\n\nfrom pilus_project.core import *\nfrom pilus_project.darknet import *\nfrom pilus_project.pascal_class import *\nfrom torchvision.utils import draw_bounding_boxes\nfrom torchvision import tv_tensors\nUsing PyTorch transforms v2 tutorial for bounding boxes. Learn more there.\n# Code from https://github.com/pytorch/vision/blob/main/gallery/transforms/helpers.py\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\ndef plot(imgs, row_title=None, **imshow_kwargs):\n    if not isinstance(imgs[0], list):\n        # Make a 2d grid even if there's just 1 row\n        imgs = [imgs]\n\n    num_rows = len(imgs)\n    num_cols = len(imgs[0])\n    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n    for row_idx, row in enumerate(imgs):\n        for col_idx, img in enumerate(row):\n            boxes = None\n            masks = None\n            if isinstance(img, tuple):\n                img, target = img\n                if isinstance(target, dict):\n                    boxes = target.get(\"boxes\")\n                    masks = target.get(\"masks\")\n                elif isinstance(target, tv_tensors.BoundingBoxes):\n                    boxes = target\n                else:\n                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n            img = F.to_image(img)\n            if img.dtype.is_floating_point and img.min() &lt; 0:\n                # Poor man's re-normalization for the colors to be OK-ish. This\n                # is useful for images coming out of Normalize()\n                img -= img.min()\n                img /= img.max()\n\n            img = F.to_dtype(img, torch.uint8, scale=True)\n            if boxes is not None:\n                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n            if masks is not None:\n                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n\n            ax = axs[row_idx, col_idx]\n            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n    if row_title is not None:\n        for row_idx in range(num_rows):\n            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n\n    plt.tight_layout()",
    "crumbs": [
      "Pascal Darknet Detection"
    ]
  },
  {
    "objectID": "detection.html#data",
    "href": "detection.html#data",
    "title": "Pascal Darknet Detection",
    "section": "Data",
    "text": "Data\n\nData loading\n\nset_seed(42)\n\n\ndata_path = fc.Path.home()/'data/'\ndata_path.ls()\n\n(#3) [Path('/home/kappa/data/VOCtrainval_06-Nov-2007.tar'),Path('/home/kappa/data/VOCdevkit'),Path('/home/kappa/data/pili')]\n\n\n\ntrn_ds, val_ds = create_voc_datasets(data_path)\ntrn_ds\n\nDataset VOCDetection\n    Number of datapoints: 2501\n    Root location: /home/kappa/data\n    StandardTransform\nTransform: Compose(\n                 RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=InterpolationMode.BILINEAR, antialias=True)\n                 RandomHorizontalFlip(p=0.5)\n                 ToImage()\n                 ToDtype(scale=True)\n                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n           )\n\n\n\nds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='train', download=False, )\n#     target_transform=voc_extract(field='bndbox'))\nds[0]\n\n(&lt;PIL.Image.Image image mode=RGB size=500x333&gt;,\n {'annotation': {'folder': 'VOC2007',\n   'filename': '000012.jpg',\n   'source': {'database': 'The VOC2007 Database',\n    'annotation': 'PASCAL VOC2007',\n    'image': 'flickr',\n    'flickrid': '207539885'},\n   'owner': {'flickrid': 'KevBow', 'name': '?'},\n   'size': {'width': '500', 'height': '333', 'depth': '3'},\n   'segmented': '0',\n   'object': [{'name': 'car',\n     'pose': 'Rear',\n     'truncated': '0',\n     'difficult': '0',\n     'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}})",
    "crumbs": [
      "Pascal Darknet Detection"
    ]
  },
  {
    "objectID": "detection.html#data-1",
    "href": "detection.html#data-1",
    "title": "Pascal Darknet Detection",
    "section": "Data",
    "text": "Data\n\nDataset\nUsing TVTensors, it is easy to create a dataset with bounding boxes. It is smart enough to apply which transform to perform based on the type.\nMore on TVTensors.\n\n# Might be useful transform..\n# transforms = v2.Compose([\n#     v2.RandomResizedCrop(size=(224, 224), antialias=True),\n#     v2.RandomPhotometricDistort(p=1),\n#     v2.RandomHorizontalFlip(p=1),\n# ])\n\nAntialiasing\nAntialiasing helps prevent visual artifacts that occur when resizing images to smaller dimensions. When you downsample an image without antialiasing, high-frequency details can create jagged edges, moiré patterns, and other visual artifacts.\nHere’s what antialiasing does:\n\nIt applies a low-pass filter (typically a Gaussian blur) before downsampling to remove high-frequency components that would cause aliasing\nThis results in smoother transitions and reduced jagged edges in the resized image\nIt’s particularly important when working with images containing fine details, text, or regular patterns\n\nSetting antialias=True (the default in newer PyTorch versions) produces higher quality downsampled images but may be slightly slower. For most deep learning applications, using antialiasing is recommended because:\n\nIt provides more consistent and visually pleasing inputs to your model\nIt can reduce misleading high-frequency artifacts that might confuse the model\nThe small performance cost is usually negligible compared to the benefits in image quality\n\nThe only time you might want to disable it (antialias=False) is if:\n\nYou’re extremely performance-sensitive and processing millions of images\nYou’re trying to exactly reproduce results from older code that didn’t use antialiasing\nYou specifically want to preserve certain high-frequency information for your particular task\n\nWe have to use wrap_dataset_for_transforms_v2 on the dataset to use TVTensors on older datasets.\n\nfrom torchvision.datasets import wrap_dataset_for_transforms_v2\n\nCode from https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py.\n\ntransforms = v2.Compose(\n    [\n        v2.Resize((224, 224)),\n        v2.ToImage(),\n        v2.RandomPhotometricDistort(p=1),\n#         v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        # zoomout is weird. do resize intead.\n        v2.RandomIoUCrop(),\n        v2.RandomHorizontalFlip(p=1),\n        v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\n\nds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='train', download=False, transforms=transforms)\nds = wrap_dataset_for_transforms_v2(ds)\nim, boxes = ds[0]\nim, boxes\n\n(Image([[[0.2118, 0.2235, 0.2275,  ..., 0.2627, 0.2627, 0.2706],\n         [0.2314, 0.2353, 0.2431,  ..., 0.2627, 0.2627, 0.2706],\n         [0.2549, 0.2549, 0.2471,  ..., 0.2706, 0.2706, 0.2706],\n         ...,\n         [0.3294, 0.3333, 0.3294,  ..., 0.3176, 0.3059, 0.3059],\n         [0.3373, 0.3373, 0.3333,  ..., 0.3255, 0.3216, 0.3216],\n         [0.3255, 0.3294, 0.3255,  ..., 0.3294, 0.3333, 0.3333]],\n \n        [[0.2078, 0.2235, 0.2275,  ..., 0.2627, 0.2510, 0.2588],\n         [0.2275, 0.2353, 0.2431,  ..., 0.2627, 0.2588, 0.2588],\n         [0.2549, 0.2549, 0.2471,  ..., 0.2706, 0.2667, 0.2588],\n         ...,\n         [0.3176, 0.3216, 0.3255,  ..., 0.3216, 0.2980, 0.2902],\n         [0.3255, 0.3255, 0.3216,  ..., 0.3294, 0.3098, 0.2980],\n         [0.3098, 0.3176, 0.3255,  ..., 0.3333, 0.3216, 0.3098]],\n \n        [[0.1961, 0.2196, 0.2275,  ..., 0.2627, 0.2627, 0.2706],\n         [0.2196, 0.2275, 0.2471,  ..., 0.2627, 0.2627, 0.2706],\n         [0.2510, 0.2549, 0.2510,  ..., 0.2706, 0.2706, 0.2706],\n         ...,\n         [0.3294, 0.3333, 0.3529,  ..., 0.3255, 0.3176, 0.3098],\n         [0.3373, 0.3373, 0.3373,  ..., 0.3333, 0.3255, 0.3255],\n         [0.3255, 0.3294, 0.3294,  ..., 0.3373, 0.3373, 0.3373]]], ),\n {'boxes': BoundingBoxes([[ 67,  65, 155, 181]], format=BoundingBoxFormat.XYXY, canvas_size=(224, 224)),\n  'labels': tensor([7])})\n\n\nBy using TVTensors, boxes and labels are automatically retrieved!\n\nbox_im = draw_bounding_boxes(im, boxes['boxes'])\nshow_image(box_im);\n\n\n\n\n\n\n\n\n\n\nDataLoader\n\nset_seed(52)\ntrn_ds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='train', download=False, transforms=transforms)\ntrn_ds = wrap_dataset_for_transforms_v2(trn_ds)\nval_ds = datasets.VOCDetection(\n    root=data_path, year=\"2007\", image_set='val', download=False, transforms=transforms)\nval_ds = wrap_dataset_for_transforms_v2(val_ds)\n\n\nbs = 64\n\n# We need a custom collation function here, since the object detection\n# models expect a sequence of images and target dictionaries. The default\n# collation function tries to torch.stack() the individual elements,\n# which fails in general for object detection, because the number of bounding\n# boxes varies between the images of the same batch.\ntrn_dl, val_dl = get_dls(trn_ds, val_ds, bs=bs, collate_fn=lambda batch: tuple(zip(*batch)),)\nxb, yb = next(iter(trn_dl))\n\n\ndef lbl_to_nm(lbl):\n    \"\"\"Convert numeric class label to string names. Pytorch datasets use labels with 1-index.\"\"\"\n    return VOC_CLASSES[lbl['labels']-1]\n\n\nlbl_to_nm(yb[0])\n\n(#1) ['train']\n\n\n\ndef mk_boxs(x,y): \n    \"\"\"Create an image with boxes.\"\"\"\n    return draw_bounding_boxes(x, y['boxes'], labels=lbl_to_nm(y))\n\n\nshow_image(mk_boxs(xb[0], yb[0]));\n\n\n\n\n\n\n\n\n\nshow_images([mk_boxs(x,y) for x,y in zip(xb[:9],yb[:9])], figsize=(10,10))",
    "crumbs": [
      "Pascal Darknet Detection"
    ]
  },
  {
    "objectID": "detection.html#learner",
    "href": "detection.html#learner",
    "title": "Pascal Darknet Detection",
    "section": "Learner",
    "text": "Learner\n\nfrom torchvision.io.image import decode_image\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\nfrom torchvision.transforms.functional import to_pil_image\n\nweights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\nmodel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\nmodel\n\nFasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): FastRCNNConvFCHead(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (1): Conv2dNormActivation(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (2): Conv2dNormActivation(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (3): Conv2dNormActivation(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n      (4): Flatten(start_dim=1, end_dim=-1)\n      (5): Linear(in_features=12544, out_features=1024, bias=True)\n      (6): ReLU(inplace=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n    )\n  )\n)\n\n\n\n# Loss function from pytorch https://github.dev/pytorch/vision/tree/main/torchvision\ndef fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n    # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -&gt; Tuple[Tensor, Tensor]\n    \"\"\"\n    Computes the loss for Faster R-CNN.\n\n    Args:\n        class_logits (Tensor)\n        box_regression (Tensor)\n        labels (list[BoxList])\n        regression_targets (Tensor)\n\n    Returns:\n        classification_loss (Tensor)\n        box_loss (Tensor)\n    \"\"\"\n\n    labels = torch.cat(labels, dim=0)\n    regression_targets = torch.cat(regression_targets, dim=0)\n\n    classification_loss = F.cross_entropy(class_logits, labels)\n\n    # get indices that correspond to the regression targets for\n    # the corresponding ground truth labels, to be used with\n    # advanced indexing\n    sampled_pos_inds_subset = torch.where(labels &gt; 0)[0]\n    labels_pos = labels[sampled_pos_inds_subset]\n    N, num_classes = class_logits.shape\n    box_regression = box_regression.reshape(N, box_regression.size(-1) // 4, 4)\n\n    box_loss = F.smooth_l1_loss(\n        box_regression[sampled_pos_inds_subset, labels_pos],\n        regression_targets[sampled_pos_inds_subset],\n        beta=1 / 9,\n        reduction=\"sum\",\n    )\n    box_loss = box_loss / labels.numel()\n\n    return classification_loss, box_loss\n\n\nmodel\n\n\nmodel.train()\n\ndls = DataLoaders(trn_dl, val_dl)\nlearn = TrainLearner(model, dls, fastrcnn_loss, lr=1e-3, \n                   cbs=[TrainCB(), DeviceCB(), ProgressCB(), MetricsCB()])\n# learn.lr_find()\n\n\nimg = xb[0]\nshow_image(img)\n\n\n\n\n\n\n\n\n\nmodel.eval()\npreprocess = weights.transforms()\n\n# Step 3: Apply inference preprocessing transforms\nbatch = [preprocess(img)]\n\n# Step 4: Use the model and visualize the prediction\nprediction = model(batch)[0]\nlabels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\nbox = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n                          labels=labels,\n                          colors=\"red\",\n                          width=4, font_size=30)\n# im = to_pil_image(box.detach())\n# im.show()\nshow_image(im)\n\n/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torchvision/utils.py:225: UserWarning: Argument 'font_size' will be ignored since 'font' is not set.\n  warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")",
    "crumbs": [
      "Pascal Darknet Detection"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pilus_project",
    "section": "",
    "text": "First, install minai:\n$ pip install git+https://github.com/galopyz/minai\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/galopyz/pilus_project.git",
    "crumbs": [
      "pilus_project"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "pilus_project",
    "section": "",
    "text": "First, install minai:\n$ pip install git+https://github.com/galopyz/minai\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/galopyz/pilus_project.git",
    "crumbs": [
      "pilus_project"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "pilus_project",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall pilus_project in Development mode\n# make sure pilus_project package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to pilus_project\n$ nbdev_prepare",
    "crumbs": [
      "pilus_project"
    ]
  },
  {
    "objectID": "index.html#how-to-use-get-voc-data",
    "href": "index.html#how-to-use-get-voc-data",
    "title": "pilus_project",
    "section": "How to use get VOC data",
    "text": "How to use get VOC data\nTo get the VOC data for YOLO, run bash voc_dataset.sh.",
    "crumbs": [
      "pilus_project"
    ]
  }
]